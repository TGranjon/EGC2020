{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objectif du projet : Utiliser le corpus export_articles_EGC_2004_2018 et faire le bilan de l'évolution de la communauté EGC ces 20 dernières années et tenter d'en prédire l'avenir."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plus précisément, on va tenter de classer les différents sujets de ces 20 dernières années de manière à identifier des tendances de sujet.\n",
    "\n",
    "En suivant ces tendances, on aura un pronostic des futurs sujets de l'EGC 2020."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'évolution du projet est suivable à ces addresses : \n",
    "- https://github.com/TGranjon/EGC2020\n",
    "- https://github.com/TGranjon/EGC2020/projects/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Notes :\n",
    "J'ai abandonné l'utilisation de TreeTagger pour lemmatiser : j'ai pas réussi à le faire fonctionner. Remplacé par FrenchLefffLemmatizer.\n",
    "\n",
    "Ainsi que Gensim : la forme de ses corpus est étrange et je pense que c'est une perte de temps à se conformer à leur modèle. Remplacé par SKLearn.\n",
    "\n",
    "Tout le programme peut être effectué sur un ordi sans GPU au prix d'un plus grand temps de traitement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "from random import randint\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "from sklearn.datasets import load_files\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Thomas\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.process_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ouverture du fichier de données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le fichier de données original contient d'innombrables erreurs dûes au remplacement hâtif du caractère \"fin de ligne\" par un caractère vide.\n",
    "\n",
    "Ainsi, plusieurs mots sont collés les uns aux autres et d'autres contiennent un tiret érroné en leur sein."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fichier de données original.\n",
    "doc = pd.read_csv(\"export_articles_EGC_2004_2018.csv\", sep='\\t')\n",
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>series</th>\n",
       "      <th>booktitle</th>\n",
       "      <th>year</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>authors</th>\n",
       "      <th>pdf1page</th>\n",
       "      <th>pdfarticle</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2018</td>\n",
       "      <td>#Idéo2017 : une plateforme citoyenne dédiée à ...</td>\n",
       "      <td>Cette plateforme a pour objectif de permettre ...</td>\n",
       "      <td>Claudia Marinica, Julien Longhi, Nader Hassine...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1002425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2018</td>\n",
       "      <td>A two level co-clustering algorithm for very l...</td>\n",
       "      <td>La classification croisée (co-clustering) est ...</td>\n",
       "      <td>Marius Barctus, Marc Boullé, Fabrice Clérot</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1002372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2018</td>\n",
       "      <td>ALGeoSPF: Un modèle de factorisation basé sur ...</td>\n",
       "      <td>La recommandation de points d'intérêts est dev...</td>\n",
       "      <td>Jean-Benoît Griesner, Talel Abdesssalem, Huber...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1002380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2018</td>\n",
       "      <td>Analyse des sentiments à partir des commentair...</td>\n",
       "      <td>L'analyse des sentiments est un processus pend...</td>\n",
       "      <td>Abdeljalil Elouardighi, Mohcine Maghfour, Hafd...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1002397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2018</td>\n",
       "      <td>Analyse en rôles sémantiques pour le résumé au...</td>\n",
       "      <td>Cet article présente une approche visant à ext...</td>\n",
       "      <td>Elyase Lassouli, Yasmine Mesbahi, Camille Prad...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1002384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2018</td>\n",
       "      <td>Analyse Ontologique de scénario dans un contex...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Marwan Batrouni, Aurélie Bertaux, Christophe N...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1002414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2018</td>\n",
       "      <td>Apport de la fouille de données pour la préven...</td>\n",
       "      <td>Avec plus de 800 000 décès par an dans le mond...</td>\n",
       "      <td>Romain Billot, Sofian Berrouiguet, Mark Larsen...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1002376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2018</td>\n",
       "      <td>Apport des modèles locaux pour les K-moyennes ...</td>\n",
       "      <td>Dans le cadre du clustering prédictif, pour at...</td>\n",
       "      <td>Vincent Lemaire, Oumaima Alaoui Ismaili</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1002379</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2018</td>\n",
       "      <td>Apprendre les relations de préférence et de co...</td>\n",
       "      <td>En classification multi-labels, chaque instanc...</td>\n",
       "      <td>Khalil Laghmari, Christophe Marsala, Mohammed ...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1002381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2018</td>\n",
       "      <td>Approche contextuelle par régression pour les ...</td>\n",
       "      <td>Les tests A/B sont des procédures utilisées pa...</td>\n",
       "      <td>Emmanuelle Claeys, Pierre Gançarski, Myriam Ma...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1002387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2018</td>\n",
       "      <td>Big Data for understanding human dynamics: the...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Fosca Giannotti</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1002363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2018</td>\n",
       "      <td>Cartes Auto-Organisatrices Incrémentales appli...</td>\n",
       "      <td>Le Clustering Collaboratif (CC) vise à faire r...</td>\n",
       "      <td>Denis Maurel, Jérémie Sublime, Sylvain Lefebvre</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1002418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2018</td>\n",
       "      <td>Catégorisation d'articles scientifiques basée ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Bastien Latard, Jonathan Weber, Germain Forest...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1002406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2018</td>\n",
       "      <td>Classification de Données Complexes par Global...</td>\n",
       "      <td>La plupart des méthodes de classification sont...</td>\n",
       "      <td>Étienne-Cuvelier, Marie-Aude-Aufaure</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1002368</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2018</td>\n",
       "      <td>Community structure in complex networks</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Santo Fortunato</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1002362</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2018</td>\n",
       "      <td>Comparaison de mesures de centralité basées su...</td>\n",
       "      <td>Définir l'importance des noeuds dans les résea...</td>\n",
       "      <td>Marwan Ghanem, Clémence Magnien, Fabien Tarissan</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1002416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2018</td>\n",
       "      <td>Complémentarités de représentations vectoriell...</td>\n",
       "      <td>La tâche de similarité sémantique textuelle co...</td>\n",
       "      <td>Julien Hay, Tim Van de Cruys, Philippe Muller,...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1002378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2018</td>\n",
       "      <td>Contextualisation de Singularités en Temps-Rée...</td>\n",
       "      <td>L'émergence de l'IoT et du traitement en temps...</td>\n",
       "      <td>Badre Belabbess, Jérémy Lhez, Musab Bairat, Ol...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1002369</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2018</td>\n",
       "      <td>Contraintes prescriptives compatibles avec OWL...</td>\n",
       "      <td>L'article définit les contraintes prescriptive...</td>\n",
       "      <td>Philippe Martin, Jun Jo</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1002366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2018</td>\n",
       "      <td>Contribution à l'étude de la distributivité d'...</td>\n",
       "      <td>Nous nous intéressons aux treillis distributif...</td>\n",
       "      <td>Alain Gély, Miguel Couceiro, Yassine Namir, Am...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1002373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2018</td>\n",
       "      <td>Découverte de motifs graduels partiellement or...</td>\n",
       "      <td>Les données séquentielles sont aujourd'hui omn...</td>\n",
       "      <td>Simon Ser, Fatiha Saïs, Maguelonne Teisseire</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1002382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2018</td>\n",
       "      <td>Définir les catégories de DBpédia avec des règ...</td>\n",
       "      <td>DBpédia, qui encode les connaissances de Wikip...</td>\n",
       "      <td>Justine Reynaud, Esther Galbrun, Mehwish Alam,...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1002398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2018</td>\n",
       "      <td>Détection de Singularités en temps-réel par co...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Badre Belabbess, Musab Bairat, Jérémy Lhez, Ol...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1002408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2018</td>\n",
       "      <td>Echantillonnage de motifs séquentiels sous con...</td>\n",
       "      <td>L'échantillonnage de motifs est une méthode no...</td>\n",
       "      <td>Lamine Diop, Cheikh Talibouya Diop, Arnaud Gia...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1002367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2018</td>\n",
       "      <td>eDOI : exploration itérative de grands graphes...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Antoine Laumond, Norbert Feron, Guy Melançon, ...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1002410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2018</td>\n",
       "      <td>Elaboration et utilisation d'une base de conna...</td>\n",
       "      <td>Ce poster rend compte d'une entreprise d'élabo...</td>\n",
       "      <td>Nicolas Faure, René-Michel Faure</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1002404</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2018</td>\n",
       "      <td>Élimination des liens inter-langues erronés da...</td>\n",
       "      <td>Un lien inter-langue dans Wikipédia est un lie...</td>\n",
       "      <td>Nacéra Bennacer Seghouani, Francesca Bugiotti,...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1002417</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2018</td>\n",
       "      <td>Et si les réseaux sociaux pouvaient nous aider...</td>\n",
       "      <td>Dans cet article, nous présentons une méthode ...</td>\n",
       "      <td>Rémy Kessler, Guy Lapalme, Fabrizio Gotti, Abd...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1002391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2018</td>\n",
       "      <td>Étiquetage thématique automatisé de corpus par...</td>\n",
       "      <td>Dans les corpus de textes scientifiques, certa...</td>\n",
       "      <td>Lucie Martinet, Hussein T. Al-Natsheh, Fabien ...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1002396</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2018</td>\n",
       "      <td>Évaluation comparative d'algorithmes de centra...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Kévin Deturck, Damien Nouvel, Frédérique Segond</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1002405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1239</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2004</td>\n",
       "      <td>MUSETTE : a framework for knowledge capture fr...</td>\n",
       "      <td>Nous présentons dans cet article une nouvelle ...</td>\n",
       "      <td>Pierre-Antoine Champin, Yannick Prié, Alain Mille</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1000912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1240</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2004</td>\n",
       "      <td>OpAC : Opérateur d'analyse en ligne basé sur u...</td>\n",
       "      <td>L'analyse en ligne OLAP (On-Line Analysis Proc...</td>\n",
       "      <td>Riadh Ben Messaoud, Sabine Rabaseda, Omar Bous...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1000889</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1241</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2004</td>\n",
       "      <td>Optimisation des requêtes temporelles sur le web</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Rim Faiz, Nizar Khayati, Khaled Mellouli</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1001126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1242</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2004</td>\n",
       "      <td>Outil de représentation des évolutions de comm...</td>\n",
       "      <td>Cet article présente un système de visualisati...</td>\n",
       "      <td>Anne Lavallard, Luigi Lancieri</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1001140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1243</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2004</td>\n",
       "      <td>PoBOC : un algorithme de</td>\n",
       "      <td>Nous décrivons l'algorithme PoBOC (Pole-Based ...</td>\n",
       "      <td>Guillaume Cleuziou, Lionel Martin, Christel Vrain</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1001007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1244</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2004</td>\n",
       "      <td>Positionnement multidimensionnel et partitionn...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Antoine Naud</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1001030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1245</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2004</td>\n",
       "      <td>Qualité et datawarehouse dans le milieu hospit...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Mireille Cosquer, François Gros, Alain Livarto...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1000905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1246</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2004</td>\n",
       "      <td>Recherche ciblée de documents sur le web</td>\n",
       "      <td>Les langages de requêtes mots-clés pour le web...</td>\n",
       "      <td>Amar-Djalil Mezaour</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1001124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1247</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2004</td>\n",
       "      <td>Recherche dans de grandes bases d'images fixes...</td>\n",
       "      <td>Une base d'images fixes peut être décrite de p...</td>\n",
       "      <td>Anicet Kouomou Choupo, Annie Morin, Laure Bert...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1000895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1248</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2004</td>\n",
       "      <td>Recherche de règles d'association hiérarchique...</td>\n",
       "      <td>L'Extraction de Connaissances dans la Bases de...</td>\n",
       "      <td>Olivier Couturier, Engelbert Mephu Nguifo, Bri...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1001154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1249</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2004</td>\n",
       "      <td>Réduction d'un jeu de règles d'association par...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Martine Cadot, Joseph Di Martino, Amedeo Napoli</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1001058</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1250</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2004</td>\n",
       "      <td>Réduction du coût d'évaluation d'une règle rel...</td>\n",
       "      <td>De nombreuses tâches en Fouille de Données vis...</td>\n",
       "      <td>Agnès Braud, Teddy Turmeaux</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1001039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1251</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2004</td>\n",
       "      <td>Règles d'identification et méthodes de visuali...</td>\n",
       "      <td>Dans l'étude du patrimoine bâti, la gestion d'...</td>\n",
       "      <td>Iwona Dudek, Jean-Yves Blaise</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1001157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1252</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2004</td>\n",
       "      <td>Régression linéaire symbolique avec variables ...</td>\n",
       "      <td>Le présent papier concerne l'extension des mét...</td>\n",
       "      <td>Filipe Afonso, Lynne Billard, Edwin Diday</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1001000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1253</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2004</td>\n",
       "      <td>Relations entre gènes impliqués dans les cance...</td>\n",
       "      <td>Des relations entre gènes et protéines impliqu...</td>\n",
       "      <td>Jean Royauté, Claire François, Alain Zasadzins...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1001119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1254</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2004</td>\n",
       "      <td>Représentation condensée de motifs émergents</td>\n",
       "      <td>Les motifs émergents sont des associations de ...</td>\n",
       "      <td>Arnaud Soulet, Bruno Crémilleux, François Rioult</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1001022</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1255</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2004</td>\n",
       "      <td>Représentation de graphes par ACP granulaire</td>\n",
       "      <td>L'extraction d'information de grands graphes r...</td>\n",
       "      <td>Bruno Gaume, Louis Ferré</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1001083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1256</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2004</td>\n",
       "      <td>Résumé de cubes de données multidimensionnelle...</td>\n",
       "      <td>Dans le contexte des entrepôts de données, et ...</td>\n",
       "      <td>Yeow Wei Choong, Anne Laurent, Dominique Laure...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1000904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1257</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2004</td>\n",
       "      <td>Sélection d'attributs et classification d'obje...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alexandre Blansché, Pierre Gançarski</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1000994</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1258</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2004</td>\n",
       "      <td>Sélection rapide en apprentissage supervisé</td>\n",
       "      <td>La sélection de variables (SdV) permet de rédu...</td>\n",
       "      <td>Nicolas Nicoloyannis, Gaëlle Legrand, Pierre-E...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1000953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1259</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2004</td>\n",
       "      <td>Sous-ensembles flous définis sur une ontologie</td>\n",
       "      <td>Les sous-ensembles flous peuvent être utilisés...</td>\n",
       "      <td>Rallou Thomopoulos, Patrice Buche, Ollivier Ha...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1000914</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1260</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2004</td>\n",
       "      <td>Uitliation de connaissances pour l'aide à la r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Amedeo Napoli, Rim Al Hulou</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1001125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1261</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2004</td>\n",
       "      <td>Un algorithme de génération des itemsets fermé...</td>\n",
       "      <td>Le traitement de grand volume de données est u...</td>\n",
       "      <td>Engelbert Mephu Nguifo, Huaiguo Fu</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1001062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1262</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2004</td>\n",
       "      <td>Une approche probabiliste pour le classement d...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Lamis Hawarah, Ana Simonet, Michel Simonet</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1001029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1263</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2004</td>\n",
       "      <td>Une étude d'algorithmes de classification supe...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Huaiyu Fu, Huaiguo Fu, Patrick Njiwoua, Engelb...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1001025</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1264</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2004</td>\n",
       "      <td>Une méthode pour l'appropriation de savoir-fai...</td>\n",
       "      <td>La gestion explicite des savoirs et savoir-fai...</td>\n",
       "      <td>Oswaldo Castillo, Nada Matta, Jean-Louis Ermine</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1000911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1265</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2004</td>\n",
       "      <td>Utilisation des graphes de proximité dans le c...</td>\n",
       "      <td>La classification suivant les plus proches voi...</td>\n",
       "      <td>Sylvain Ferrandiz, Marc Boullé</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1001061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1266</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2004</td>\n",
       "      <td>Validation de graphes conceptuels</td>\n",
       "      <td>Les travaux menés en validation des connaissan...</td>\n",
       "      <td>Juliette Dibie-Barthélemy, Ollivier Haemmerlé,...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1000913</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1267</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2004</td>\n",
       "      <td>Veille technologique assistée par la Fouille d...</td>\n",
       "      <td>Le domaine de la veille technologique vise à r...</td>\n",
       "      <td>François Jacquenet, Christine Largeron, Stépha...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1001097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1268</th>\n",
       "      <td>Revue des Nouvelles Technologies de l'Information</td>\n",
       "      <td>EGC</td>\n",
       "      <td>2004</td>\n",
       "      <td>Vers un entrepôt de données pour la gestion de...</td>\n",
       "      <td>Les entrepôts de données sont l'un des plus im...</td>\n",
       "      <td>Hicham Hajji, Nourdine Badji, Jean-Pierre Asté</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p1&amp;p=10...</td>\n",
       "      <td>http://editions-rnti.fr/render_pdf.php?p=1001163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1269 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 series booktitle  year  \\\n",
       "0     Revue des Nouvelles Technologies de l'Information       EGC  2018   \n",
       "1     Revue des Nouvelles Technologies de l'Information       EGC  2018   \n",
       "2     Revue des Nouvelles Technologies de l'Information       EGC  2018   \n",
       "3     Revue des Nouvelles Technologies de l'Information       EGC  2018   \n",
       "4     Revue des Nouvelles Technologies de l'Information       EGC  2018   \n",
       "5     Revue des Nouvelles Technologies de l'Information       EGC  2018   \n",
       "6     Revue des Nouvelles Technologies de l'Information       EGC  2018   \n",
       "7     Revue des Nouvelles Technologies de l'Information       EGC  2018   \n",
       "8     Revue des Nouvelles Technologies de l'Information       EGC  2018   \n",
       "9     Revue des Nouvelles Technologies de l'Information       EGC  2018   \n",
       "10    Revue des Nouvelles Technologies de l'Information       EGC  2018   \n",
       "11    Revue des Nouvelles Technologies de l'Information       EGC  2018   \n",
       "12    Revue des Nouvelles Technologies de l'Information       EGC  2018   \n",
       "13    Revue des Nouvelles Technologies de l'Information       EGC  2018   \n",
       "14    Revue des Nouvelles Technologies de l'Information       EGC  2018   \n",
       "15    Revue des Nouvelles Technologies de l'Information       EGC  2018   \n",
       "16    Revue des Nouvelles Technologies de l'Information       EGC  2018   \n",
       "17    Revue des Nouvelles Technologies de l'Information       EGC  2018   \n",
       "18    Revue des Nouvelles Technologies de l'Information       EGC  2018   \n",
       "19    Revue des Nouvelles Technologies de l'Information       EGC  2018   \n",
       "20    Revue des Nouvelles Technologies de l'Information       EGC  2018   \n",
       "21    Revue des Nouvelles Technologies de l'Information       EGC  2018   \n",
       "22    Revue des Nouvelles Technologies de l'Information       EGC  2018   \n",
       "23    Revue des Nouvelles Technologies de l'Information       EGC  2018   \n",
       "24    Revue des Nouvelles Technologies de l'Information       EGC  2018   \n",
       "25    Revue des Nouvelles Technologies de l'Information       EGC  2018   \n",
       "26    Revue des Nouvelles Technologies de l'Information       EGC  2018   \n",
       "27    Revue des Nouvelles Technologies de l'Information       EGC  2018   \n",
       "28    Revue des Nouvelles Technologies de l'Information       EGC  2018   \n",
       "29    Revue des Nouvelles Technologies de l'Information       EGC  2018   \n",
       "...                                                 ...       ...   ...   \n",
       "1239  Revue des Nouvelles Technologies de l'Information       EGC  2004   \n",
       "1240  Revue des Nouvelles Technologies de l'Information       EGC  2004   \n",
       "1241  Revue des Nouvelles Technologies de l'Information       EGC  2004   \n",
       "1242  Revue des Nouvelles Technologies de l'Information       EGC  2004   \n",
       "1243  Revue des Nouvelles Technologies de l'Information       EGC  2004   \n",
       "1244  Revue des Nouvelles Technologies de l'Information       EGC  2004   \n",
       "1245  Revue des Nouvelles Technologies de l'Information       EGC  2004   \n",
       "1246  Revue des Nouvelles Technologies de l'Information       EGC  2004   \n",
       "1247  Revue des Nouvelles Technologies de l'Information       EGC  2004   \n",
       "1248  Revue des Nouvelles Technologies de l'Information       EGC  2004   \n",
       "1249  Revue des Nouvelles Technologies de l'Information       EGC  2004   \n",
       "1250  Revue des Nouvelles Technologies de l'Information       EGC  2004   \n",
       "1251  Revue des Nouvelles Technologies de l'Information       EGC  2004   \n",
       "1252  Revue des Nouvelles Technologies de l'Information       EGC  2004   \n",
       "1253  Revue des Nouvelles Technologies de l'Information       EGC  2004   \n",
       "1254  Revue des Nouvelles Technologies de l'Information       EGC  2004   \n",
       "1255  Revue des Nouvelles Technologies de l'Information       EGC  2004   \n",
       "1256  Revue des Nouvelles Technologies de l'Information       EGC  2004   \n",
       "1257  Revue des Nouvelles Technologies de l'Information       EGC  2004   \n",
       "1258  Revue des Nouvelles Technologies de l'Information       EGC  2004   \n",
       "1259  Revue des Nouvelles Technologies de l'Information       EGC  2004   \n",
       "1260  Revue des Nouvelles Technologies de l'Information       EGC  2004   \n",
       "1261  Revue des Nouvelles Technologies de l'Information       EGC  2004   \n",
       "1262  Revue des Nouvelles Technologies de l'Information       EGC  2004   \n",
       "1263  Revue des Nouvelles Technologies de l'Information       EGC  2004   \n",
       "1264  Revue des Nouvelles Technologies de l'Information       EGC  2004   \n",
       "1265  Revue des Nouvelles Technologies de l'Information       EGC  2004   \n",
       "1266  Revue des Nouvelles Technologies de l'Information       EGC  2004   \n",
       "1267  Revue des Nouvelles Technologies de l'Information       EGC  2004   \n",
       "1268  Revue des Nouvelles Technologies de l'Information       EGC  2004   \n",
       "\n",
       "                                                  title  \\\n",
       "0     #Idéo2017 : une plateforme citoyenne dédiée à ...   \n",
       "1     A two level co-clustering algorithm for very l...   \n",
       "2     ALGeoSPF: Un modèle de factorisation basé sur ...   \n",
       "3     Analyse des sentiments à partir des commentair...   \n",
       "4     Analyse en rôles sémantiques pour le résumé au...   \n",
       "5     Analyse Ontologique de scénario dans un contex...   \n",
       "6     Apport de la fouille de données pour la préven...   \n",
       "7     Apport des modèles locaux pour les K-moyennes ...   \n",
       "8     Apprendre les relations de préférence et de co...   \n",
       "9     Approche contextuelle par régression pour les ...   \n",
       "10    Big Data for understanding human dynamics: the...   \n",
       "11    Cartes Auto-Organisatrices Incrémentales appli...   \n",
       "12    Catégorisation d'articles scientifiques basée ...   \n",
       "13    Classification de Données Complexes par Global...   \n",
       "14              Community structure in complex networks   \n",
       "15    Comparaison de mesures de centralité basées su...   \n",
       "16    Complémentarités de représentations vectoriell...   \n",
       "17    Contextualisation de Singularités en Temps-Rée...   \n",
       "18    Contraintes prescriptives compatibles avec OWL...   \n",
       "19    Contribution à l'étude de la distributivité d'...   \n",
       "20    Découverte de motifs graduels partiellement or...   \n",
       "21    Définir les catégories de DBpédia avec des règ...   \n",
       "22    Détection de Singularités en temps-réel par co...   \n",
       "23    Echantillonnage de motifs séquentiels sous con...   \n",
       "24    eDOI : exploration itérative de grands graphes...   \n",
       "25    Elaboration et utilisation d'une base de conna...   \n",
       "26    Élimination des liens inter-langues erronés da...   \n",
       "27    Et si les réseaux sociaux pouvaient nous aider...   \n",
       "28    Étiquetage thématique automatisé de corpus par...   \n",
       "29    Évaluation comparative d'algorithmes de centra...   \n",
       "...                                                 ...   \n",
       "1239  MUSETTE : a framework for knowledge capture fr...   \n",
       "1240  OpAC : Opérateur d'analyse en ligne basé sur u...   \n",
       "1241   Optimisation des requêtes temporelles sur le web   \n",
       "1242  Outil de représentation des évolutions de comm...   \n",
       "1243                          PoBOC : un algorithme de    \n",
       "1244  Positionnement multidimensionnel et partitionn...   \n",
       "1245  Qualité et datawarehouse dans le milieu hospit...   \n",
       "1246           Recherche ciblée de documents sur le web   \n",
       "1247  Recherche dans de grandes bases d'images fixes...   \n",
       "1248  Recherche de règles d'association hiérarchique...   \n",
       "1249  Réduction d'un jeu de règles d'association par...   \n",
       "1250  Réduction du coût d'évaluation d'une règle rel...   \n",
       "1251  Règles d'identification et méthodes de visuali...   \n",
       "1252  Régression linéaire symbolique avec variables ...   \n",
       "1253  Relations entre gènes impliqués dans les cance...   \n",
       "1254       Représentation condensée de motifs émergents   \n",
       "1255       Représentation de graphes par ACP granulaire   \n",
       "1256  Résumé de cubes de données multidimensionnelle...   \n",
       "1257  Sélection d'attributs et classification d'obje...   \n",
       "1258        Sélection rapide en apprentissage supervisé   \n",
       "1259     Sous-ensembles flous définis sur une ontologie   \n",
       "1260  Uitliation de connaissances pour l'aide à la r...   \n",
       "1261  Un algorithme de génération des itemsets fermé...   \n",
       "1262  Une approche probabiliste pour le classement d...   \n",
       "1263  Une étude d'algorithmes de classification supe...   \n",
       "1264  Une méthode pour l'appropriation de savoir-fai...   \n",
       "1265  Utilisation des graphes de proximité dans le c...   \n",
       "1266                  Validation de graphes conceptuels   \n",
       "1267  Veille technologique assistée par la Fouille d...   \n",
       "1268  Vers un entrepôt de données pour la gestion de...   \n",
       "\n",
       "                                               abstract  \\\n",
       "0     Cette plateforme a pour objectif de permettre ...   \n",
       "1     La classification croisée (co-clustering) est ...   \n",
       "2     La recommandation de points d'intérêts est dev...   \n",
       "3     L'analyse des sentiments est un processus pend...   \n",
       "4     Cet article présente une approche visant à ext...   \n",
       "5                                                   NaN   \n",
       "6     Avec plus de 800 000 décès par an dans le mond...   \n",
       "7     Dans le cadre du clustering prédictif, pour at...   \n",
       "8     En classification multi-labels, chaque instanc...   \n",
       "9     Les tests A/B sont des procédures utilisées pa...   \n",
       "10                                                  NaN   \n",
       "11    Le Clustering Collaboratif (CC) vise à faire r...   \n",
       "12                                                  NaN   \n",
       "13    La plupart des méthodes de classification sont...   \n",
       "14                                                  NaN   \n",
       "15    Définir l'importance des noeuds dans les résea...   \n",
       "16    La tâche de similarité sémantique textuelle co...   \n",
       "17    L'émergence de l'IoT et du traitement en temps...   \n",
       "18    L'article définit les contraintes prescriptive...   \n",
       "19    Nous nous intéressons aux treillis distributif...   \n",
       "20    Les données séquentielles sont aujourd'hui omn...   \n",
       "21    DBpédia, qui encode les connaissances de Wikip...   \n",
       "22                                                  NaN   \n",
       "23    L'échantillonnage de motifs est une méthode no...   \n",
       "24                                                  NaN   \n",
       "25    Ce poster rend compte d'une entreprise d'élabo...   \n",
       "26    Un lien inter-langue dans Wikipédia est un lie...   \n",
       "27    Dans cet article, nous présentons une méthode ...   \n",
       "28    Dans les corpus de textes scientifiques, certa...   \n",
       "29                                                  NaN   \n",
       "...                                                 ...   \n",
       "1239  Nous présentons dans cet article une nouvelle ...   \n",
       "1240  L'analyse en ligne OLAP (On-Line Analysis Proc...   \n",
       "1241                                                NaN   \n",
       "1242  Cet article présente un système de visualisati...   \n",
       "1243  Nous décrivons l'algorithme PoBOC (Pole-Based ...   \n",
       "1244                                                NaN   \n",
       "1245                                                NaN   \n",
       "1246  Les langages de requêtes mots-clés pour le web...   \n",
       "1247  Une base d'images fixes peut être décrite de p...   \n",
       "1248  L'Extraction de Connaissances dans la Bases de...   \n",
       "1249                                                NaN   \n",
       "1250  De nombreuses tâches en Fouille de Données vis...   \n",
       "1251  Dans l'étude du patrimoine bâti, la gestion d'...   \n",
       "1252  Le présent papier concerne l'extension des mét...   \n",
       "1253  Des relations entre gènes et protéines impliqu...   \n",
       "1254  Les motifs émergents sont des associations de ...   \n",
       "1255  L'extraction d'information de grands graphes r...   \n",
       "1256  Dans le contexte des entrepôts de données, et ...   \n",
       "1257                                                NaN   \n",
       "1258  La sélection de variables (SdV) permet de rédu...   \n",
       "1259  Les sous-ensembles flous peuvent être utilisés...   \n",
       "1260                                                NaN   \n",
       "1261  Le traitement de grand volume de données est u...   \n",
       "1262                                                NaN   \n",
       "1263                                                NaN   \n",
       "1264  La gestion explicite des savoirs et savoir-fai...   \n",
       "1265  La classification suivant les plus proches voi...   \n",
       "1266  Les travaux menés en validation des connaissan...   \n",
       "1267  Le domaine de la veille technologique vise à r...   \n",
       "1268  Les entrepôts de données sont l'un des plus im...   \n",
       "\n",
       "                                                authors  \\\n",
       "0     Claudia Marinica, Julien Longhi, Nader Hassine...   \n",
       "1           Marius Barctus, Marc Boullé, Fabrice Clérot   \n",
       "2     Jean-Benoît Griesner, Talel Abdesssalem, Huber...   \n",
       "3     Abdeljalil Elouardighi, Mohcine Maghfour, Hafd...   \n",
       "4     Elyase Lassouli, Yasmine Mesbahi, Camille Prad...   \n",
       "5     Marwan Batrouni, Aurélie Bertaux, Christophe N...   \n",
       "6     Romain Billot, Sofian Berrouiguet, Mark Larsen...   \n",
       "7               Vincent Lemaire, Oumaima Alaoui Ismaili   \n",
       "8     Khalil Laghmari, Christophe Marsala, Mohammed ...   \n",
       "9     Emmanuelle Claeys, Pierre Gançarski, Myriam Ma...   \n",
       "10                                      Fosca Giannotti   \n",
       "11      Denis Maurel, Jérémie Sublime, Sylvain Lefebvre   \n",
       "12    Bastien Latard, Jonathan Weber, Germain Forest...   \n",
       "13                 Étienne-Cuvelier, Marie-Aude-Aufaure   \n",
       "14                                      Santo Fortunato   \n",
       "15     Marwan Ghanem, Clémence Magnien, Fabien Tarissan   \n",
       "16    Julien Hay, Tim Van de Cruys, Philippe Muller,...   \n",
       "17    Badre Belabbess, Jérémy Lhez, Musab Bairat, Ol...   \n",
       "18                              Philippe Martin, Jun Jo   \n",
       "19    Alain Gély, Miguel Couceiro, Yassine Namir, Am...   \n",
       "20         Simon Ser, Fatiha Saïs, Maguelonne Teisseire   \n",
       "21    Justine Reynaud, Esther Galbrun, Mehwish Alam,...   \n",
       "22    Badre Belabbess, Musab Bairat, Jérémy Lhez, Ol...   \n",
       "23    Lamine Diop, Cheikh Talibouya Diop, Arnaud Gia...   \n",
       "24    Antoine Laumond, Norbert Feron, Guy Melançon, ...   \n",
       "25                     Nicolas Faure, René-Michel Faure   \n",
       "26    Nacéra Bennacer Seghouani, Francesca Bugiotti,...   \n",
       "27    Rémy Kessler, Guy Lapalme, Fabrizio Gotti, Abd...   \n",
       "28    Lucie Martinet, Hussein T. Al-Natsheh, Fabien ...   \n",
       "29      Kévin Deturck, Damien Nouvel, Frédérique Segond   \n",
       "...                                                 ...   \n",
       "1239  Pierre-Antoine Champin, Yannick Prié, Alain Mille   \n",
       "1240  Riadh Ben Messaoud, Sabine Rabaseda, Omar Bous...   \n",
       "1241           Rim Faiz, Nizar Khayati, Khaled Mellouli   \n",
       "1242                     Anne Lavallard, Luigi Lancieri   \n",
       "1243  Guillaume Cleuziou, Lionel Martin, Christel Vrain   \n",
       "1244                                       Antoine Naud   \n",
       "1245  Mireille Cosquer, François Gros, Alain Livarto...   \n",
       "1246                                Amar-Djalil Mezaour   \n",
       "1247  Anicet Kouomou Choupo, Annie Morin, Laure Bert...   \n",
       "1248  Olivier Couturier, Engelbert Mephu Nguifo, Bri...   \n",
       "1249    Martine Cadot, Joseph Di Martino, Amedeo Napoli   \n",
       "1250                        Agnès Braud, Teddy Turmeaux   \n",
       "1251                      Iwona Dudek, Jean-Yves Blaise   \n",
       "1252          Filipe Afonso, Lynne Billard, Edwin Diday   \n",
       "1253  Jean Royauté, Claire François, Alain Zasadzins...   \n",
       "1254   Arnaud Soulet, Bruno Crémilleux, François Rioult   \n",
       "1255                           Bruno Gaume, Louis Ferré   \n",
       "1256  Yeow Wei Choong, Anne Laurent, Dominique Laure...   \n",
       "1257               Alexandre Blansché, Pierre Gançarski   \n",
       "1258  Nicolas Nicoloyannis, Gaëlle Legrand, Pierre-E...   \n",
       "1259  Rallou Thomopoulos, Patrice Buche, Ollivier Ha...   \n",
       "1260                        Amedeo Napoli, Rim Al Hulou   \n",
       "1261                 Engelbert Mephu Nguifo, Huaiguo Fu   \n",
       "1262         Lamis Hawarah, Ana Simonet, Michel Simonet   \n",
       "1263  Huaiyu Fu, Huaiguo Fu, Patrick Njiwoua, Engelb...   \n",
       "1264    Oswaldo Castillo, Nada Matta, Jean-Louis Ermine   \n",
       "1265                     Sylvain Ferrandiz, Marc Boullé   \n",
       "1266  Juliette Dibie-Barthélemy, Ollivier Haemmerlé,...   \n",
       "1267  François Jacquenet, Christine Largeron, Stépha...   \n",
       "1268     Hicham Hajji, Nourdine Badji, Jean-Pierre Asté   \n",
       "\n",
       "                                               pdf1page  \\\n",
       "0     http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "1     http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "2     http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "3     http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "4     http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "5     http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "6     http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "7     http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "8     http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "9     http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "10    http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "11    http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "12    http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "13    http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "14    http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "15    http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "16    http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "17    http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "18    http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "19    http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "20    http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "21    http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "22    http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "23    http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "24    http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "25    http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "26    http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "27    http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "28    http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "29    http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "...                                                 ...   \n",
       "1239  http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "1240  http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "1241  http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "1242  http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "1243  http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "1244  http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "1245  http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "1246  http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "1247  http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "1248  http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "1249  http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "1250  http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "1251  http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "1252  http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "1253  http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "1254  http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "1255  http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "1256  http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "1257  http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "1258  http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "1259  http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "1260  http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "1261  http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "1262  http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "1263  http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "1264  http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "1265  http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "1266  http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "1267  http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "1268  http://editions-rnti.fr/render_pdf.php?p1&p=10...   \n",
       "\n",
       "                                            pdfarticle  \n",
       "0     http://editions-rnti.fr/render_pdf.php?p=1002425  \n",
       "1     http://editions-rnti.fr/render_pdf.php?p=1002372  \n",
       "2     http://editions-rnti.fr/render_pdf.php?p=1002380  \n",
       "3     http://editions-rnti.fr/render_pdf.php?p=1002397  \n",
       "4     http://editions-rnti.fr/render_pdf.php?p=1002384  \n",
       "5     http://editions-rnti.fr/render_pdf.php?p=1002414  \n",
       "6     http://editions-rnti.fr/render_pdf.php?p=1002376  \n",
       "7     http://editions-rnti.fr/render_pdf.php?p=1002379  \n",
       "8     http://editions-rnti.fr/render_pdf.php?p=1002381  \n",
       "9     http://editions-rnti.fr/render_pdf.php?p=1002387  \n",
       "10    http://editions-rnti.fr/render_pdf.php?p=1002363  \n",
       "11    http://editions-rnti.fr/render_pdf.php?p=1002418  \n",
       "12    http://editions-rnti.fr/render_pdf.php?p=1002406  \n",
       "13    http://editions-rnti.fr/render_pdf.php?p=1002368  \n",
       "14    http://editions-rnti.fr/render_pdf.php?p=1002362  \n",
       "15    http://editions-rnti.fr/render_pdf.php?p=1002416  \n",
       "16    http://editions-rnti.fr/render_pdf.php?p=1002378  \n",
       "17    http://editions-rnti.fr/render_pdf.php?p=1002369  \n",
       "18    http://editions-rnti.fr/render_pdf.php?p=1002366  \n",
       "19    http://editions-rnti.fr/render_pdf.php?p=1002373  \n",
       "20    http://editions-rnti.fr/render_pdf.php?p=1002382  \n",
       "21    http://editions-rnti.fr/render_pdf.php?p=1002398  \n",
       "22    http://editions-rnti.fr/render_pdf.php?p=1002408  \n",
       "23    http://editions-rnti.fr/render_pdf.php?p=1002367  \n",
       "24    http://editions-rnti.fr/render_pdf.php?p=1002410  \n",
       "25    http://editions-rnti.fr/render_pdf.php?p=1002404  \n",
       "26    http://editions-rnti.fr/render_pdf.php?p=1002417  \n",
       "27    http://editions-rnti.fr/render_pdf.php?p=1002391  \n",
       "28    http://editions-rnti.fr/render_pdf.php?p=1002396  \n",
       "29    http://editions-rnti.fr/render_pdf.php?p=1002405  \n",
       "...                                                ...  \n",
       "1239  http://editions-rnti.fr/render_pdf.php?p=1000912  \n",
       "1240  http://editions-rnti.fr/render_pdf.php?p=1000889  \n",
       "1241  http://editions-rnti.fr/render_pdf.php?p=1001126  \n",
       "1242  http://editions-rnti.fr/render_pdf.php?p=1001140  \n",
       "1243  http://editions-rnti.fr/render_pdf.php?p=1001007  \n",
       "1244  http://editions-rnti.fr/render_pdf.php?p=1001030  \n",
       "1245  http://editions-rnti.fr/render_pdf.php?p=1000905  \n",
       "1246  http://editions-rnti.fr/render_pdf.php?p=1001124  \n",
       "1247  http://editions-rnti.fr/render_pdf.php?p=1000895  \n",
       "1248  http://editions-rnti.fr/render_pdf.php?p=1001154  \n",
       "1249  http://editions-rnti.fr/render_pdf.php?p=1001058  \n",
       "1250  http://editions-rnti.fr/render_pdf.php?p=1001039  \n",
       "1251  http://editions-rnti.fr/render_pdf.php?p=1001157  \n",
       "1252  http://editions-rnti.fr/render_pdf.php?p=1001000  \n",
       "1253  http://editions-rnti.fr/render_pdf.php?p=1001119  \n",
       "1254  http://editions-rnti.fr/render_pdf.php?p=1001022  \n",
       "1255  http://editions-rnti.fr/render_pdf.php?p=1001083  \n",
       "1256  http://editions-rnti.fr/render_pdf.php?p=1000904  \n",
       "1257  http://editions-rnti.fr/render_pdf.php?p=1000994  \n",
       "1258  http://editions-rnti.fr/render_pdf.php?p=1000953  \n",
       "1259  http://editions-rnti.fr/render_pdf.php?p=1000914  \n",
       "1260  http://editions-rnti.fr/render_pdf.php?p=1001125  \n",
       "1261  http://editions-rnti.fr/render_pdf.php?p=1001062  \n",
       "1262  http://editions-rnti.fr/render_pdf.php?p=1001029  \n",
       "1263  http://editions-rnti.fr/render_pdf.php?p=1001025  \n",
       "1264  http://editions-rnti.fr/render_pdf.php?p=1000911  \n",
       "1265  http://editions-rnti.fr/render_pdf.php?p=1001061  \n",
       "1266  http://editions-rnti.fr/render_pdf.php?p=1000913  \n",
       "1267  http://editions-rnti.fr/render_pdf.php?p=1001097  \n",
       "1268  http://editions-rnti.fr/render_pdf.php?p=1001163  \n",
       "\n",
       "[1269 rows x 8 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#### Fichier de données corrigé.\n",
    "##### La correction va jusqu'à la ligne 733.\n",
    "doc = pd.read_csv(\"export_articles_EGC_2004_2018_Copie.csv\", sep='\\t')\n",
    "doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En travaillant avec title et abstract, on va réaliser les transformations.\n",
    "Tous les documents ont des titres mais ils n'ont pas tous d'abstract.\n",
    "On va donc utiliser une concaténation du titre et de l'abstract afin d'être sûr d'avoir de l'information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lemmatisation et nettoyage des sujets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut mettre les textes en minuscules, tokeniser, retirer la ponctuation, (retirer les nombres), retirer les mots vide puis lemmatiser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fichier des mots à retirer du texte.\n",
    "cleaner = open(\"clean.txt\",\"r\", encoding=\"utf-8\")\n",
    "stops = []\n",
    "for word in cleaner:\n",
    "    stops.append(word.replace(\"\\n\",\"\"))\n",
    "cleaner.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = FrenchLefffLemmatizer()\n",
    "\n",
    "# Lemmatiser le texte.\n",
    "def lemm_tokens(tokens, lemmer):\n",
    "    lemmed = []\n",
    "    for item in tokens:\n",
    "        lemmed.append(lemmer.lemmatize(item))\n",
    "    return lemmed\n",
    "\n",
    "def no_numbers(tokens):\n",
    "    for t in range(len(tokens)):\n",
    "        tokens[t] == \"\".join(i for i in tokens[t] if not i.isdigit())\n",
    "    for token in tokens:\n",
    "        if token == \"\":\n",
    "            tokens.remove(\"\")\n",
    "\n",
    "# Retirer la ponctuation et autres symboles.\n",
    "def punct(text):\n",
    "    for word in text.split(\" \"):\n",
    "        if word.endswith(\".\"):\n",
    "            text = text.replace(word, word.replace(\".\",\"\"))\n",
    "        if word.endswith(\",\"):\n",
    "            text = text.replace(word, word.replace(\",\",\"\"))\n",
    "        if word.endswith(\"?\"):\n",
    "            text = text.replace(word, word.replace(\"?\",\"\"))\n",
    "        if word.endswith(\"!\"):\n",
    "            text = text.replace(word, word.replace(\"!\",\"\"))\n",
    "        if word.endswith(\";\"):\n",
    "            text = text.replace(word, word.replace(\";\",\"\"))\n",
    "        if word.endswith(\":\"):\n",
    "            text = text.replace(word, word.replace(\":\",\"\"))\n",
    "        if word.startswith(\"(\"):\n",
    "            text = text.replace(word, word.replace(\"(\",\"\"))\n",
    "        if word.endswith(\")\"):\n",
    "            text = text.replace(word, word.replace(\")\",\"\"))\n",
    "        if word.startswith(\"'\"):\n",
    "            text = text.replace(word, word.replace(\"'\",\"\"))\n",
    "        if word.endswith(\"'\"):\n",
    "            text = text.replace(word, word.replace(\"'\",\"\"))\n",
    "        if word.startswith(\"d'\"):\n",
    "            text = text.replace(word, word.replace(\"d'\",\"\"))\n",
    "        if word.startswith(\"l'\"):\n",
    "            text = text.replace(word, word.replace(\"l'\",\"\"))\n",
    "        if word.startswith(\"d’\"):\n",
    "            text = text.replace(word, word.replace(\"d’\",\"\"))\n",
    "        if word.startswith(\"l’\"):\n",
    "            text = text.replace(word, word.replace(\"l’\",\"\"))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Effectuer les opérations de pré-traitement du texte.\n",
    "# Retirer la ponctuation, tokeniser puis lemmatiser.\n",
    "def tokenize(text):\n",
    "    text = punct(text)\n",
    "    #text = text.translate(str.maketrans('','',string.punctuation))\n",
    "    print(text)\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    #alpha = no_numbers(tokens)\n",
    "    clean = [word for word in tokens if word not in stops]\n",
    "    lemms = lemm_tokens(clean,lemmer)\n",
    "    #stems = stem_tokens(tokens, stemmer)\n",
    "    return lemms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#idéo2017  une plateforme citoyenne dédiée à analyse des tweets lors des événements politiques\n",
      "cette plateforme a pour objectif de permettre aux citoyens analyser par eux-mêmes les tweets politiques lors événements spécifiques en france pour le cas de élection présidentielle de 2017 #idéo2017 analysait en quasi temps réel les messages des candidats et fournissait leurs principales caractéristiques usage du lexique politique et des comparaisons entre les candidats\n",
      "a two level co-clustering algorithm for very large data sets\n",
      "la classification croisée co-clustering) est une technique qui permet extraire la structure sous-jacente existante entre les lignes et les colonnes une table de données sous forme de blocs plusieurs applications utilisent cette technique cependant de nombreux algorithmes de co-clustering actuels ne passent pas à l'échelle une des approches utilisées avec succès est la méthode modl qui optimise un critère de vraisemblance régularisée cependant pour des tailles plus importante cette méthode atteint sa limite dans cet article nous présentons un nouvel algorithme de co-clustering à deux niveaux qui compte tenu du critère modl permet de traiter efficacement de données de très grande taille ne pouvant pas tenir en mémoire nos expériences montrent que approche proposée gagne en temps de calcul tout en produisant des solutions de qualité\n",
      "algeospf un modèle de factorisation basé sur du clustering géographique pour la recommandation de poi\n",
      "la recommandation de points intérêts est devenue une caractéristique essentielle des réseaux sociaux géo-localisés qui a accompagné émergence des échanges massifs de données digitales cependant les faibles densités de points intérêts visités par les utilisateurs rendent le problème difficile à traiter autant plus que les espaces de mobilité des utilisateurs sont très hétérogènes allant de la ville au monde entier dans ce papier nous explorons impact une approche de clustering spatial sur la qualité de la recommandation notre approche est basée sur un modèle de factorisation de matrices de poisson et un réseau social inféré des différents comportements de mobilité nous avons conduit une évaluation comparative des performances de notre approche sur un jeu de données réaliste les résultats expérimentaux montrent que notre approche permet une précision supérieure aux techniques de recommandation alternatives\n",
      "analyse des sentiments à partir des commentaires facebook publiés en arabe standard ou dialectal marocain par une approche apprentissage automatique\n",
      "analyse des sentiments est un processus pendant lequel la polarité (positive négative ou neutre un texte donné est déterminée nous nous intéressons dans ce travail à analyse des sentiments à partir des commentaires facebook réels partagés en arabe standard ou dialectal marocain par une approche basée sur apprentissage automatique ce processus commence par la collecte des commentaires et leur annotation à aide du crowdsourcing suivi une phase de prétraitement du texte afin extraire des mots arabes réduits à leur racine ces mots vont être utilisés pour la construction des variables entrée en utilisant plusieurs combinaisons de schémas extraction et de pondération pour réduire la dimensionnalité une méthode de sélection de variables est appliquée les résultats obtenus des expérimentations sont très prometteurs\n",
      "analyse en rôles sémantiques pour le résumé automatique\n",
      "cet article présente une approche visant à extraire les informations exprimées dans un corpus de textes et en produire un résumé plusieurs variantes de méthodes extractives de résumé de texte ont été implémentées et évaluées leur principale originalité réside dans exploitation de structures appelées cds pour clause description structure issues un composant annotation en rôles sémantiques et non directement des phrases composant les textes le résumé obtenu est un sous-ensemble des cds issus du corpus origine  ce format permettra dans la suite la détection incohérences textuelles dans ce travail nous retransformons les cds résumés en texte pour permettre la comparaison de notre approche avec celles de la littérature les premiers résultats sont très encourageants les variantes que nous proposons obtiennent généralement de meilleurs scores que des implémentations de méthodes de référence\n",
      "analyse ontologique de scénario dans un contexte big data\n",
      "nan\n",
      "apport de la fouille de données pour la prévention du risque suicidaire\n",
      "avec plus de 800 000 décès par an dans le monde le suicide est la troisième cause de décès évitable il y a 20 fois plus de tentatives impliquant de nombreuses hospitalisations des coûts humains et sociétaux énormes ces dernières années les modalités de collecte de données sociologiques et cliniques concernant les patients reçus en consultation après une tentative ont connu de profonds changements liés aux outils numériques nous présentons les principaux résultats un processus complet de fouille de données sur un échantillon de suicidants de deux hôpitaux européens le premier objectif est identifier des groupes de patients similaires et le second identifier des facteurs de risque associés au nombre de tentatives des méthodes non supervisées acm et clustering et supervisées arbres de régression sont appliquées pour y répondre les résultats mettent en lumière apport de la fouille de données à des fins descriptives ou explicatives\n",
      "apport des modèles locaux pour les k-moyennes prédictives\n",
      "dans le cadre du clustering prédictif pour attribuer la classe aux groupes formés à la fin de la phase apprentissage le vote majoritaire est la méthode communément utilisée cependant cette approche comporte certaines limitations qui influent directement sur la qualité des résultats obtenus en termes de prédiction pour surmonter ce problème nous proposons incorporer des modèles prédictifs localement dans les clusters formés afin améliorer la qualité prédictive du modèle global les résultats expérimentaux montrent que cette incorporation permet obtenir des résultats en termes de prédiction significativement meilleurs par rapport à ceux obtenus en utilisant le vote majoritaire ainsi que des résultats très compétitifs avec ceux obtenus par des algorithmes performants apprentissage supervisé “similaires” ceci est effectué sans dégrader le pouvoir descriptif explicatif) du modèle global\n",
      "apprendre les relations de préférence et de co-occurrence entre les labels en classification multi-labels\n",
      "en classification multi-labels chaque instance est associée à un ou plusieurs labels par exemple un morceau de musique peut être associé aux labels heureux et 'relaxant' des relations de co-occurrence peuvent exister entre les labels  par exemple les labels heureux et triste ne peuvent pas être associés au même morceau de musique les labels peuvent aussi avoir des relations de préférence  par exemple pour un morceau de musique contenant plusieurs piques le label heureux est préféré par rapport au label 'relaxant' les relations entre les labels peuvent aider à mieux prédire les labels associés aux instances les approches existantes peuvent apprendre soit les relations de co-occurrence soit les relations de préférence ce travail introduit une approche permettant de combiner apprentissage des deux types de relations les expérimentations menées montrent que la nouvelle approche introduite offre les meilleurs résultats de prédiction par rapports à cinq approches de état de l'art\n",
      "approche contextuelle par régression pour les tests a/b\n",
      "les tests a/b sont des procédures utilisées par les entreprises du web et de la santé entre autres pour mesurer impact un changement de version une variable par rapport à un objectif bien qu'un nombre de plus en plus important de données soit disponible la mise en place concrète un tel test peut impliquer un coût important relatif à observation et à évaluation une variation lorsque celle-ci n'est pas optimale dans ce papier nous présentons une nouvelle approche intégrant le principe un bandit contextuel prenant en compte ces variables via une procédure de stratification\n",
      "big data for understanding human dynamics the power of networks\n",
      "nan\n",
      "cartes auto-organisatrices incrémentales appliquées au clustering collaboratif\n",
      "le clustering collaboratif cc) vise à faire ressortir les structures communes présentes dans plusieurs vues indépendantes en se basant sur une première étape de clustering locale effectuée dans notre cas à aide de cartes auto-organisatrices som pour self organizing maps en anglais) pour faire face à la quantité toujours croissante de données disponibles utilisation de méthodes de clustering incrémentales est devenue nécessaire ce papier présente un algorithme de som incrémentales compatibles avec les contraintes du cc les expérimentations conduites sur plusieurs jeux de données démontrent la validitéde cette méthode et présentent influence de la taille du batch utilisé lors del'apprentissage\n",
      "catégorisation articles scientifiques basée sur les relations sémantiques des mots-clés\n",
      "nan\n",
      "classification de données complexes par globalisation de mesures de similarité via les moyennes quasi-arithmétiques\n",
      "la plupart des méthodes de classification sont conçues pour des types particuliers de données données numériques textuelles catégoriques fonctionnelles probabilistes ou encore de type graphes cependant les données générées dans notre quotidien sont en général composées de données de types mixtes par exemple si nous considérons la prévention cardiaque dans le domaine de la santé les applications vont combiner des données issues de capteurs avec autres données telles que l'âge le niveau d'effort la fréquence cardiaque maximale des histogrammes de fréquences cardiaques moyennes lors de précédents efforts etc ceci nous amène à la problématique de construire des classes en tenant compte de ces différentes données et de définir une mesure de similarité à partir des similarités de paires objets sur les différents types de variables dans cet article nous proposons une méthode de classification basée sur la fusion des matrices de similarité à aide des moyennes quasi-arithmétiques qui permet de choisir les différentes “dimensions” des données à considérer et ce quel que soit le type de données pour autant qu'une mesure de similarité ou de dissimilarité existe pour chacun des types de données ce qui est très souvent le cas\n",
      "community structure in complex networks\n",
      "nan\n",
      "comparaison de mesures de centralité basées sur les plus courts chemins dans les réseaux dynamiques\n",
      "définir importance des noeuds dans les réseaux statiques est une question de recherche très étudiée depuis de nombreuses années dernièrement des adaptations des métriques classiques ont été proposées pour les réseaux dynamiques ces méthodes reposent sur des approches très différentes dans leur façon évaluer importance des noeuds à un instant donné il est donc nécessaire de pouvoir les évaluer et les comparer dans cet article nous comparons trois approches existes pour mieux comprendre ce qui les différencie nous montrons que la nature des jeux de données influe grandement sur le comportement des méthodes et que pour certains entre eux la notion importance n'est pas toujours pertinente\n",
      "complémentarités de représentations vectorielles pour la similarité sémantique\n",
      "la tâche de similarité sémantique textuelle consiste à exprimer automatiquement un nombre reflétant la similarité sémantique de deux fragments de texte chaque année depuis 2012 les campagnes de semeval déroulent cette tâche de similarité sémantique textuelle cet article présente une méthode associant différentes représentations vectorielles de phrases dans objectif améliorer les résultats obtenus en similarité sémantique notre hypothèse est que différentes représentations permettraient de représenter différents aspects sémantiques,et par extension améliorer les similarités calculées la principale difficulté étant de sélectionner les représentations les plus complémentaires pour cette tâche notre système se base sur le système vainqueur de la campagne de 2015 ainsi que sur notre méthode de sélection par complémentarité les résultats obtenus viennent confirmer intérêt de cette méthode lorsqu'ils sont comparés aux résultats de la campagne de 2016\n",
      "contextualisation de singularités en temps-réel par extraction de connaissances du web des données\n",
      "émergence de iot et du traitement en temps-réel oblige les entreprises à considérer la détection anomalies comme un élément clé de leur activité afin de garantir une haute précision dans le processus de détection des métadonnées fournissant un contexte spatio-temporel sur les mesures des capteurs sont nécessaires dans cet article nous présentons un système générique qui aide à capturer analyser qualifier et stocker les informations contextuelles un domaine application donné approche proposée est basée sur des méthodes sémantiques qui exploitent des ontologies pour évaluer la pertinence de information contextuelle après une description des composants principaux de l'architecture la performance et la pertinence du système sont démontrées par une évaluation sur des ensembles de données du monde réel\n",
      "contraintes prescriptives compatibles avec owl2-er pour évaluer la complétude ontologies\n",
      "article définit les contraintes prescriptives comme des règles permettant aux moteurs inférence de vérifier que certains objets formels sont réellement utilisés – pas seulement inférés – ou non dans certaines conditions il montre que ces contraintes nécessitent de ne pas exploiter de mécanisme héritage ou autres mécanismes ajoutant des relations à des objets durant les tests des conclusions des règles il donne une méthode générale pour effectuer cela et des commandes sparql pour implémenter cette méthode lorsque les règles sont représentées via des relations sous-classe-de entre conditions et conclusions article illustre ces commandes avec la vérification de patrons de conception d'ontologies plus généralement approche peut être utilisée pour vérifier la complétude une ontologie ou représenter dans une ontologie plutôt que par des requêtes ou des procédures ad hoc des contraintes permettant de calculer un degré de complétude d'ontologie approche peut ainsi aider l'élicitation la modélisation ou la validation de connaissances\n",
      "contribution à étude de la distributivité un treillis de concepts\n",
      "nous nous intéressons aux treillis distributifs dans le cadre de analyse formelle de concepts (fca) la motivation primitive vient de la phylogénie et des graphes médians pour représenter les dérivations biologiques et les arbres parcimonieux la fca propose des algorithmes efficaces de construction de treillis de concepts cependant un treillis de concepts n'est pas en correspondance avec un graphe médian sauf s'il est distributif où idée étudier la transformation un treillis de concepts en un treillis distributif pour ce faire nous nous appuyons sur le théorème de représentation de birkhoff qui nous permet de systématiser la transformation un contexte quelconque en un contexte de treillis de concepts distributif ainsi nous pouvons bénéficier de algorithmique de fca pour construire mais aussi visualiser les treillis de concepts distributifs et enfin étudier les graphes médians associés\n",
      "découverte de motifs graduels partiellement ordonnés  application aux données expériences scientifiques\n",
      "les données séquentielles sont aujourd'hui omniprésentes et concernent divers domaines d'application la fouille de données de séquences permet extraire des informations et des connaissances pouvant être à forte valeur ajoutée cependant lorsque les données de séquences sont riches en données numériques des méthodes de fouille de données plus fines sont nécessaires pour extraire des connaissances plus expressives représentant la variabilité des valeurs numériques ainsi que leur éventuelle interdépendance dans cet article,nous présentons une nouvelle méthode de découverte de séquences graduelles fréquentes représentées par des graphes à partir une source de données de séquences en rdf resource description framework 1) ces dernières sont transformées en graphes graduels partiellement ordonnés gpo nous proposons un algorithme permettant de découvrir les sous-graphes gpo fréquents une expérimentation sur deux jeux de données réelles ont montré la faisabilité et la pertinence de notre approche\n",
      "définir les catégories de dbpédia avec des règles associations et des redescriptions\n",
      "dbpédia qui encode les connaissances de wikipédia est devenue une base de référence pour le web des données les ressources peuvent y être répertoriées par des catégories définies manuellement dont la sémantique n'est pas directement accessible par des machines dans cet article nous proposons de remédier à cette lacune au moyen de méthodes de fouille de données à savoir la recherche de règles associations et de motifs apparentés nous présentons une étude comparative de ces variantes sur une partie de dbpédia et discutons le potentiel des différentes approches\n",
      "détection de singularités en temps-réel par combinaison apprentissage automatique et web sémantique basés sur spark\n",
      "nan\n",
      "echantillonnage de motifs séquentiels sous contrainte sur la norme\n",
      "échantillonnage de motifs est une méthode non-exhaustive pour découvrir des motifs pertinents qui assure une bonne interactivité tout en offrant des garanties statistiques fortes grâce à sa nature aléatoire curieusement une telle approche explorée pour les motifs ensemblistes et les sous-graphes ne a pas encore été pour les données séquentielles dans cet article nous proposons la première méthode échantillonnage de motifs séquentiels outre le passage aux séquences originalité de notre approche est introduire une contrainte sur la norme pour maîtriser la longueur des motifs tirés et éviter écueil de la « longue traîne » nous démontrons que notre méthode fondée sur une procédure aléatoire en deux étapes effectue un tirage exact malgré le recours à un échantillonnage avec rejet les expérimentations montrent qu'elle reste performante\n",
      "edoi  exploration itérative de grands graphes multi-couches basée sur une mesure de intérêt de utilisateur\n",
      "nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elaboration et utilisation une base de connaissances un domaine technique\n",
      "ce poster rend compte une entreprise élaboration un système de représentation des connaissances pour le domaine géotechnique\n",
      "élimination des liens inter-langues erronés dans wikipédia\n",
      "un lien inter-langue dans wikipédia est un lien qui mène un article appartenant à une édition linguistique à un autre article décrivant le même concept dans une autre langue ces liens sont ajoutés manuellement par les utilisateurs de wikipédia et ainsi ils sont susceptibles être erronés dans ce papier,nous proposons une approche pour élimination automatique des liens interlangues le principe de base est que la présence un lien erroné est révélée par existence un chemin de liens inter-langues reliant deux articles appartenant à une même édition linguistique notre approche élimine des liens inter-langues à partir de ceux qui ont un faible score de correction jusqu'à ce qu'il n'y ait plus de chemins entre deux articles une même édition linguistique les résultats de notre évaluation sur un sous-graphe de wikipédia consistant en 8 langues montre que approche est prometteuse\n",
      "et si les réseaux sociaux pouvaient nous aider dans nos choix de carrière\n",
      "dans cet article nous présentons une méthode analyse de corpus afin de générer deux interfaces originales de visualisation dans le domaine de l'e-recrutement notre approche s'appuie sur des millions de profils issus de plusieurs réseaux sociaux et sur des milliers offres emploi collectées sur internet nous décrivons dans ces travaux les étapes nécessaires pour leur réalisation la première visualisation est une carte dynamique indiquant les métiers qui recrutent dans quel domaine dans quelle région tandis que la seconde met en avant les parcours professionnels et permet observer les perspectives ainsique les antécédents à plus ou moins long terme pour chaque métier considéré\n",
      "étiquetage thématique automatisé de corpus par représentation sémantique\n",
      "dans les corpus de textes scientifiques certains articles issus de communautés de chercheurs différentes peuvent ne pas être décrits par les mêmes mots-clés alors qu'ils partagent la même thématique ce phénomène cause des problèmes dans la recherche d'information ces articles étant mal indexés et limite les échanges potentiellement fructueux entre disciplines scientifiques notre modèle permet attribuer automatiquement une étiquette thématique aux articles au moyen un apprentissage des représentations sémantiques articles du corpus déjà étiquetés passant bien à l'échelle cette méthode a pu être testée sur une bibliothèque numérique articles scientifiques comportant des millions de documents nous utilisons un réseau sémantique de synonymes pour extraire davantage articles sémantiquement similaires et nous les fusionnons avec ceux obtenus par un modèle de classement thématique cette méthode combinée présente de meilleurs taux de rappel que les versions utilisant soit le réseau sémantique seul soit la seule représentation sémantique des textes\n",
      "évaluation comparative algorithmes de centralité pour la détection influenceurs\n",
      "nan\n",
      "exploration et analyses multi-objectifs de séries temporelles de données météorologiques\n",
      "cet article présente les investigations menées sur les données mesurées par des capteurs positionnés dans cinq villes de île de la réunion des analyses exploratoires préalables permettent de comparer les caractéristiques statistiques des villes considérées relativement aux différentes variables météorologiques mesurées flux solaires diffus et global pression atmosphérique humidité température force et direction du vent) nous appliquons diverses transformations sur les données avant analyser les séries univariées ou multivariées agrégées au pas de heure ou de la journée afin de construire des modèles de prédiction une approche classique de clustering de séries temporelles est testée deux algorithmes de biclustering appliqués successivement ont permis de grouper les journées observations partageant des paramètres météorologiques horaires une caractérisation des biclusters une visualisation calendaire de leur succession ainsi qu'une recherche de séquences fréquentes permettent exploiter les résultats et de faciliter leur interprétation\n",
      "extraction de chaînes cohérentes en vue de reconstuire la trajectoire de information\n",
      "sur internet information se propage en particulier au travers des documents textuels cette propagation soulève de nombreux défis  identifier une information suivre son évolution dans le temps comprendre les mécanismes qui régissent sa propagation etc étant donné un document parmi un grand corpus dans lequel de nombreuses informations circulent pouvons-nous retrouver les chemins empruntés par information pour arriver à ce document  nous proposons de définir la notion de trajectoire comme ensemble des chemins le long desquels de information s'est propagée et nous proposons une méthode pour l'estimer nous avons mis en oeuvre une évaluation humaine pour juger de la qualité des chemins calculés nous montrons que les évaluations concordent la plupart du temps et que notre algorithme est efficace pour retrouver les bons chemins\n",
      "extraction de connaissances sur les défaillances de compteurs essieux\n",
      "cet article propose une méthode analyse pour des enregistrements opérationnels un ensemble de compteurs d'essieux qui constituent un élément central à infrastructure ferroviaire notre objectif est de fournir une façon efficace extraire automatiquement des éléments de connaissance concernant les défaillances de ces systèmes puisque les données fournies ne contiennent pas de vérité de terrain sur les causes de défaillances les informations et leurs causes doivent être extraites des relations sous-tendant les événements enregistrés après une phase de prétraitement les événements sont groupés en fonction des relations qui ont été mises en lumière entre eux ces regroupements peuvent ensuite être utilisés pour créer des classes événements en utilisant un système de classification adapté au delà de cette application spécifique cette approche est une façon nouvelle aborder les problèmes analyse de fiabilité\n",
      "fouille de motifs graduels fermés fréquents sous contrainte de la temporalité\n",
      "nan\n",
      "fouille de motifs temporels négatifs\n",
      "dans cet article nous étudions le problème de extraction de motifs fréquents contenant des événements positifs des événements négatifs spécifiant absence événement ainsi que des informations temporelles sur le délai entre ces événements nous définissons la sémantique de tels motifs et proposons la méthode ntgsp basée sur des approches de état de l'art les performances de la méthode sont évaluées sur des données commerciales fournies par edf électricité de france)\n",
      "interrogation de données structurellement hétérogènes dans les bases de données orientées documents\n",
      "les systèmes orientés documents permettent de stocker tout document quel que soit leur schéma cette flexibilité génère une potentielle hétérogénéité des documents qui complexifie leur interrogation car une même entité peut être décrite selon des schémas différents cet article présente une approche interrogation transparente des systèmes orientés documents pour cela nous proposons de générer un dictionnaire de façon automatique lors de insertion des documents et qui associe à chaque attribut tous les chemins permettant y accéder ce dictionnaire permet de réécrire la requête utilisateur à partir de disjonctions de chemins afin de retrouver tous les documents quelles que soient leurs structures nos expérimentations montrent des coûts exécution de la requête réécrite largement acceptables comparés au coût une requête sur schémas homogènes\n",
      "kti-mooc un système de recommandation pour la personnalisation du processus échange informations dans les moocs\n",
      "afin aider les apprenants à tirer profit du mooc massive openonline course qu'ils suivent nous proposons un outil pour recommander à chacun entre eux une liste ordonnée des “apprenants leaders” capables de le soutenir durant son processus d'apprentissage la phase de recommandation est basée sur une approche aide à la décision multicritère pour la prédiction périodique des “apprenants leaders” etant donnée hétérogénéité des profils des apprenants nous recommandons à chacun entre eux les leaders appropriés à son profil en utilisant la distance euclidienne et le filtrage démographique\n",
      "exploitation de données contextuelles pour la recommandation hôtels\n",
      "les systèmes de recommandation ont pour rôle aider les utilisateurs submergés par la quantité information à faire de bons choix à partir de vastes catalogues de produits le déploiement de ces systèmes dans industrie hôtelièreest confronté à des contraintes spécifiques limitant la performance des approches traditionnelles les systèmes de recommandation hôtels souffrent en particulier un problème de démarrage à froid continu à cause de la volatilité des préférences des voyageurs et du changement de comportements en fonction du contexte dans cet article nous présentons le problème de recommandation hôtels ainsi que ses caractéristiques distinctives nous proposons de nouvelles méthodes contextuelles qui prennent en compte les dimensions géographique et temporelle ainsi que la raison du voyage afin de générer les listes de recommandation nos expérimentations sur des jeux de données réels soulignent la contribution des données contextuelles à amélioration de la qualité de recommandation\n",
      "ontologie ontobiotope pour étude de la biodiversité microbienne\n",
      "intégration des données hétérogènes en sciences de la vie est un sujet de recherche majeur importance et le volume considérable des informations sur les milieux de vie des microorganismes dans tous les domaines tels que la santé agriculture ou environnement justifie le développement de traitements automatisés nous proposons ici ontologie ontobiotope dont nous décrivons les principes de construction ainsi que des exemples utilisation pour annotation et indexation sémantique des habitats microbiens décrits en langue naturelle dans les documents scientifiques\n",
      "long-range influences in social) networks\n",
      "nan\n",
      "mainmise sur les médias et suivi de communautés dans les graphes dynamiques\n",
      "ce court article présente le design et utilisation un tableau de bord visuel permettant d'explorer questionner et comprendre évolution des communautés un graphe dynamique exemple ayant motivé la conception et la réalisation de ce tableau de bord est celui un réseau affiliation des personnalités présentes dans les médias français le suivi de communautés s'avère utile pour cerner le biais potentiel induit de la co-présence répétée des mêmes personnalités dans les émissions de radio et de télévision au cours du temps\n",
      "mean-shift  clustering scalable et distribué\n",
      "nous présentons dans ce papier un nouvel algorithme mean-shift utilisant les k-plus proches voisins pour la montée du gradient nnms  nearestneighbours mean shift) le coût computationnel intensif de ce dernier a longtemps limité son utilisation sur des jeux de données complexes où un partitionnement en clusters non ellipsoïdaux serait bénéfique or une implémentation scalable de algorithme ne compense pas augmentation du temps exécution en fonction de la taille du jeu de données en raison de sa complexité quadratique afin de pallier ce problème nous avons introduit le \"locality sensitivehashing\" lsh) qui est une approximation de la recherche des k-plus proches voisins ainsi qu'une règle empirique pour le choix du k la combinaison de ces améliorations au sein du nnms offre opportunité un traitement pertinentaux problématiques du clustering appliquée aux données massives\n",
      "méta-analyse ordinale enquêtes opinion application aux usages de internet des objets en entreprise\n",
      "la multiplicité des enquêtes opinion sur un même sujet nécessite la construction de synthèses qui agrègent les résultats obtenus dans des conditions indépendantes dans cet article nous proposons une nouvelle approche ordinale de méta-analyse qui consiste à rechercher un ordre consensus qui rend compte « au mieux » des ordres partiels entre les modalités issus des résultats des différentes enquêtes nous modélisons ce problème par une variante une recherche un ordre médian sur les sommets un graphe orienté pondéré et nous développons un algorithme de séparation-évaluation pour le résoudre notre approche est appliquée sur un ensemble enquêtes internationales portant sur les motivations et les freins à intégration de internet des objets dans les entreprises\n",
      "méthode basée sur les ensembles approximatifs pour apprentissage incrémental en présence des données déséquilibrées\n",
      "ce papier propose une méthode basée sur la théorie des ensembles approximatifs et dédiée à apprentissage supervisé incrémental dans un contexte de données déséquilibrées cette méthode consiste en trois phases  la construction une table de décision inférence un ensemble de règles de décision et la classification de chaque action potentielle dans une des classes de décision prédéfinies la méthode mai2p est validée dans le contexte des moocs massive open online courses)\n",
      "méthode apprentissage pour extraire les localisations dans les microblogs\n",
      "nan\n",
      "modélisation des métadonnées un data lake en data vault\n",
      "avec avènement des mégadonnées informatique décisionnelle a dû trouver des solutions pour gérer des données de très grands volume et variété les lacs de données data lakes répondent à ces besoins du point du vue du stockage mais nécessitent la gestion de métadonnées adéquates pour garantir un accès efficace aux données sur la base un modèle multidimensionnel de métadonnées conçu pour un lac de données présentant un défaut évolutivité de schéma nous proposons utilisation un data vault pour traiter ce problème pour montrer la faisabilité de cette approche nous instancions notre modèle conceptuel de métadonnées en modèles logiques et physiques relationnelet orienté document nous comparons également les modèles physiques en termes de stockage et de temps de réponse aux requêtes sur les métadonnées\n",
      "nfb protocole de notarisation des documents dans la blockchain\n",
      "nan\n",
      "nouveau modèle de sélection de caractéristiques basé sur la théorie des ensembles approximatifs pour les données massives\n",
      "nan\n",
      "palm un algorithme parallèle pour extraire des clusters de liens dans les réseaux sociaux\n",
      "dans cet article nous nous intéressons à optimisation du processus de recherche de clusters de liens nous proposons en particulier algorithme palm stattner et al. 2017) qui vise à améliorer efficacité du processus extraction par exploration conjointe de plusieurs zones de espace de recherche ainsi nous commençons par démontrer que espace des solutions forme un treillis de concepts nous proposons ensuite une approche qui explore en parallèle les branches de ce treillis tout en réduisant espace de recherche en s'appuyant sur différentes propriétés les bonnes performances de notre algorithme sont démontrées en le comparant avec algorithme extraction d'origine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peerus review un outil de recherche experts scientifiques\n",
      "nous proposons un outil de recherche experts appliqué au monde académique sur les données générées par entreprise dsrt dans le cadre de son application peerus 1 un utilisateur soumet le titre le résumé et optionnellement les auteurs et le journal de publication un article scientifique et se voit proposerune liste experts, potentiels reviewers de article soumis algorithme de recherche est un système de votes reposant sur un modèle du langage entrainé à partir un ensemble de plusieurs millions articles scientifiques outil est accessible à chacun sous la forme une application web intitulée peerus review 2\n",
      "perforecast  un outil de prévision de évolution de séries temporelles pour le planning capacitaire\n",
      "nous présentons perforecast un outil qui vise à automatiser le processus de planning capacitaire en utilisant des données temporelles univariées et des modèles prédictifs configurés automatiquement objectif est anticiper les problèmes de dimensionnement dans les infrastructures orange qui assurent la délivrance un service aux clients il s'agira par exemple de prévoir au plus « tôt » la surcharge un serveur afin de commander en avance de nouvelles machines avant la détérioration du service considéré) les démarches de dimensionnent et achat étant longues et coûteuses plus elles sont effectuées tôt meilleure sera la qualité de service\n",
      "prédiction du rayonnement solaire par apprentissage automatique\n",
      "cet article décrit une approche flexible pour la prédiction à court terme de variables météorologiques en particulier nous nous intéressons à la prédiction du rayonnement solaire à une heure cette tâche est une grande importance pratique dans optique optimiser les resources énergétiques solaires comme le défi egc 2018 nous fournit des données météorologiques enregistrées sur cinq sites géographiques de île de la réunion nous utilisons ces données historiques comme base pour créer des modèles de prédiction et nous testons la performance de ces modèles selon le site considéré après avoir décrit notre méthode de nettoyage de données et de normalisation nous combinons une méthode de sélection de variables basée sur les modèles arima autoregressiveintegrated moving average à utilisation de méthodes de régression génériques telles que les arbres de régression et les réseaux de neurones\n",
      "prétraitement de données spatialement imprécises pour une classification supervisée basée sur les images satellitaires\n",
      "dans un problème de classification supervisée les données apprentissage proviennent souvent inventaires acquis sur le terrain par des experts du domaine toutefois la localisation de ces inventaires est approximative en raison de la précision intrinsèque des gps portables utilisés) cette imprécision spatiale est particulièrement problématique lorsque ces données sont utilisées pour entrainer un classifieur sur des images satellitaires très haute résolution(thr) en effet la précision spatiale des inventaires peut être dans certains cas bien inférieure à celles de ces images dans ce papier nous proposons trois approches visant à améliorer la précision spatiale des données terrain via des prétraitements le principe est exploiter les images satellitaires thr disponibles pour corriger spatialement les données terrain nos expérimentations mettent en avant intérêt de ces pré-traitements sur un jeu de données constitué de 24 inventaires habitats coralliens et une image satellitaire thr (worldview-2)\n",
      "prise en compte de la structure des documents pour une indexation performante\n",
      "nan\n",
      "propositions pour améliorer une méthode de prédiction du succès une campagne de financement participatif\n",
      "le financement participatif est un mode de financement un projet faisant appel à un grand nombre de personnes qui a connu une forte croissance avec émergence internet et des réseaux sociaux cependant plus de 60 % des projets ne sont pas financés il est donc important de bien préparer sa campagne de financement de plus en cours de campagne il est crucial avoir une estimation rapide de son succès afin de pouvoir réagir rapidement (restructuration communication  des outils de prédiction sont alors indispensables nous proposons dans cet article plusieurs pistes amélioration pour la prédiction du montant levé lors une campagne de financement participatif en utilisant algorithme k-nn la première proposition consiste à utiliser un algorithme de clustering afin de segmenter ensemble apprentissage et faciliter le passage à l'échelle la seconde proposition consiste à extraire des caractéristiques pertinentes depuis les séries temporelles et les informations sur les campagnes pour avoir une représentation vectorielle\n",
      "qu'est-ce qu'un bon système apprentissage  la réponse a évolué avec le temps et demain\n",
      "apprentissage automatique pardon le « machine learning » a envahi la sphère médiatique grâce à des succès impressionnants comme la victoire une machine au go ou la promesse de véhicules autonomes arrivant très prochainement sur nos routes de fait tant exploitation des données massives que la production de code machine à partir de expérience de la machine plutôt que par des humains met apprentissage automatique au coeur de intelligence artificielle très certainement cela signifie que nous savons répondre à la question « qu'est-ce qu'un bon système apprentissage  » et qu'il ne nous reste plus qu'à en décliner la réponse pour obtenir des systèmes adaptés à chaque domaine applicatif pourtant la réponse à cette question a profondément évolué au cours des 60 dernières années au point que les publications sur apprentissage automatique il y a quelques décennies semblent venir une autre planète et ne sont ailleurs plus enseignés aux étudiants et ceci pas seulement parce que les connaissances passées seraient jugées obsolètes mais parce qu'elles ne semblent pas pertinentes avons-nous donc raison  nos précurseurs avaient-ils tort  et nos successeurs nous citeront-ils dans leurs manuels  dans cette présentation nous examinerons quelques moments clés de histoire de apprentissage automatique correspondant à des tournants dans la manière de considérer ce qu'est un bon système apprentissage. et nous nous demanderons si nous vivons un autre moment charnière dans lequel changent notre perspective la question que nous cherchons à résoudre dans nos recherches les concepts manipulés et la manière écrire nos papiers\n",
      "recommendation-based keyword search over relational databases\n",
      "récemment la recherche par mots-clés dans les bases de données relationnelles a suscité un intérêt grandissant en raison de sa facilité d'utilisation bien que des recherches approfondies fussent dernièrement effectuées dans ce contexte la plupart de ces recherches non seulement nécessitent unaccès préalable aux données ce qui restreint leur applicabilité si cette condition n'est pas vérifiée mais aussi renvoient des réponses très génériques cependant fournir aux utilisateurs des réponses personnalisées est devenu plus que jamais nécessaire en raison de la surabondance de données qui peut déranger l'utilisateur le défi de retourner des réponses pertinentes et personnalisées qui satisfont les besoins des utilisateurs demeure inspiré par application réussie de la technique de filtrage collaboratif dans les systèmes de recommandation nous proposons une nouvelle approche basée sur les mots-clés pour fournir aux utilisateurs des résultats personnalisés basés sur hypothèse que seulement une information sur le schéma de la base de données est disponible\n",
      "reconnaissance et indexation automatique des registres de la chancellerie française 1300-1483)\n",
      "les documents manuscrits sont parmi les témoins les plus importants de histoire européenne ces dernières années importantes collections de manuscrits historiques ont été numérisées et mises à disposition du public et des chercheurs cependant la richesse des informations qu'ils contiennent est encore largement inaccessible car seul les images et quelques méta-données sont disponibles idéal pour les utilisateurs serait de pouvoir faire des recherches textuelles comme pour les livres imprimés modernes (https//booksgooglefr/) si les technologies analyse de documents historiques et de reconnaissance écriture manuscrite sont encore trop peu performantes pour permettre utilisation directe de la transcription brute il est possible de mettre à la disposition des utilisateurs un moteur de recherche textuel basé sur une indexation automatique des images de documents manuscrits cette indexation se base sur une transcription automatique mais tire profit de la capacité de la machine à générer des hypothèses reconnaissance multiples et pondérées cette technologie a permis de rendre accessible pour la première fois à la recherchetextuelle les registres de la chancellerie royale française 1302 -1483) un des corpus de documents historiques les plus emblématiques pour la france ouvrant ainsi la voie à de nouvelles méthodes de recherche en histoire  http//www.himanis.org/\n",
      "reframing for non-linear dataset shift\n",
      "les modèles de classification discriminante supposent que les données de formation et dedéploiement ont les mêmes distributions attributs de données ces modèles donnent des performances très variées lorsqu'ils sont déployés dans des conditions variées avec différentes distributions de données ce phénomène est appelé dataset shift dans cet article nous avons fourni une méthode qui détermine abord s'il y a un changement significatif dans les distributions attributs entre les ensembles de données apprentissage et de déploiement s'il existe un changement dans les données la méthode proposée utilise ensuite une approche de hill climbing pour cartographier ce décalage quelle que soit sa nature c'est-à-dire linéaire ou non linéaire à équation pour la transformation quadratique les résultats expérimentaux sur trois jeux de données réels montrent de forts gains de performance obtenus par la méthode proposée par rapport aux méthodes précédemment établies telles que le reconditionnement et le recadrage linéaire\n",
      "régression laplacienne semi-supervisée pour la reconstitution des dates de pose des réseaux assainissement\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la date de pose est souvent un facteur principal explication de la dégradationdes conduites assainissement pour les gestionnaires de ces réseaux connaître cette information permet ainsi par utilisation de modèles de détérioration de prédire état de santé actuel des conduites non encore inspectées cette connaissance est primordiale pour prendre des décisions dans un contexte de forte contrainte budgétaire objectif est ainsi de reconstituer ces dates de pose à partir des caractéristiques du patrimoine et de son environnement les données à manipuler présentent plusieurs niveaux de complexité importants leurs sources sont hétérogènes leur volume est important et les informations sur leur étiquetage dates) sont limitées  seulement 24 % du linéaire est connu pour les réseaux assainissement de la métropole de lyon la base de données sous-jacente contient les caractéristiques connues des conduites profil géométrique matériau utilisé etc) dans ce papier nous proposons de mesurer effet et impact de quelques méthodes apprentissage statistique semi-supervisé et de proposer ainsi une approche alternative adaptée à la reconstitution de ce type de données\n",
      "réseau bayésien pour la gestion de obsolescence dans une base informations en vue de évaluation du risque de chute des personnes âgées\n",
      "évaluation périodique du risque de chute des personnes âgées requiert des informations fiables et nombreuses comme il n'est pas possible de recueillir régulièrement toutes ces informations les observations sont faites au fil du temps et conservées ce qui entraîne une problématique liée au vieillissement des informations cet article traite de la détection des informations obsolètes dans une base informations sur une personne âgée nous proposons une solution comportant un modèle de connaissances sur les personnes âgées sous forme un réseau bayésien et un module de raisonnement chargé de la détection et de la gestion des contradictions et des doutes sur les informations\n",
      "savoir au dela de voir vision artificielle et raisonnement logique\n",
      "nan\n",
      "sémantique des données observation en neuro-imagerie selon un point de vue réaliste\n",
      "objectif de ce travail est de décrire avec une approche réaliste la signification des données observation en neuro-imagerie sous un format formel pour faciliter leur interprétation par les cliniciens et leur réutilisation dans autres contextes\n",
      "temporal hints in the cultural heritage discourse what can an ontology of time as it is worded reveal\n",
      "dans le champ des sciences patrimoniales la dimension temporelle de information joue un rôle à évidence majeur tant pour interpréter et analyser que pour relier des faits isolés mais la façon dont cette dimension est verbalisée pose des problèmes de formalisation non triviaux pourtant cette verbalisation que on associe souvent au terme-chapeau d'incertitude peut être lue en dissociant une part le caractère mal connu un fait documenté irréductible et les choix faits par le producteur de information pour la relativiser dans cette contribution nous proposons un modèle formel permettant observer et analyser de façon systématique cette couche de verbalisation expérience est menée sur des données fortement hétérogènes souvent origine citoyenne documentant le petit patrimoine matériel et immatériel ce cas étude est donc limité mais il apparait néanmoins comme portant une question de fond allant au-delà du cas d'espèce la contribution détaille abord la grille analyse indices temporels proposée puis relate expérimentation concrète associée ontologie owl) il n'est pas fait état une quelconque prétention à un résultat généralisable stricto sensu mais cette expérience peut contribuer à nourrir de façcon pragmatique un débat nécessaire sur la formalisation indices temporels dans les sciences historiques\n",
      "un modèle bayésien de co-clustering de données mixtes\n",
      "nous proposons un modèle de co-clustering de données mixtes et uncritère bayésien de sélection du meilleur modèle le modèle infère automatiquement les discrétisations optimales de toutes les variables et effectue un coclustering en minimisant un critère bayésien de sélection de modèle un avantage de cette approche est qu'elle ne nécessite aucun paramètre utilisateur de plus le critère proposé mesure de façon exacte la qualité un modèle tout en étant régularisé optimisation de ce critère permet donc améliorer continuellement les modèles trouvés sans pour autant sur-apprendre les données les expériences réalisées sur des données réelles montrent intérêt de cette approche pour analyse exploratoire des grandes bases de données\n",
      "une approche sémantique hybride pour la recommandation des articles actualité à large échelle\n",
      "les portails actualités en ligne produisent un flux information ayant un volume et une vélocité importants dans ce contexte il devient plus difficile de proposer en temps réel des recommandations dynamiques adaptées aux intérêts de chaque utilisateur dans cet article nous présentons une approche hybride pour la recommandation des articles actualité reposant sur analyse sémantique du contenu disponible approche est basée sur hybridation de plusieurs approches personnalisées et non personnalisées pour remédier au problèmede démarrage à froid expérimentation de notre approche dans un environnementà large échelle et à fortes contraintes temps réel dans le cadre du challenge newsreel a permis évaluer la qualité de ses recommandations et de confirmer apport de la sémantique dans le processus de recommandation\n",
      "une méthode pour estimation désagrégée de données de population à aide de données ouvertes\n",
      "nous présentons dans ce travail une méthode de désagrégation pour estimation de population à échelle locale à partir de données ouvertes globales notre but est estimer notamment le nombre de personnes résidant dans chaque bâtiment de la zone d'intérêt à partir de données à plus grande échelle une description fine à échelle résidentielle est tout abord effectuée à partir des données d'openstreetmap les surfaces des bâtiments habitation ou usage mixte habitation et activités sont notamment identifiées nous effectuons ensuite une désagrégation à partir de données de grille de population à grande échelle 1km2 par carreau) guidée par les surfaces des bâtiments compris dans chaque carreau de la grille ensuite nous effectuons une désagrégation à partir de données de grille de population à grande échelle 1km2 par carreau) guidée par les distributions spatiales découvertes à étape précédente nous utilisons exclusivement des données ouvertes pour favoriser la réplicabilité et pour pouvoir appliquer notre méthode à toute région d'intérêt pour peu que la qualité des données soit suffisante évaluation et la validation du résultat dans le cas de plusieurs villes françaises sont effectuées à aide de données de recensement insee\n",
      "unitex/gramlab plateforme libre basée sur des lexiques et des grammaires pour le traitement des corpus textuels\n",
      "objectif de notre recherche est de répondre aux besoins croissants et divers extraction information pertinente exprimés par de nombreuses disciplines nous utilisons pour cela analyseur multilingue de corpus unitex/gram-lab développé à université paris-est marne-la-vallée il fait appel à une approche symbolique et utilise des ressources linguistiques dictionnaires électroniques et grammaires locales cette présentation ne constitue qu'une prise en main unitex/gramlab et ne reflète que très partiellement les possibilités du logiciel et son champ d'utilisation notamment pour extraction information, qui s'étend du monde de la recherche à celui de l'industrie\n",
      "universal-endpoint.com  une plateforme accès simple au web des données\n",
      "universal-endpoint.com est une plateforme web permettant un accèssimple au web des données par trois aspects  i) une plateforme de correspondance,pour accès aux bases du web des données depuis un seul point d'accèscentralisé ii) le langage simpleparql pour une écriture intuitive de requêtes sous forme de triplets à la manière de sparql mais ne nécessitant pas une connaissance préalable des bases du web des données et iii) une aide à la rédaction de requêtes sparql\n",
      "utilisation de techniques de modélisation thématiques pour la détection de nouveauté dans des flux de données textuelles\n",
      "avec avènement des réseaux sociaux et la multiplication des messages produits au sujet des entreprises mieux comprendre les retours clients est devenu un enjeu primordial des techniques de classification automatique et de modélisation thématique permettent ors et déjà observer les principales tendances observées dans ces données il est intéressant dans une optique d'anticipation observer les thématiques émergentes et de les identifier avant qu'elles ne prennent de l'ampleur afin de résoudre cette problématique nous avons étudié la piste de utilisation de modèles lda pour détecter les documents relatifs à ces thématiques émergentes nous avons testé trois systèmes sur plusieurs scénarios arrivées de la nouveauté dans le flux de données nous montrons queles modèles thématiques permettent de détecter cette nouveauté mais que cela dépend du scénario envisagé\n",
      "visualisation dynamique de connaissances  application aux interactions entre facteurs de risque des maladies cardiovasculaires\n",
      "nan\n",
      "a hybrid approach for detecting influencers in social media\n",
      "la détection influenceurs dans les réseaux sociaux s'appuie généralement sur une structure de graphe représentant les utilisateurs et leurs interactions récemment cette tâche a tenu compte en sus de la structure du graphe,du contenu textuel généré par les utilisateurs notre approche s'inscrit dans cette lignée  des informations sont extraites du contenu textuel par des règles linguistiques puis sont intégrées dans un système apprentissage automatique nous montrerons le prototype développé et son interface de visualisation qui facilite interprétation des résultats\n",
      "analyse des dynamiques spatio-temporelles à partir de séries temporelles images satellitaires\n",
      "la télédétection est un domaine qui regroupe les techniques et les outils permettant observation de la terre notamment acquisition images satellitaires la méthode proposée dans cet article permet une analyse automatique de séries temporelles de telles images nos travaux introduisent un nouvelle approche pour analyse et le clustering de séries temporelles images satellitaire (stis) ce processus se divise en deux parties dans un premier temps nous retraçons les changements radiométriques une zone en représentant son évolution au cours du temps par un graphe dit graphe d'évolution dans un deuxième temps nous introduisons une représentation synthétique des graphes évolutions afin de pouvoir appliquer un algorithme de clustering permettant un regroupement par types évolutions identifiées les expérimentations menées nous ont permis de valider notre approche sur une zone d'étude\n",
      "analyse exploratoire de corpus textuels pour le journalisme investigation\n",
      "nous proposons un outil de visualisation analytique conçu pour et avec une journaliste investigation pour exploration de corpus textuels notre outil combine une technique de biclustering disjoint pour extraire des sujets de haut niveau avec une méthode de biclustering non-disjoint pour révéler plus finement les variantes de sujets une vue ensemble des sujets de haut niveau est proposée sous forme une treemap puis une visualisation hiérarchique radiale coordonnée avec une heatmap permet inspecter et de comparer les variantes de sujet et accéder aux contenus origine à la demande\n",
      "anonymiser des données multidimensionnelles à aide du coclustering\n",
      "dans cet article nous proposons une méthodologie pour anonymiser une table de données multidimensionnelles contenant des données individuelles(soit n individus décrits par m variables) objectif est de publier une table anonyme construite à partir une table initiale qui protège contre le risque de réidentification en autres termes on ne doit pas pouvoir retrouver dans les données publiées un individu présent dans la table originale la solution proposée consite à agréger les données à aide une technique de coclustering puis à utiliser le modèle produit pour générer une table de données synthétiques du même format que les données initiales les données synthétiques qui contiennent des individus fictifs peuvent maintenant être publiées les données produites sont évaluées en termes utilité pour différentes tâches de fouille analyse exploratoire classification et de niveau de protection\n",
      "application du coclustering à analyse exploratoire une table de données\n",
      "la classification croisée est une technique analyse non supervisée qui permet extraire la structure sous-jacente existante entre les individus et les variables une table de données sous forme de blocs homogènes cette technique se limitant aux variables de même nature soit numériques soit catégorielles nous proposons de étendre en proposant une méthodologie en deux étapes lors de la première étape toutes les variables sont binarisées selon un nombre de parties choisi par l'analyste par discrétisation en fréquences égales dans le cas numérique ou en gardant les valeurs les plus fréquentes dans le cas catégoriel la deuxième étape consiste à utiliser une méthode de coclustering entre individus et variables binaires conduisant à des regroupements individus une part et de parties de variables autre part nous appliquons cette méthodologie sur plusieurs jeux de donnée en la comparant aux résultats une analyse par correspondances multiples acm appliquée aux même données binarisées\n",
      "application mobile pour évaluation un algorithme de calcul de distance entre des items musicaux\n",
      "les systèmes de recommandation permettent de présenter à un utilisateur des éléments susceptibles de l'intéresser la mise en place de tels systèmes dans les domaines culturels soulève souvent le questionnement de la place de la diversité de la nouveauté et surtout de la découverte nous pensons que être humain bien qu'ayant ordinairement une tendance à se placer dans une zone de confort correspondant à ce qu'il connaît apprécie occasionnellement être poussé à des explorations le faisant sortir de sa routine nous avons développé dans cette optique une méthode basée sur la dissimilarité qui élargit les centres intérêt des utilisateurs nous avons réussi à délimiter une zone intermédiaire entre des items « trop similaires » et des items « trop différents » afin de valider cette hypothèse nous avons développé une application qui permet de tester et de valider cette méthode dans cet article de démonstration nous expliquons le concept de « zone intermédiaire » nous détaillons le fonctionnement de l'application puis nous présentons les résultats obtenus à partir des tests effectués\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apprentissage espaces prétopologiques dans un cadre multi-instance pour la structuration de données\n",
      "nous présentons dans cet article une méthode supervisée de structuration en dag un ensemble d'éléments étant donnés une structure cible et un ensemble de relations sur ces éléments il s'agit apprendre un modèle de structuration par combinaison des relations initiales nous formalisons ce problème dans le cadre de la théorie de la prétopologie qui permet atteindre des modèles de structuration complexes nous montrons que la non-idempotence de la fonction adhérence rentre dans le cadre du formalisme de apprentissage supervisé) multi-instance et nous proposons un algorithme apprentissage reposant sur le dénombrement des «sacs» positifs et négatifs plutôt que sur un ensemble apprentissage standard une première expérimentation de cette méthode est présentée dans un cadre applicatif de fouille de textes consistant à apprendre un modèle de structuration taxonomique un ensemble de termes\n",
      "apprentissage de structures séquentielles pour extraction entités et de relations dans des textes appels offres\n",
      "dans cet article nous présentons une étude exploitant des méthodes apprentissage automatique de structures séquentielles pour extraire des relations sémantiques dans des textes issus de bases appels d'offres une des relations que nous considérons concerne emprise un projet d'aménagement caractérisée par une association entre les concepts qui définissent les infrastructures bâtiments) et les concepts qui définissent leur(s surface(s d'implantation étude propose une analyse comparée approches à base de champs conditionnels aléatoires (crf) de crf ordre supérieur (h-crf) de crf semi-markoviens modèles de markov cachés hmm) et de perceptrons structurés\n",
      "approche préventive pour une gestion élastique du traitement parallèle et distribué de flux de données\n",
      "dans un contexte de traitement de flux de données il est important de garantir à utilisateur des propriétés de performance qualité des résultats et passage à l'échelle mettre en adéquation ressources et besoins pour n'allouer que les ressources nécessaires au traitement efficace des flux est un défi actualité majeur au croisement des problématiques du big data et du green it approche que nous suggérons permet adapter dynamiquement et automatiquement le degré de parallélisme des différents opérateurs composant une requête continue selon évolution du débit des flux traités nous proposons i une métrique permettant estimer activité future des opérateurs selon évolution des flux en entrée ii approche autoscale évaluant a priori intérêt une modification du degré de parallélisme des opérateurs en prenant en compte impact sur le traitement des données dans sa globalité iii grâce à une intégration de notre proposition à apache storm nous exposons des tests de performance comparant notre approche par rapport à la solution native de cet outil\n",
      "cadre evaluation pour la méta analyse de données\n",
      "nan\n",
      "classification ascendante hiérarchique à noyaux et une application aux données textuelles\n",
      "la formule de lance et williams permet unifier plusieurs méthodes de classification ascendante hiérarchique (cah) dans cet article nous supposons que les données sont représentées dans un espace euclidien et nous établissons une nouvelle expression de cette formule en utilisant les similarités cosinus au lieu des distances euclidiennes au carré notre approche présente les avantages suivants une part elle permet étendre naturellement les méthodes classiques de cah aux fonctions noyau autre part elle permet appliquer des méthodes écrêtage permettant de rendre la matrice de similarités creuse afin améliorer la complexité de la cah application de notre approche sur des tâches de classification automatique de données textuelles montre une partque le passage à échelle est amélioré en mémoire et en temps de traitement autre part que la qualité des résultats est préservée voire améliorée\n",
      "classification objets 3d par extraction aléatoire de sous-parties discriminantes pour étude du sous-sol en prospection pétrolière\n",
      "dans cet article nous proposons une nouvelle approche de classification objets 3d inspirée des time series shapelets de ye et keogh (2009) idée est utiliser des sous-surfaces discriminantes pour la classification concernée afin de prendre en compte la nature locale des éléments pertinents cela permet à utilisateur avoir connaissance des sous-parties qui ont été utiles pour déterminer appartenance un objet à une classe les résultats obtenus confirment intérêt de la sélection aléatoire de caractéristiques candidates pour la pré-sélection attributs en classification supervisée\n",
      "classification multi-labels graduée apprendre les relations entre les labels ou limiter la propagation erreur \n",
      "la classification multi-labels graduée est la tâche affecter à chaque donnée ensemble des labels qui lui correspondent selon une échelle graduelle de degrés d'appartenance les labels peuvent donc avoir à la fois des relations ordre et de co-occurrence un côté le fait ignorer les relations entre les labels risque aboutir à des prédictions incohérentes et un autre côté le fait de prendre en compte ces relations risque de propager erreur de prédiction un label à tous les labels qui lui sont reliés les approches de état art permettent soit ignorer les relations entre les labels soit apprendre uniquement les relations correspondant à une structure de dépendance figée approche que nous proposons permet apprentissage des relations entre les labels sans fixer une structure de dépendance au préalable elle est basée sur un ensemble de classifieurs mono-labels un pour chaque label idée est apprendre abord toutes les relations entre les labels y compris les relations cycliques ensuite les dépendances cycliques sont résolues en supprimant les relations intérêt minimal des mesures sont proposées pour évaluer intérêt apprendre chaque relation ces mesures permettent agir sur le compromis entre apprentissage de relations pour une prédiction cohérente et la minimisation du risque de la propagation erreur de prédiction\n",
      "classification parcimonieuse pour aide à la reconnaissance de cibles radar\n",
      "dans le présent papier nous proposons étude et application une nouvelle approche pour aide à la reconnaissance automatique de cibles (atr pour automatic target recognition à partir des images à synthèse ouverture inverse (isar pour inverse synthetic aperture radar) cette approche est composée de deux phases principales dans la première phase nous utilisons deux méthodes statistiques pour extraire les caractéristiques discriminants à partir des images isar nous nous intéressons dans ce travail aux deux descripteurs multi-échelles issus des deux méthodes sift scale-invariant feature transform etla décomposition en ondelettes complexes dt-cwt dual-tree complex wa-velet transform qui sont calculées disjointement ensuite nous modélisons séparément les descripteurs issus des deux méthodes précédentes sift et dt-cwt par la loi gamma les paramètres statistiques estimés sont utilisés pour la deuxième phase dédiée à la classification dans cette deuxième phase une classification parcimonieuse (src pour sparse representation-based classification est proposée afin évaluer et valider notre approche nous avons eu recours aux données réelles images issues une chambre anéchoïque les résultats expérimentaux montrent que approche proposée peut atteindre un taux de reconnaissance élevé et dépasse largement utilisation du même descripteur avec le classifieur machine à vecteurs de support (svm pour support vector machine)\n",
      "co-clustering de données mixtes à base des modèles de mélange\n",
      "la classification croisée co-clustering) est une technique non supervisée qui permet extraire la structure sous-jacente existante entre les lignes et les colonnes une table de données sous forme de blocs plusieurs approches ont été étudiées et ont démontré leur capacité à extraire ce type de structure dans une table de données continues binaires ou de contingence cependant peu de travaux ont traité le co-clustering des tables de données mixtes dans cet article nous étendons utilisation du co-clustering par modèles à blocs latents au cas des données mixtes variables continues et variables binaires) nous évaluons efficacité de cette extension sur des données simulées et nous discutons ses limites potentielles\n",
      "comparaison et évaluation de mesures de similarité entre concepts un treillis\n",
      "cet article se situe dans le cadre de analyse de concepts formels acf) qui fournit des classes les extensions objets partageant des caractères similaires les intensions) une description par des attributs étant associée à chaque classe dans un article récent une nouvelle mesure de similarité entre deux concepts dans un treillis de concepts a été introduite permettant une normalisation par la taille du treillis dans cet article nous comparons cette mesure de similarité avec des mesures existantes soit basées sur la cardinalité des ensembles ou issues de la conception ontologies et basées sur la structure hiérarchique du treillis une comparaison statistique avec des méthodes existantes est effectuée et testée pour leur consistance\n",
      "conception un modèle généraliste pour évaluation un test a/b\n",
      "nan\n",
      "découverte de sous-groupes avec les arbres de recherche de monte carlo\n",
      "découvrir des règles qui distinguent clairement une classe une autre reste un problème difficile de tels motifs permettent de suggérer des hypothèses pouvant expliquer une classe la découverte de sous-groupes subgroup discovery sd) un cadre qui définit formellement cette tâche extraction de motifs est toujours confrontée à deux problèmes majeurs i) définir des mesures de qualité appropriées qui caractérisent la singularité un motif et ii) choisir une heuristique exploration de espace de recherche correcte lorsqu'une énumération complète est irréalisable à ce jour les algorithmes de sd les plus efficaces sont basés sur une recherche en faisceau beam search bs) la collection de motifs extraits manque cependant de diversité en raison de la nature gloutonne de exploration nous proposons ici utiliser une technique exploration récente la recherche arborescente de monte carlo monte carlo tree search mcts) le compromis entre exploitation et exploration ainsi que la puissance de la recherche aléatoire permettent obtenir une solution disponible à tout moment et de surpasser généralement les approches de type bs notre étude empirique,avec plusieurs mesures de qualité sur divers jeux de données de référence et du monde réel démontre la qualité de notre approche\n",
      "deep dive on smart cities by scaling reasoning and interpreting the semantics of iot\n",
      "modern cities are facing tremendous amount of information captured from internal infrastructures and/or exogenous sensors human included this talk presents how big and heterogenous city data has been captured represented unified to serve one of the most pressing city objective improving quality of city in particular how understanding and reducing traffic congestion we will also present lessons learnt from the deployment of our system and experimentation in dublin (ireland) bologna (italy) miami usa) and rio (brazil)\n",
      "défi egc 2017 modélisation cost-sensitive et enrichissement de données\n",
      "la conférence egc'2017 propose un défi dont le contexte est la gestion des espaces verts pour la ville de grenoble et notamment des arbres qui y sont présents objectif est de proposer un modèle basé sur des données fournies qui permettrait de prédire au mieux les arbres malades ainsi que la localisation potentielle de la maladie après avoir obtenu quelques résultats intéressants avec des modèles standards notre approche utilisant un modèle cost-sensitive one against all csoaa) nous permet obtenir une exactitude de 086 uneprécision de 088 et un rappel de 0,91 sur la prédiction unilabel et une précision/rappel micro de 0,82/0,74 ainsi qu'une précision/rappel macro de 0,66/0,46 pour la prédiction multilabel extraction de connaissances pour la tâche 2 nous a permis de mettre en relief intérêt de ajout de données sur la nature des maladies et la concentration de la pollution dans la ville\n",
      "description interactive de intérêt de utilisateur via échantillonnage de motifs\n",
      "la plupart des méthodes extraction de motifs requièrent que utilisateur formalise son intérêt avec une mesure intérêt et des seuils utilisateur est souvent incapable expliciter son intérêt mais il saura juger si un motif donné est pertinent ou non dans cet article nous proposons une nouvelle méthode de découverte de motifs interactive en supposant que seule une partie des données est intéressante pour utilisateur. en intégrant le retour utilisateur de motifs proposés un à un notre méthode vise à échantillonner des motifs avec une probabilité proportionnelle à leur fréquence apparition au sein des transactions implicitement préférées par utilisateur. nous démontrons que notre méthode identifie exactement les transactions implicitement préférées par utilisateur sous réserve de la consistance de ses retours des expérimentations montrent les bonnes performances de approche en terme de précision et rappel\n",
      "détection de fausses informations dans les réseaux sociaux  vers des approches multi-modales\n",
      "nan\n",
      "enhanced user-user collaborative filtering recommendation algorithm based on semantic ratings\n",
      "nan\n",
      "evolution temporelle de communautés représentatives  mesures et visualisation\n",
      "la problématique de ce papier est identifier dans un graphe dynamique les communautés les plus représentatives sur une période donnée de mesurer leur stabilité et en visualiser les évolutions majeures notre cas usage concerne étude de la visibilité médiatique des communautés et des individus grâce aux données relatives aux émissions télévisuelles et radiophoniques entre 2011 et 2015 a partir une détection de communautés sur intégralité de la période nous proposons des mesures de stabilité et activité des communautés et proposons une visualisation de leur évolution temporelle\n",
      "expression des connaissances en langage naturel  singularité et normalité une sélection\n",
      "nan\n",
      "extraction automatique de paysages en imagerie satellitaire et enrichissement sémantique\n",
      "nous présentons ici une méthode originale pour automatisation de la détection de paysages dans une image satellite deux enjeux majeurs apparaissent dans ce processus le premier réside dans la faculté à prendre en compte ensemble des connaissances expertes tout au long du travail analyse de l'image le second est de réussir à structurer et pérenniser ces connaissances de façon à les rendre interopérables et exploitables dans le cadre du web de données nous présentons en quoi la collaboration de plusieurs stratégies alliant les traitements de l'image le calcul de caractéristiques spécifiques et la programmation logique inductive (pli) vient alimenter le processus d'automatisation et comment intégration de la connaissance au travers de la construction ontologies dédiées permet de répondre pleinement à ces enjeux\n",
      "extraction de chroniques discriminantes\n",
      "extraction de motifs séquentiels vise à extraire des comportements récurrents dans un ensemble de séquences lorsque ces séquences sont étiquetées extraction de motifs discriminants engendre des motifs caractéristiques de chaque classe de séquences cet article s'intéresse à extraction des chroniques discriminantes où une chronique est un type de motif temporel représentant des durées inter-évènements quantitatives article présente algorithme dcm dont originalité réside dans utilisation de méthodes apprentissage automatique pour extraire les intervalles temporels les performances computationnelles et le pouvoir discriminant des chroniques extraites sont évalués sur des données synthétiques et réelles\n",
      "extraction de relations pour le peuplement une base de connaissance à partir de tweets\n",
      "dans une base de connaissance les entités se veulent pérennes mais certains événements induisent que les relations entre ces entités sont instables c'est notamment le cas pour des relations entre organisations produits ou marques entités qui peuvent être rachetées dans cet article nous proposons une approche permettant extraire des relations appartenance entre deux entités afin de peupler une base de connaissance extraction des relations à partir une source dynamique informations telle que twitter permet atteindre cet objectif en temps réel approche consiste à modéliser les événements en s'appuyant sur une ressource lexico-sémantique une fois les entités liées au web des données ouvertes en particulier dbpedia) des règles linguistiques sont appliquées pour finalement générer les triplets rdf qui représentent les événements\n",
      "extraction des évolutions récurrentes dans un unique graphe dynamique attribué\n",
      "un grand nombre applications nécessitent analyser un unique graphe attribué évoluant dans le temps cette tâche est particulièrement complexe car la structure du graphe et les attributs associés à chacun de ses noeuds ne sont pas figés dans ce travail nous nous focalisons sur la découverte de motifs récurrents dans un tel graphe ces motifs des séquences de sous-graphes connexes représentent les évolutions récurrentes de sous-ensembles de noeuds et de leurs attributs différentes contraintes ont été définies (eg fréquence volume connectivité non redondance continuité et un algorithme original a été proposé les expérimentations réalisées sur des jeux de données synthétiques et réelles démontrent intérêt de approche proposée et son passage à l'échelle\n",
      "extraction et chaînage supervisés de connaissances un corpus entretiens en histoire des sciences\n",
      "nan\n",
      "extraction et inférence de connaissances à partir assemblages mécaniques définis par une représentation cao 3d\n",
      "extraction de connaissances à partir de modèles géométriques 3d et les raisonnements associés constituent un enjeu important pour permettre le développement ontologies capables de décrire fonctionnellement des produits manufacturés dans ce contexte nous nous appuyons sur la logique déductive apportée par une base de connaissances étroitement couplée à un modeleur géométrique 3d les raisonnements faisant appel au concept de forme 3d restent difficiles à formaliser et les informations géométriques difficiles à extraire nous proposons une formalisation de propriétés telles que à la même forme que est de la même famille que pour montrer comment extraction informations géométriques 3d est reliée à ces propriétés par la suite une formalisation de propriétés telles que est un empilage' est un regroupement est introduite pour montrer les raisonnements qui contribuent à la structuration assemblages 3d ces propriétés sont illustrées à aide un exemple de pompe hydraulique\n",
      "face2graph base de données graphe et visualisation pour annotation archives vidéos\n",
      "nous proposons dans ce travail utiliser la flexibilité des modèles de base de données graphe et la représentation intuitive du réseau social afin de visuellement explorer annoter et vérifier des détections de visages dans une archive de 15 années de journaux télévisés\n",
      "faciliter les contributions personnelles pour préserver la mémoire des événements historiques\n",
      "un aspect essentiel dans la préservation du patrimoine culturel réside dans la collecte et assemblage des témoignages provenant de citoyens ordinaires dans cet article nous présentons une architecture logicielle facilitant la saisie et le partage de témoignages concernant la période de la construction européenne au luxembourg en rédigeant son témoignage utilisateur obtient les résultats une extraction de connaissances sur le contenu saisi indiquant notamment des entités et informations liées\n",
      "génération de rdf à partir de sources de données aux formats hétérogènes\n",
      "contrairement à ce que promeut le web des données les données exposées par la plupart des organisations sont dans des formats non-rdf tels que csv json ou xml de plus sur le web des objets les objets contraints préféreront des formats binaires tels que exi ou cbor aux formats rdf textuels dans ce contexte rdf peut toutefois servir de lingua franca pour interopérabilité sémantique intégration de données aux formats hétérogènes le raisonnement et le requêtage dans ce but plusieurs outils et formalismes permettent de transformer des documents non-rdf vers rdf les plus flexibles étant basés sur des langages de transformation ou de correspondance (grddl xsparql r2rml rml csvw etc) cet article définit un nouveau langage sparqlgenerate qui permet de générer du rdf à partir i) une base de données rdf et ii) un nombre quelconque de documents aux formats arbitraires originalité de sparql-generate est qu'il étend sparql 1.1 et peut donc i) être appris facilement par les ingénieurs de la connaissance familiers de sparql ii) être implémenté au dessus de n'importe quel moteur sparql existant iii) tirer parti des mécanismes extension de sparql pour prendre en compte de futurs formats\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gestion de connaissances en temps réel depuis des flux massifs de données et apprentissage automatique\n",
      "analyse en temps-réel de données massives envoyées par des capteurs a connu ces dernières années un essor important du fait de hétérogénéité de ces données application de modèles de machine learning spécialement calibrés pour des cas usages précis a permis extraire et inférer des informations de très grandes valeurs néanmoins peu de systèmes proposent une implémentation distribuée sur un vrai cluster industriel permettant de tirer profit de capacités de calcul décuplées nous présentons ici une démonstration de détection anomalie sur réseau souterrain eau potable en île-de-france réalisé avec notre plateforme dénotée waves\n",
      "interopérabilité sémantique libérale pour les services et les objets\n",
      "le web des données promeut utilisation de rdf comme modèle pour les données structurées sur le web cependant la majorité des services web consomment et exposent principalement du csv json ou xml des format non-rdf il est peu probable que tous ces services se convertissent un jouraux formats rdf existants ceci est autant plus vrai dans le contexte du web des objets puisque les formats rdf sont pour la plupart textuels alors que les objets contraints préféreront des formats binaires tels que exi ou cbor dans cet article nous proposons une approche pour permettre interopérabilité sémantique de ces services et objets tout en leur laissant la liberté utiliser leurs formats préférés notre approche s'ancre sur les principes de architecture du web et ceux du web des données liées et repose sur la définition de présentationrdf en supposant qu'une présentation rdf soit identifiée par une iri de déréférençable sur le web nous montrons comment avec différents protocole du web un client/serveur peut faire comprendre à autre partie comment le contenu une message peut être interprété en rdf ou généré à partir de rdf nous nommons ceci la négociation de présentation rdf en utilisant ces principes nous montrons comment les services et objets existants pourraient être rendus interopérables à moindre coût sur le web sémantique\n",
      "k-spectral centroïd pour des données massives\n",
      "nous nous intéressons à la classification non supervisée de séries chronologiques pour ce faire nous utilisons algorithme k-spectral centroïd (k-sc) une variante des k-means k-spectral centroïd utilise une mesure de dissimilarité entre séries chronologiques invariante par translation et par changement d'échelle cet algorithme est coûteux en temps de calcul  lors de la phase d'affectation il nécessite de tester toutes les translations possibles pour identifier la meilleure  lors de la phase de représentation le calcul du nouveau barycentre nécessite extraction de la plus petite valeur propre une matrice nous proposons dans ce travail trois optimisations de k-sc identification de la meilleure translation peut être réalisée efficacement en utilisant la transformée de fourier discrète chaque matrice peut être calculée incrémentalement le calcul du nouveau barycentre peut s'effectuer à moindre coût grâce à la méthode de la puissance itérée ces trois optimisations fournissent exactement la même classification que k-sc\n",
      "machine learning based classification of android apps through text features\n",
      "nan\n",
      "machine learning for the semantic web filling the gaps in ontology mining\n",
      "in the semantic web view ontologies play a key role they act as shared vocabularies to be used for semantically annotating web resources and they allow to perform deductive reasoning for making explicit knowledge that is implicitly contained within them however noisy/inconsistent ontological knowledge bases may occur being the web a shared and distributed environment thus making deductive reasoning no more straightforwardly applicable machine learning techniques and specifically inductive learning methods could be fruitfully exploited in this case additionally machine learning methods jointly with standard reasoning procedure could be usefully employed for discovering new knowledge from an ontological knowledge base that is not logically derivable the focus of the talk will be on various ontology mining problems and on how machine learning methods could be exploited for coping with them for ontology mining are meant all those activities that allow to discover hidden knowledge from ontological knowledge bases by possibly using only a sample of data specifically by exploiting the volume of the information within an ontology machine learning methods could be of great help for semi-)automatically enriching and refining existing ontologies for detecting concept drift and novelties within ontologies and for discovering hidden knowledge patterns also possibly exploiting other sources of information) if on one hand this means to abandon sound and complete reasoning procedures for the advantage of uncertain conclusions on the other hand this could allow to reason on large scale and to to dial with the intrinsic uncertainty characterizing the web that for its nature could have incomplete and/or contradictory information\n",
      "mesure de la confiance dans les systèmes information  application aux données de navires\n",
      "ces dernières années la prolifération rapide des capteurs et des objets communicants de tous types a significativement enrichi le contenu des systèmes d'information cependant cela suscite de nouvelles questions quant à la confiance que on peut accorder aux informations et aux sources informations en effet ces sources peuvent être leurrées ou sous emprise un tiers qui falsifie ou altère les informations cet article propose donc aborder la sécurité des systèmes informations sous angle de la confiance dans les sources informations en premier lieu la définition puis évaluation de la confiance dans un réseau hétérogène sont introduits une modélisation des sources est ensuite proposée la confiance dans ces sources informations est abordée au travers de deux caractéristiques la compétence et la sincérité extraction de la confiance est réalisée via un ensemble de mesures de ces deux caractéristiques une expérience basée sur plusieurs sources simulées à partir un jeu de données réelles montrent la pertinance de l'approche approche qui peut être transposée à autres systèmes d'information cette étude est appliquée à analyse des données de navigation et de positionnement un navire\n",
      "mesure de similarité entre treillis basée sur des correspondances explicites\n",
      "ce document se situe dans le cadre de analyse de concepts formels (acf) une méthode de hiérarchisation algébrique des données basée sur la notion intension / extension partageant maximalement attributs et objets nous présentons ici une mesure de similarité basée sur des correspondances entre deux treillis de galois définie par un modèle expressif utilisant des correspondance entre objets et entre attributs des deux treillis un point clé de notre approche est que ces correspondances peuvent ne pas être des fonctions associant un objet (resp attribut un treillis avec plusieurs objets (resp attributs de autre treillis\n",
      "nouveau modèle pour un passage à échelle de la 0-subsomption\n",
      "le test de 0-subsomption opération fondamentale en programmation logique inductive pli) pour tester la validité une hypothèse sur les exemples est particulièrement coûteux ainsi les systèmes apprentissage de pli les plus récent ne passent pas à l'échelle nous proposons donc un nouveau modèle de 0-subsomption fondé sur un réseau d'acteurs dans le but de pouvoir décider la subsomption sur de très grandes clauses\n",
      "optimisation des performances dans les entrepôts de données nosql en colonnes\n",
      "le modèle nosql orienté colonnes propose un schéma de données flexible et hautement dénormalisé dans cet article nous proposons une méthode implantation un entrepôt de données dans un système nosql en colonnes notre méthode est basée sur une stratégie de regroupement des attributs issus des tables de faits et de dimensions sous forme de familles de colonnes nous utilisons deux algorithmes oep et k-means pour évaluer notre méthode nous avons effectué plusieurs tests sur le benchmark tpc-ds au sein du sgbd nosql orienté colonnes hbase avec une architecture de type mapreduce sur une plateforme hadoop\n",
      "pharmacovigilance du web social par une approche fondée sur les bases de connaissances du web sémantique\n",
      "nan\n",
      "porgy  a visual analytics platform for system modelling and analysis based on graph rewriting\n",
      "porgy est un environnement interactif utilisé pour la modélisation de systèmes obtenus à partir de règles de réécriture pilotés à aide de stratégies et basées sur des graphes utilisant des noeuds à ports cette démonstration présente quelques uns des aspects de visualisation analytique proposés par porgy cette dernière facilite la modélisation du système sa simulation ainsi que analyse des résultats à différentes échelles\n",
      "prédiction de défauts dans les arbres du parc végétal grenoblois et préconisations pour les futures plantations\n",
      "nous décrivons dans cet article notre réponse au défi egc 2017 une analyse exploratoire des données a tout abord permis de comprendre les distributions des différentes variables et de détecter de fortes corrélations nous avons défini deux variables supplémentaires à partir des variables du jeu de données plusieurs algorithmes de classification supervisée ont été expérimentés pour répondre à la tâche numéro 1 du défi les performances ont été évaluées par valisation croisée cela nous a permis de sélectionner les meilleurs classifieurs uni-label et multi-label autant sur la tâche uni-label que multi-label le meilleur classifieur dépasse les références environ 2% nous avons également exploré la tâche numéro 2 du défi une part des règles association ont été recherchées autre part le jeu de données a été enrichi avec des connaissances telles que des données climatiques (pluviométrie température vent ou des données taxonomiques dans le domaine de la botanique (famille ordre super-ordre) en outre des données géographiques et cartographiques sont exploitées dans un outil de visualisation une partie des données sur les arbres\n",
      "prédiction du montant levé lors une campagne de financement participatif par la méthode des plus proches voisins\n",
      "le financement participatif est un mode de financement un projet faisant appel à un grand nombre de personnes contrairement aux modes de financement traditionnels il a connu une forte croissance avec émergence internet et des réseaux sociaux cependant plus de 60% des projets ne sont pas financés il est donc important de bien préparer sa campagne de financement de plus en cours de campagne il est crucial avoir une estimation rapide de son succès afin de pouvoir réagir rapidement (restructuration communication  des outils de prédiction sont alors indispensables nous proposons dans cet article une méthode de prédiction du montant final levé lors une campagne de financement participatif utilisant algorithme k-nn  en utilisant historique de campagnes passées nous déterminons celles qui sont les plus similaires à une campagne en cours nous utilisons alors les montants finaux pour faire une estimation nous comparons plusieurs mesures de distance pour déterminer les plus proches voisins nos résultats indiquent que le dernier état une campagne seul est suffisant pour obtenir une bonne prédiction\n",
      "prévision à court terme des flux de voyageurs du réseau ferré urbain  une approche par les réseaux bayésiens dynamiques\n",
      "nous proposons une approche de prévision à court terme des flux de voyageurs du réseau ferré île-de-france basée sur les réseaux bayésiens dynamiques la structure du modèle repose sur les relations de causalité entre les flux adjacents et permet intégrer offre de transport en présence de données manquantes apprentissage est réalisé via algorithme espérance-maximisation em) structurel en appliquant notre approche sur une ligne de métro les résultats obtenus sont globalement supérieurs à ceux des autres méthodes testées\n",
      "prototype de clustering exploratoire pour aide à la segmentation des clients\n",
      "le clustering est une technique largement répandue pour la définition de profils dans le cadre de aide à la gestion de la relation client (crm) cependant les outils classiques sont généralement limités car ils ne prennent pas en compte la connaissance métier de analyste et ne permettent pas exploration interactive des données nous décrivons ici un prototype qui permet à un expert marketing explorer interactivement les données pour la recherche de profils des clients mais aussi analyser les profils construits à aide de différentes visualisations synthétiques et étudier leurs évolutions au cours du temps\n",
      "recommandations et prédictions de préférences basées sur la combinaison de données sémantiques et de folksonomie\n",
      "dans les systèmes de recommandation approche du filtrage sur le contenu est revenue en force face à celle du filtrage collaboratif grâce à arrivée du paradigme de apprentissage profond et des techniques de word embedding dans cette même veine avènement des folksonomies et du web sémantique a apporté une meilleure compréhension des profils des utilisateurs et des caractéristiques des articles à recommander dans cet article nous nous intéressons au domaine musical et nous introduisons un nouveau calcul de mesure de préférences intégrée dans un système de recommandations basées sur le contenu en testant notre approche sur le jeu de données last.fm nous montrons que utilisation de termes issus une folksonomie associés à des informations issues du web sémantique permet améliorer le processus de recommandation musicale\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconnaissance de sections et entités dans les décisions de justice  application des modèles probabilistes hmm et crf\n",
      "une décision de justice est un document textuel rapportant le dénouement une affaire judiciaire les juristes s'en servent régulièrement comme source interprétation de la loi et de compréhension de opinion des juges la masse disponible de décisions exige des solutions automatiques pour aider les acteurs du droit nous proposons adresser certains des défis liés à la recherche et analyse du volume croissant de décisions de justice en france dans un projet plus global la première phase de ce projet porte sur extraction information des décisions dans objectif de construire une base de connaissances jurisprudentielles structurant et organisant les décisions une telle base facilite analyse descriptive et prédictive de corpus de décisions cet article présente une application des modèles probabilistes pour la segmentation des décisions et la reconnaissance entités dans leur contenu (lieu date participants règles de loi ) nos tests montrent avantage approches basées sur les champs aléatoires conditionnels crf) par rapport à des modèles plus simples et rapides basés sur les modèles cachés de markov (hmm) nous présentons ici les aspects techniques de la sélection et annotation du corpus d'apprentissage et la définition de descripteurs discriminants la spécificité des textes est important et doit être prise en compte lors de application de méthodes extraction information dans un domaine spécifique\n",
      "sélection ciblée des descripteurs visuels pour la recherche d'images une approche basée sur les règles association\n",
      "nan\n",
      "sélection et transformation de variables pour la classification multi-label par une approche mdl\n",
      "la classification multi-label est une extension de la classification supervisée au cas de plusieurs labels elle a connu un regain intérêt récent dans la communauté du machine learning de par son utilité dans plusieurs domaines comme pour tout problème de machine learning le besoin de prétraiter les données multi-label est apparu comme une nécessité afin améliorer les performances des classifieurs dans cet article nous introduisons une nouvelle méthode permettant de prétraiter des variables descriptives par discrétisation ou groupement de valeur dans le cas de plusieurs labels à prédire le choix du meilleur prétraitement est posé comme un problème de sélection de modèle et est résolu au moyen une approche bayésienne une étude comparative est réalisée avec autres méthodes de état de art afin de positionner la nouvelle méthode et de montrer intérêt de la sélection de variables pour la classification\n",
      "subspace clustering et visualisation des flux de données\n",
      "dans ce papier nous proposons une nouvelle approche de subspace clustrering pour les flux de données permettant à utilisateur de suivre visuellement le changement dans le comportement du flux cette approche détecte impact des variables sur évolution du flux tout en visualisant les étapes du subspace clustering en temps réel en premier lieu nous appliquons un clustering sur ensemble de variables afin identifer les sous-espaces ensuite un clustering est appliqué sur les individus dans chaque sous-espace\n",
      "suivi de évolution de clusters de liens dans des réseaux sociaux dynamiques\n",
      "de nombreuses méthodes ont été proposées pour extraire des clusters des réseaux sociaux si un travail important est aujourd'hui mené sur la conception de méthodes innovantes capables de rechercher des clusters de nature différente la plupart des approches font hypothèse de réseaux statiques une des récentes méthodes concerne notamment la recherche de liens conceptuels il s'agit une nouvelle approche de clustering de liens qui exploite à la fois la structure du réseau et les attributs des noeuds dans le but identifier des liens fréquents entre des groupes de noeuds au sein desquels les noeuds partagent des attributs communs dans ce travail nous nous intéressons au suivi des liens conceptuels dans des réseaux dynamiques c'est-à-dire des réseaux qui connaissent des changements structurels importants nous cherchons en particulier à comprendre comment les liens conceptuels se forment et évoluent au cours du développement du réseau pour ce faire nous proposons un ensemble de mesures qui visent à capturer des comportements caractérisant évolution de ces clusters notre approche est ainsi utilisée pour comprendre évolution des liens conceptuels extraits sur deux réseaux réels  un réseau de co-auteurs articles scientifiques et un réseau de communications mobiles les résultats obtenus permettent de mettre en lumière des tendances significatives dans évolution des clusters sur ces deux réseaux\n",
      "support uniforme de types de données personnalisés dans rdf et sparql\n",
      "les littéraux sont les noeuds terminaux du modèle de données rdf et permettent encoder des données telles que des nombres \"12.5\"ˆˆxsd:decimal),des dates (\"2017-01-26t23:57:15\"ˆˆxsd:datetime) ou tout autre type information \"vert pomme\"ˆˆex:couleur) les moteurs rdf/sparql savent tester égalité ou comparer les littéraux rdf dont le type de données leur est connu ce qui est le cas de xsd:decimal et xsd:datetime) mais lorsqu'un type de données est inconnu un moteur rdf/sparql comme ex:couleur) il n'a à priori aucun moyen en « découvrir » la sémantique dans cet article nous attaquons ce problème et étudions comment permettre i) aux éditeurs de données de publier la définition de types de données personnalisés sur le web et ii) aux moteurs rdf/sparql de découvrir à la volée ces types de données personnalisés et de les utiliser de manière uniforme nous discutons de différentes solutions possibles qui tirent partie des principes du web des données et détaillons une solution concrète basée sur le déréférencement et le langage javascript suffisemment générique pour être utilisée pour des types de données personnalisés arbitrairement complexes\n",
      "sur évaluation et élaboration un jeu de données de référence de bonne qualité en télédétection\n",
      "en analyse images de télédétection les données de référence venant étiqueter les objets des images y jouent un rôle crucial mais sont parfois imprécises voire incertaines et en nombre limité dans cet article nous préentons une méthodologie pour amélioration de données de référence pour la télédétection en trois étapes  réalignement des données évaluation via crowdsourcing et création un jeu de données de référence de bonne qualité\n",
      "un critère évaluation pour les k-moyennes prédictives\n",
      "algorithme des k-moyennes prédictives est un des algorithmes de clustering prédictif visant à décrire et à prédire une manière simultanée contrairement à la classification supervisée et au clustering traditionnel la performance de ce type algorithme est étroitement liée à sa capacité à réaliser un bon compromis entre la description et la prédiction or à notre connaissance il n'existe pas dans la littérature un critère analytique permettant de mesurer ce compromis cet article a pour objectif de proposer une version modifiée de indice davies-bouldin nommée sdb permettant ainsi évaluer la qualité des résultats issus de algorithme des k-moyennes prédictives cette modification se base sur intégration une nouvelle mesure de dissimilarité permettant établir une relation entre la proximité des observations en termes de distance et leur classe d'appartenance les résultats expérimentaux montrent que la version modifiée de indice db parvient à mesurer la qualité des résultats issus de algorithme des k-moyennes prédictives\n",
      "un générateur de réseaux dynamiques attribués avec structure communautaire\n",
      "nous proposons une nouvelle approche pour générer des graphes dynamiques avec attributs munis une structure communautaire reflétant les propriétés connues des graphes de terrain comme attachement préférentiel ou l'homophilie le générateur développé permet de construire une suite de graphes formant ainsi un réseau dynamique il offre la possibilité de visualiser évolution de ces graphes à travers une interface dédiée cette interface présente aussi plusieurs mesures évaluées sur chacun des graphes du réseau pour vérifier dans quelle mesure les propriétés du réseau sont préservées au cours de son évolution\n",
      "un modèle de factorisation de poisson pour la recommandation de points intérêt\n",
      "explosion des volumes de données circulant sur les réseaux sociaux géo-localisés lbsn) rend possible extraction des préférences des utilisateurs en particulier ces préférences peuvent être utilisées pour recommander à utilisateur des points intérêt en adéquation avec son profil aujourd'hui la recommandation de points intérêt est devenue une composante essentielle des lbsn malheureusement les méthodes de recommandation traditionnelles échouent à s'adapter aux contraintes propres aux lbsn telles que la ”sparsité” très élevée des données ou prendre en compte influence géographique dans ce papier nous présentons un modèle de recommandation basée sur la factorisation de poisson qui offre une solution efficace à ces contraintes nous avons testé notre modèle via des expérimentations sur un jeu de données réaliste issu du lbsn foursquare ces expériences nous ont permis de démontrer une meilleure qualité de recommandation que 3 modèles de l'état-de-l'art\n",
      "une approche extraction de motifs graduels fermés) fréquents sous contrainte de la temporalité\n",
      "la fouille de motifs graduels a pour but la découverte de co-variations fréquentes entre attributs numériques dans une base de données plusieurs algorithmes extraction automatique de tels motifs ont été proposés la principale différence entre ces algorithmes réside dans la sémantique de variation considérée dans certains domaines d'application on trouve des bases de données dont les objets sont munis une relation ordre temporel ainsi du fait de leur sémantique de variation les algorithmes de la littérature sont inadaptés pour de telles données dans ce contexte nous proposons une approche de fouille de motifs graduels sous contrainte ordre temporel qui réduit le nombre de motifs générés une étude expérimentale sur des bases de données paléoécologiques permet apprendre les groupements indicateurs qui modélisent évolution de la biodiversité les connaissances apportées par ces groupements montre intérêt de notre approche pour le domaine environnemental\n",
      "une approche innovante pour la compréhension des comportements de diffusion  personnalité et neutralité\n",
      "nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "une approche logique pour la fouille de règles association\n",
      "la découverte de règles association à partir de données transactionnelles est une tâche largement étudiée en fouille de données les algorithmes proposés dans ce cadre partagent la même méthodologie en deux étapes à savoir énumération des itemsets fréquents suivie par étape de génération de règles dans cet article nous proposons une nouvelle approche basée sur la satisfiabilité propositionnelle pour extraire les règles association en une seule étape pour montrer la flexibilité et la déclarativité de notre approche nous considérons également deux autres variantes à savoir la fouille de règles association fermées et la fouille de règles indirectes les expérimentation sur plusieurs jeux de données montrent que notre approche offre de meilleures performances comparée à des approches spécialisées\n",
      "une approche sociologique de la place des calculs dans les mondes numériques\n",
      "dans cette présentation on souhaite présenter un regard de sociologue sur les transformations sociales politiques et culturelles du développement des mondes numériques dans nos sociétés les enjeux que doivent relever la fabrication environnements informatiques prennent aujourd'hui de plus en plus importance  protection de la vie privée personnalisation des calculs guidage des conduites ouverture des données éthique des automates etc comment nos sociétés réagissent-elles et s'adaptent-elles à ces mutations  dans cette conférence on propose une réflexion sur le rôle joué par les algorithmes du web dans la construction de espace public numérique comment les calculateurs produisent-ils de la visibilité  a partir de quels principes le pagerank de google les métriques du web social ou les outils de recommandation décident-ils de donner la prééminence à telle information plutôt qu'à telle autre  ces différentes familles de calcul cherchent à mesurer et à valoriser des principes différents  la popularité l'autorité la réputation et la prédiction efficace approche proposée dans cette conférence soutient que les manières de calculer enferment des représentations particulières des individus et de leur place dans nos sociétés comprendre les algorithmes c'est aussi un moyen de redonner du pouvoir aux utilisateurs et de favoriser une critique éclairée de la manière dont le calcul s'introduit de plus en plus dans nos vies numériques\n",
      "une mesure expertise pour le crowdsourcing\n",
      "le crowdsourcing un enjeu économique majeur est le fait externaliser une tâche interne une entreprise vers le grand-public la foule c'est ainsi une forme de sous-traitance digitale destinée à toute personne susceptible de pouvoir réaliser la tâche demandée généralement rapide et non automatisable évaluation de la qualité du travail des participants est cependant un problème majeur en crowdsourcing en effet les contributions doivent être contrôlées pour assurer efficacité et la pertinence une campagne plusieurs méthodes ont été proposées pour évaluer le niveau expertise des participants ce travail a la particularité de proposer une méthode de calcul de degrés expertise en présence de données dont ordre de classement est connu les degrés expertise sont ensuite considérés sur des données sans ordre pré-établi cette méthode fondée sur la théorie des fonctions de croyance tient compte des incertitudes des réponses et est évaluée sur des données réelles une campagne réalisée en 2016\n",
      "une métrique de sélection de variables appliquée à la centralité et à la détection des rôles communautaires\n",
      "la f-mesure de trait est une métrique de sélection de variables statistique sans paramètres qui a montré de bonnes performances pour la classification étiquetage de clusters ou encore la mesure de qualité des clusters dans cet article nous proposons évaluer son utilisation dans le contexte des graphes de terrain et de leur structure communautaire pour bénéficier de son système sans paramètres et de ses performances bien évaluées nous étudions donc sur des graphes synthétiques réalistes les corrélations qui existent entre la f-mesure de trait et certaines mesures de centralité mais surtout avec des mesures destinées à caractériser le rôle communautaire des noeuds nous montrons ainsi que cette mesure est liée à la centralité des noeuds du réseau et qu'elle est particulièrement adaptée à la mesure de leur connectivité au regard de la structure de communautés nous observons par ailleurs que les mesures usuelles de détection des rôles communautaires sont fortement dépendantes de la taille des communautés alors que celles que nous proposons sont par définition liées à la densité de la communauté ce qui rend les résultats comparables un réseau à un autre ceci offre donc la possibilité applications comme le suivi temporel de la structure des communautés enfin le processus de sélection appliqué aux noeuds permet de disposer un système universel contrairement aux seuils fixés auparavant empiriquement pour établissement des rôles communautaires\n",
      "une plateforme analyse opinions en temps réel sur twitter avec recommandation\n",
      "nan\n",
      "veille information sur le web avec re-watch\n",
      "les algorithmes apprentissage automatique peuvent être utilisés pour créer des outils de recommandation qui permettent de prédire la pertinence un document pour une thématique de veille donnée en se basant sur les précédents jugements de pertinence donnés pour cette thématique pour autres documents ces outils de recommandation permettent de filtrer dans un flux entrant de documents ceux qui sont susceptibles être pertinents sans que utilisateur ait besoin de déterminer lui-même les mots clefs marquant adéquation un document pour un sujet de la veille bien que cette problématique de rechercheait été abondamment abordée les outils de veille information pour le web intégrant un apprentissage en sont encore à leur balbutiements nous présentons ici application web re-watch permettant la définition un thème de veille la sélection de sources information sur le web relatives à ce thème et adaptation des scores de pertinence des documents aux retours de utilisateur. application permet aussi pour chaque thème une auto-évaluation de la qualité du filtrage et une interrogation du moteur de recherche google cette application encore en cours de développement est néanmoins actuellement fonctionnelle et accessible sur le web à url suivante  http//www.specific searchcom\n",
      "vers un échantillonnage de flux de données transformé\n",
      "nan\n",
      "vers une instance française de nell  chaîne tln multilingue et modélisation ontologie\n",
      "nous présentons les étapes de préparation de la création une instance nouvelle de nell dédiée au français nell est à la fois un processus de lecture et de compréhension automatique du web et un ensemble de base de connaissances de faits en anglais en portuguais et très prochainement en français cette mise en place de la nouvelle instance de nell a donné lieu à amélioration de la chaîne nlp en la généralisant au multilangue ainsi qu'au développement une ontologie par correspondance avec ontologie en anglais nous présenterons le processus de mise en place et de lancement de la nouvelle instance nell français avec interface de visualisation et de supervision humaine des données collectées\n",
      "vipe  un outil interactif de classification multilabel de messages courts\n",
      "nous présentons un outil interactif de classification multilabel développé au sein du groupe orange et utilisé pour analyse d'opinions basé sur un algorithme de factorisation rapide de matrice il permet à un utilisateur importer des textes courts (tweets mails enquêtes ... de définir des labels intérêts « client globalement satisfait » « évoque la rapidité du débit » ... et de proposer pour chaque texte des recommandations de labels et pour chaque label des recommandations de textes\n",
      "“engage moi” from retrieval effectiveness user satisfaction to user engagement\n",
      "the effective prediction of a click remains a primary challenge in the areas of search digital media and online advertising in the context of search satisfying a user's ´ zs information needby returning results that they will click on is an important objective in any information retreival system consequently information retrieval systems have had a long and varied history of how to evaluate their effectiveness of responding to a given query however building such a system that not only only returns relevant results to a user query but also encourages a long-term relationship between the user and the system is far more challenging in this talk we review the current state-of-the-art evaluation approaches for search before exploring other ways of quantifying more long-term engagement measures finally the talk ends with a proposal of how the two approaches can be considered together to create a service that optimises for the query and the longer term engagement aspects\n",
      "a relevant passage retrieval and re-ranking approach for open-domain question answering\n",
      "les systèmes de questions-réponses sqr)s visent à retourner directement des réponses précises à des questions posées en langage naturel extraction et le reclassement des passages sont considérés comme les tâches les plus difficiles dans un sqr typique et exigent encore un effort non trivial dans cet article nous proposons une nouvelle approche pour extraction et le reclassement des passages en utilisant les n-grammes et svm notre système extraction de passages basé sur la technique des n-grammes repose sur une nouvelle mesure de similarité entre un passage et une question les passages extraits sont ensuite réordonnés en utilisant un modèle basé sur ranksvm combinant différentes mesures de similarité afin de retourner le passage le plus pertinent pour une question donnée nos expériences et nos résultats étaient prometteurs et ont démontré que notre approche est concurrentielle\n",
      "adaptation des mappings entre systèmes organisation de la connaissance du domaine biomédical\n",
      "cette thèse de doctorat propose une approche originale pour adapter les mappings basés sur les changements détectés dans évolution de socs du domaine biomédiacal notre proposition consiste à comprendre précisément les mappings entre socs à exploiter les types de changements intervenant lorsque les socs évoluent puis à proposer des actions de modification des mappings appropriées nos contributions sont multiples i) nous avons réalisé un travail expérimental approfondi pour comprendre évolution des mappings entre socs nous proposons des méthodes automatiques ii) pour analyser les mappings affectés par évolution de socs et iii) pour reconnaître évolution des concepts impliqués dans les mappings via des patrons de changement enfin iv) nous proposons des techniques adaptation des mappings à base d'heuristiques nous proposons un cadre complet pour adaptation des mappings appelé dykosmap et un prototype logiciel nous avons évalué les méthodes proposées et le cadre formel avec des jeux de données réelles contenant plusieurs versions de mappings entre socs du domaine biomédical les résultats des expérimentations ont démontré efficacité des principes sous-jacents à approche proposée la maintenance des mappings en grande partie automatique est de bonne qualité\n",
      "analyse activité et exposition de la vie privée sur les médias sociaux\n",
      "anonymous use of social network do not prevent users from privacy risks resulting from infering and cross-checking information published by themselves or their relationhips with this in mind we have conducted a survey in order to measure sensitiveness of personal data published on social media and to analyze the users behaviors we have shown that 76% of internet users that have answered the survey are vulnerable to identity or sensitive data disclosure our study is completed by the description of an automatic procedure that shows how easily these vulnerabilities can be exploited and motivates the need for more advanced protection mechanisms\n",
      "analyse exploratoire par k-coclustering avec khiops coviz\n",
      "en analyse exploratoire identification et la visualisation des interactions entre variables dans les grandes bases de données est un défi dhillon et al. 2003 kolda et sun 2008) nous présentons khiops coviz un outil qui permet explorer par visualisation les relations importantes entre deux ou plusieurs variables qu'elles soient catégorielles et/ou numériques la visualisation un résultat de coclustering de variables prend la forme une grille ou matrice dont les dimensions sont partitionnées les variables catégorielles sont partitionnées en clusters et les variables numériques en intervalles outil permet plusieurs variantes de visualisations à différentes échelles de la grille au moyen de plusieurs critères intérêt révélant diverses facettes des relations entre les variables\n",
      "analyse géographique de séries de publications  application aux conférences egc\n",
      "dans cet article nous présentons une méthodologie originale permettant de faire des analyses scientométriques basées sur trois dimensions (spatiale temporelle et thématique à partir un corpus de publications cette méthodologie comporte 3 étapes  1) la préparation et la validation des données pour compléter les critères usuels tels que les noms d'auteurs affiliation  par des critères spatiaux temporels et thématiques  2) indexation des contenus des publications et métadonnées associées  3) analyse et/ou la recherche information multidimentionnelle les expérimentations sont menées sur la série de publications des conférences egc de 2004 à 2015\n",
      "analyses synchroniques et diachroniques des thématiques egc- défi ecg 2016\n",
      "les articles scientifiques publiés dans les actes des conférences egc qui se déroulent chaque année depuis 2001 constituent la richesse de ces évènements mettant en avant le fer de lance de la recherche francophone portant sur la gestion et extraction de connaissances nous nous sommes penchés sur analyse de ces publications scientifiques afin en extraire essence en termes de thématiques de recherches abordées premièrement nous avons analysé les points communs et les spécificités des publications dans les différentes éditions de la conférence ainsi que les principales différences entre les éditions consécutives puis nous nous sommes intéressés à la façon dont les publications s'articulant autour des thématiques extraites et sur lesquelles nous avons essayé de visualiser une approximation sémantique enfin nous nous sommes intéressé à évolution des thématiques depuis les débuts de cette conférence et jusqu'à édition 2015\n",
      "apprentissage du signal prix de l'électricité arbres de régression séries temporelles et prédictions à long terme\n",
      "predicting the price of the electricity commodity in the long term is a challenge that current techniques do not meet satisfactorily karakatsani et bunn 2010 weron 2014) in this paper we introduce a new regression tree based model that yields good predictions on a long-term period with low computational resources requirements our approach is validated by temporal series collected from an electricity provider\n",
      "approche de clustering de flux basée sur les graphes de voisinage\n",
      "we propose a neighborhood-based approach for data streams clustering instead of processing each new element one by one we propose to process each group of new elements simultaneously a neighborhood-based clustering is applied on each new group we also define an incremental construction method of the neighborhood graph based on the stream evolution to validate the approach we apply it to multiple data sets and we compare it with various stream clustering approaches\n",
      "arbres de modèles et flux de données incomplets\n",
      "model tree is a useful and convenient method for predictive analytics in data streams often this issue is solved by pre-processing techniques applied prior to the training phase of the model in this article we propose a new method that estimates and adjusts missing values before the model tree training a prototype was developed and tested on several data streams\n",
      "associer argumentation et simulation en aide à la décision  illustration en agroalimentaire\n",
      "prendre une décision impliquant plusieurs acteurs aux objectifs divergents nécessite de considérer des informations tant qualitatives – les préférences des acteurs sur les décisions possibles – que quantitatives – les paramètres servant indicateurs pour les acteurs dans cet article nous nous intéressons à association de ces deux types d'approches le modèle qualitatif considéré est l'argumentation le modèle quantitatif simulant les scénarios découlant de chaque décision est la dynamique des systèmes cet article s'intéresse aux éléments permettant de connecter les deux formalismes un exemple en agroalimentaire vient en appui à cette réflexion\n",
      "caractérisation instances apprentissage pour un méta-mining évolutionnaire\n",
      "machine learning has proven to be a powerful tool in diverse fields and is getting more and more widely used by non-experts one of the foremost difficulties they encounter lies in the choice and calibration of the machine learning algorithm to use our objective is thus to provide assistance in the matter using a meta-learning approach based on an evolutionary heuristic we introduce here this approach as a potential solution to the limitation of current data characterization\n",
      "catégorisation et désambiguïsation des intérêts des individus dans le web social\n",
      "cet article présente une approche pour la catégorisation et la désambiguïsation des intérêts que les individus renseignent sur les réseaux sociaux en utilisant wikipédia\n",
      "clustering par apprentissage de distance guidé par des préférences sur les attributs\n",
      "ces dernières années de nombreuses méthodes semi-supervisées de clustering ont intégré des contraintes entre paires objets ou étiquettes de classe afin que le partitionnement final soit en accord avec les besoins de utilisateur pourtant dans certains cas où les dimensions études sont clairement définies il semble opportun de pouvoir directement exprimer des contraintes sur les attributs pour explorer des données de plus une telle formulation permettrait éviter les écueils classiques de la malédiction de la dimensionnalité et de interprétation des clusters cet article propose de prendre en compte les préférences de utilisateur sur les attributs afin de guider apprentissage de la distance pendant le clustering plus précisément nous montrons comment paramétrer la distance euclidienne par une matrice diagonale dont les coefficients doivent être au plus proche des poids fixés par utilisateur cette approche permet ajuster le clustering pour obtenir un compromis entre les approches guidées par les données et par utilisateur nous observons que ajout des préférences est parfois essentiel pour atteindre un clustering de meilleure qualité\n",
      "clustering visuel semi-interactif\n",
      "nous proposons dans cet article une approche de clustering visuel semi-interactif approche proposée utilise la perception visuelle pour guider utilisateur dans le processus interactif les clusters sont extraits de manière successive et itérative puis évalués selon leur ordre d'extraction pour utilisateur,approche semi-interactive permet non seulement évaluer les classes en fonction un critère déterminé mais aussi évaluer influence de extraction un cluster sur ceux précédemment extraits un protocole de test est présenté afin de comparer cette approche avec les approches purement automatiques et purement interactives cet article est un résumé un papier accepté 1 pour un journal international\n",
      "combinaison de méthodes numériques et symboliques pour analyse de données métabolomiques\n",
      "our work consists in developing a workflow using knowledge discovery methodologies to propose advanced predictive biomarkers discovery solutions from metabolomic data we propose to use machine learning algorithms for feature selection and fca for visualization\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concept drift vs suicide comment un peut prévenir l'autre\n",
      "le suicide devient année en année une problématique plus préoccupante les organismes de santé tels que oms se sont engagés à réduire le nombre de suicides de 10% dans ensemble des pays membres ici 2020 si le suicide est généralement un geste impulsif il existe souvent des actes et des paroles qui peuvent révéler un mal être et représenter des signes précurseurs de prédispositions au suicide objectif de cette étude est de mettre en place un système pour détecter semi-automatiquement ces comportements et ces paroles au travers des réseaux sociaux des travaux précédents ont proposé la classification de messages issus de twitter suivant des thèmes liés au suicide  tristesse blessures psychologiques état mental etc dans cette étude nous ajoutons la dimension temporelle pour prendre en compte évolution de état des personnes monitorées nous avons implémenté pour cela différentes méthodes apprentissage dont une méthode originale de concept drift nous avons expérimenté avec succès cette méthode sur des données réelles issues du réseau social facebook\n",
      "construction incrémentale une structure hiérarchique pour exploration visuelle et interactive de larges collections images\n",
      "dans cet article nous étudions de manière conjointe la construction et exploration visuelle une structure de classification pour de très grande base images pour garantir que la structure construite vérifiera les contraintes de taille nécessaires à sa visualisation dans une interface web tout en reflétant les propriétés topologiques des données (clusters) nous combinons la classification hiérarchique de birch balanced iterative reducing and clustering using hierarchies avec la construction de graphes de voisinage  un graphe de voisinage est créé et mis à jour de manière incrémentale pour représenter les fils de chaque noeud de l'arbre de plus un ensemble images représentatives est remonté à chaque noeud interne pour guider utilisateur lors de exploration visuelle de l'arbre ensemble des algorithmes utilisés sont incrémentaux pour gérer insertion de nouvelles images dans la collection nous présentons les premiers résultats sur des dizaines de milliers images qui peuvent être ainsi structurées en une minute de temps de calcul exploration dans interface est fluide grâce aux propriétés de la structure construite\n",
      "contributions à la coloration des hypergraphes basées sur les traverses minimales\n",
      "in this paper we propose two contributions about the determination of chromatic number and the verification of the 2-colorability property we introduce an unreleased relation between the problem of hypergraph coloring and the computation of minimal transversals hypergraph and especially a subset of them thereby we propose two algorithms in order to optimize the verification of the 2-colorability property of hypergraphs and the evaluation of the chromatic number experiments carried out on several types of hypergraphs showed that our algorithm obtains very interesting results\n",
      "découverte de labels dupliqués par exploration du treillis des classifieurs binaires\n",
      "analyse des données comportementales représente aujourd'hui un grand enjeu tout individu génère des traces activité et de mobilité lorsqu'elles sont associées aux individus ou labels qui les ont créées il est possible de construire un modèle qui prédit avec précision appartenance une nouvelle trace sur internet il est cependant fréquent qu'un utilisateur possède différentes identités virtuelles ou labels doublons les ignorer provoque une grande réduction de la précision de identification il est ainsi question dans cet article du problème de déduplication de labels et on présente une méthode originale basée sur exploration du treillis des classifieurs binaires chaque sous-ensemble de labels est classifié face à son complémentaire et des contraintes rendent possible identification des labels doublons en élaguant espace de recherche des expérimentations sont menées sur des données issues du jeu vidéo starcraft 2 les résultats sont de bonne qualité et encourageants\n",
      "découverte de motifs intelligibles et caractéristiques anomalies dans les traces unitaires\n",
      "de nombreuses industries manufacturières s'intéressent aujourd'hui à exploitation des grandes collections de traces unitaires les applications sont multiples et vont du simple \"reporting\" à la détection de fraudes en passant parla gestion de retours ou encore la mise en évidence incohérences dans les circuits de distribution une étape importante consiste à détecter des anomalies dans des collections de traces si les travaux concernant la détection anomalies sont assez nombreux peu permettent de caractériser les anomalies détectées par une description intelligible étant donné un ensemble de traces unitaires nous développons une méthode extraction de motifs pour détecter et contextualiser des comportements non conformes à un modèle expert fourni ou construit à partir des données) le degré anomalie est alors quantifié grâce à la proportion du nombre de mouvements des objets qui ne sont pas prévus dans le modèle expert cette recherche est financée partiellement par un programme industriel qui ne permet ni de dévoiler le contexte concret ni de parler des données réelles ainsi nous validons empiriquement la valeur ajoutée de la méthode proposée par étude de traces de mobilité dans un jeu vidéo  nous pouvons alors discuter un motif qui explicite les raisons de inexpérience de certains joueurs\n",
      "défi egc 2016  analyse par motifs fréquents et topic modeling\n",
      "dans le domaine de analyse de textes extraction de motifs est une technique très populaire pour mettre en évidence des relations fréquentes entre les mots de même les techniques de topic modeling ont largement fait leurs preuves lorsqu'il s'agit de classer automatiquement des ensembles de textes partageant des thématiques similaires ainsi ce papier a pour ambition de montrer intérêt de utilisation conjointe de ces deux techniques afin de mettre en évidence sous la forme un graphe biparti des mots partageant des thématiques similaires mais aussi leurs relations fréquentes intra et inter thématiques les données du défi egc 2016 permettent de valider intérêt de l'approche tout en montrant évolution des thématiques et des mots clés parmi les papiers de la conférence egc sur ces onze dernières années\n",
      "défi egc 2016 vues conceptuelles des collaborations aux conférences egc depuis 2004 une modélisation descriptive\n",
      "dans ce travail nous analysons les données concernant les articles publiés à la conférence egc notre objectif est identifier et de comprendre les tendances en matière de collaborations pour ce faire nous adoptons une modélisation descriptive à travers une approche réseau qui consiste à générer tout abord le réseau de collaborations des auteurs à partir des données nous enrichissons ensuite les noeuds de ce réseau une dizaine attributs individuels extraits à partir des données enfin nous recherchons des cues conceptuelles une approche récente de clustering de liens qui permet de synthétiser des réseaux en mettant en évidence les ensembles attributs retrouvés fréquemment liés dans le réseau les résultats obtenus montrent les tendances existantes dans les comportements de collaborations dans ce papier nous présentons ces tendances et montrons comment elles évoluent selon différents seuils d'extraction\n",
      "détection de données aberrantes à partir de motifs fréquents sans énumération exhaustive\n",
      "la détection de données aberrantes outliers) consiste à détecter des observations anormales au sein des données durant la dernière décennie des méthodes de détection outliers utilisant les motifs fréquents ont été proposées elles extraient dans une première phase tous les motifs fréquents puis assignent à chaque transaction un score mesurant son degré aberration en fonction du nombre de motifs fréquents qui la couvre) dans cet article nous proposons deux nouvelles méthodes pour calculer le score aberration fondé sur les motifs fréquents (fpof) la première méthode retourne le fpof exact de chaque transactions sans extraire le moindre motif cette méthode s'avère en temps polynomial par rapport à la taille du jeu de données la seconde méthode est une méthode approchée où utilisateur final peut contrôler erreur maximale sur estimation du fpof une étude expérimentale montre intérêt des deux méthodes pour les jeux de données volumineux où une approche exhaustive échoue à calculer une solution exacte pour un même nombre de motifs la précision de notre méthode approchée est meilleure que celle de la méthode classique\n",
      "détection de messages falsifiés de localisation de navires\n",
      "the automatic identification system was initially designed for safety purposes however the system is not secured and the messages contain errors and undergo attacks and falsifications this article proposes a methodological approach for the detection of falsified ais messages\n",
      "enrichissement de schéma multidimensionnel en constellation grâce à la classification ascendante hiérarchique\n",
      "les hiérarchies sont des structures cruciales dans un entrepôt de données puisqu'elles permettent agrégation de mesures dans le but de proposer une vue analytique plus ou moins globale sur les données entreposées selon le niveau hiérarchique auquel on se place cependant peu de travaux s'intéressent à la construction de hiérarchies via un algorithme de fouille de données prenant en compte le contexte multidimensionnel de la dimension concernée dans cet article nous proposons donc un algorithme implémenté sur une architecture rolap permettant enrichir une dimension avec des données factuelles\n",
      "évaluation et prédiction de la centralité de groupes de recherche dans un réseau de collaborations scientifiques\n",
      "de nos jours il y a un fort intérêt pour de nouvelles méthodes évaluation des groupes de recherche afin de quantifier impact de leur travail sur toute la communauté scientifique et de tenter de prédire leurs performances dans le futur dans ce contexte nous proposons une nouvelle approche hybride qui mesure la centralité un groupe de chercheurs publiants cette mesure profite de expressivité et de la capacité inférence apportées par une modélisation ontologique des groupes et des thématiques inférées et une modélisation en graphe qui permet explorer les interactions entre ces différents groupes au fil du temps ce modèle permet également de détecter les groupes capables de collaborer avec autres tout en maintenant un haut niveau de production et identifier ceux qui sont plus déterminants sur les thématiques déduites afin de développer des collaborations de recherche plus fructueuses\n",
      "exploration des données du défi egc 2016 à aide un système information logique\n",
      "nous présentons dans cet article les méthodes employées et les résultats obtenus en réponse au défi egc 2016 notre approche repose une part sur des chaînes automatiques de traitements linguistiques en français et en anglais utilisant le plus possible des ressources et outils publics et autre part sur un environnement exploration des données basé sur les systèmes information logique  ces systèmes exploitent une généralisation des treillis de concepts formels appliquée aux données attribut-valeur ou au web sémantique\n",
      "extension de c-sparql pour échantillonnage de flux de graphes rdf\n",
      "les technologies du web sémantique sont de plus en plus utilisées pour la gestion de flux de données plusieurs systèmes de traitement de flux rdf ont été proposés  c-sparql cqels sparqlstream ep-sparql sparkwave etc ces derniers étendent tous à la base le langage interrogation sémantique sparql les données à entrée du système sont volumineuses et générées en continu à un rythme rapide et variable de ce fait le stockage et le traitement de la totalité du flux deviennent coûteux et le raisonnement presque impossible par conséquent le recours à des techniques permettant de réduire la charge tout en conservant la sémantique des données permet optimiser les traitement voire le raisonnement cependant aucune des extensions de sparql n'inclut cette fonctionnalité ainsi dans cet article nous proposons étendre le système c-sparql pour générer des échantillons à la volée sur flux de graphes rdf nous ajoutons trois opérateurs échantillonnage (uniform reservoir et chain à la syntaxe de c-sparql les expérimentations montrent la performance de notre extension en terme de temps d'exécution et de la préservation de la sémantique des données\n",
      "extraction automatique affixes pour la reconnaissance entités nommées chimiques\n",
      "nous détaillerons ici une approche permettant de détecter des affixes à partir de dictionnaires en se basant sur algorithme de la plus longue sous-chaîne commune dans le cadre de la reconnaissance entités nommées chimiques sur chemdner nous verrons ensuite des méthodes de sélection et de tri afin de les intégrer au mieux dans un système apprentissage automatique\n",
      "extraction de clés de liage de données résumé étendu\n",
      "de grandes quantités de données sont publiées sur le web des données les lier consiste à identifier les mêmes ressources dans deux jeux de données permettant exploitation conjointe des données publiées mais extraction de liens n'est pas une tâche facile nous avons développé une approche qui extrait des clés de liage link keys) les clés de liage étendent la notion de clé de algèbre relationnelle à plusieurs sources de données elles sont fondées sur des ensembles de couples de propriétés identifiant les objets lorsqu'ils ont les mêmes valeurs ou des valeurs communes pour ces propriétés on présentera une manière extraire automatiquement les clés de liage candidates à partir de données cette opération peut être exprimée dans analyse formelle de concepts la qualité des clés candidates peut-être évaluée en fonction de la disponibilité cas supervisé ou non cas non supervisé un échantillon de liens la pertinence et de la robustesse de telles clés seront illustrées sur un exemple réel\n",
      "extraction de commentaires utilisateurs sur le web\n",
      "dans cet article nous présentons commentsminer une solution extraction non-supervisée pour extraction de commentaires utilisateurs notre approche se base sur une combinaison de techniques de fouille de sous-arbres fréquents extraction de données et apprentissage de classement nos expérimentations montrent que commentsminer permet de résoudre le problème extraction de commentaires sur 84% un jeu de données représentatif et publiquement accessible loin devant les techniques existantes extraction.\n",
      "extraction de connaissances dans les systèmes information pervasifs par analyse formelle de concepts\n",
      "nous présentons une méthode extraction de connaissances dans des systèmes information pervasifs nous étudions impact du contexte environnement) un utilisateur sur les applications qu'il utilise sur son smartphone notre proposition pour gérer la complexité des données contextuelles repose sur analyse formelle de concepts et les treillis de galois nous nous focalisons sur automatisation du processus interprétation de ces treillis pour généraliser extraction de connaissances et passer à l'échelle nous présentons des métriques originales illustrées sur des données réelles\n",
      "fabrique logicielle de réseaux sociaux spécialisés aspects fonctionnels\n",
      "this paper introduces a software factory for developing social networks this factory takes an abstract social network and creates a concrete one using mechanisms such as sub-typing and behavior overloading\n",
      "fairness-aware data mining\n",
      "in data mining we often have to learn from biased data because for instance data comes from different batches or there was a gender or racial bias in the collection of social data in some applications it may be necessary to explicitly control this bias in the models we learn from the data recently this topic received considerable interest both in the research community as well as more general as witnessed by several recent articles in popular news media such as the new york times in this talk i will introduce and motivate research in fairness-aware data mining different techniques in unsupervised and supervised data mining will be discussed dividing these techniques into three categories algorithms of the first category adapt the input data in such a way to remove harmful biases while the second adapts the learning algorithms and the third category modifies the output models in such a way that its predictions become unbiased furthermore different ways to quantify unfairness and indirect and conditional discrimination will be discussed each with their own pros and cons with this talk i hope to convincingly argument the validity and necessity of this often contested research area\n",
      "fodomust une plateforme pour la fouille de données multistratégie multitemporelle\n",
      "la plateforme fodomust 1 est une implantation concrète des méthodes librairies et interfaces proposées au sein d'icube elle intègre une version multisource de la méthode de classification collaborative multistratégie samarah elle propose aussi un ensemble algorithmes de segmentation soit propres à icube soit faisant appel à l'otb enfin trois interfaces dédiées chacune à un type de données différent permettent une interaction avec l'utilisateur sa principale originalité est qu'elle permet la classification basée sur dtw dynamictimewarping) de données temporelles symboliques ou numériques et de séries temporelles images\n",
      "fouille de motifs séquentiels avec asp\n",
      "cet article présente utilisation de la programmation par ensembles résponses asp) pour répondre à une tâche de fouille de motifs séquentiels la syntaxe de l'asp proche du prolog en fait un langage très pertinent pour représenter des connaissances de manière aisée et ses mécanismes de résolutions basés sur des solveurs efficaces en font une solution alternative aux approches de programmation par contraintes pour la fouille déclarative de motifs nous proposons un premier encodage de la tâche classique extraction de motifs séquentiels et de ses variantes motifs clos et maximaux) nous comparons les performances calculatoires de ses encodages avec une approche de programmation par contraintes les performances obtenues sont inférieures aux approches de programmation par contraintes mais encodage purement déclaratif offre plus de perspectives intégration de connaissances expertes\n",
      "fusion de données redondantes  une approche explicative\n",
      "nous nous intéressons dans le cadre du projet anr qualinca au traitement des données redondantes nous supposons dans cet article que cette redondance a déjà été établie par une étape préalable de liage de données la question abordée est la suivante  comment proposer une représentation unique en fusionnant les \"duplicats\" identifiés  plus spécifiquement comment décider pour chaque propriété de la donnée considérée quelle valeur choisir parmi celles figurant dans les \"duplicats\" à fusionner  quelle méthode adopter dans le but de pouvoir par la suite retracer et expliquer le résultat obtenu de façon transparent et compréhensible par utilisateur  nous nous appuyons pour cela sur une approche de décision multicritère et d'argumentation\n",
      "génération de contraintes pour le clustering à partir une ontologie - application à la classification images satellites\n",
      "utilisation des connaissances a priori peut fortement améliorer la classification non-supervisée injection de ces connaissances sous forme de contraintes sur les données figure parmi les techniques les plus efficaces de la littérature cependant la génération des contraintes est très coûteuse et demande intervention de expert  la sémantique apportée par étiquetage de expert est aussi perdue dans ce type de techniques seuls les contraintes sont retenues par le clustering dans cet article nous proposons une nouvelle approche hybride exploitant le raisonnement à base ontologie pour générer automatiquement des contraintes permettant de guider et améliorer le clustering utilisation une ontologie comme connaissance a priori a plusieurs avantages elle permet interprétation automatisée des connaissances ajoute de la modularité dans la chaîne de traitement et améliore la qualité du clustering en prenant en compte la vision de l'utilisateur pour évaluer notre approche nous avons appliquée à la classification images satellites et les résultats obtenus démontrent des améliorations notables à la fois au niveau de la qualité du clustering et au niveau de étiquetage sémantique des clusters sans intervention de expert.\n",
      "identification de classes sémantiques basée sur des mesures de proximité sémantique\n",
      "semantic relations are the core of a growing number of knowledge-intensive systems the need to validate automatically such relations remains an up-to-date challenge in this paper we present a web-based method enabling the automatic identification of the class of a semantic relation using measures based on syntactic patterns as entry features for a learning algorithm we are able to successfully identify 72% of semantic relations divided in 4 classes in a semantically rich environment\n",
      "intégration de connaissances lexicales et sémantiques pour analyse de sentiments dans les sms\n",
      "with the explosive growth of the social media (forums blogs and social networks on the web the exploitation of these new information sources became essential in this paper we present a new automatic method to integrate knowledge for sentiment detection from a sms corpus by combining lexical and semantic information\n",
      "intégration des influences géographique et temporelle pour la recommandation de points intérêt\n",
      "la recommandation de points intérêts ou poi) est devenue un problème majeur avec émergence des réseaux sociaux ou lbsn) à la différence des approches de recommandation traditionnelles les données des lbsn présentent des caractéristiques géographique et temporelle importantes qui limitent les performances des algorithmes traditionnels existant intégration de ces caractéristiques dans un unique modèle de factorisation pour augmenter la qualité de la recommandation n'a pas été un problème très étudié jusqu'à présent dans ce papier nous présentons geomf-td une extension un modèle de factorisation géographique avec des dépendances temporelles nos expérimentations sur un jeu de données réel montre jusqu'à 20% de gain sur la précision de la recommandation\n",
      "khiops outil apprentissage supervisé automatique pour la fouille de grandes bases de données multi-tables\n",
      "khiops est un outil apprentissage supervisé automatique pour la fouille de grandes bases de données multi-tables importance prédictive des variables est évaluée au moyen de modèles de discrétisation dans le cas numérique et de groupement de valeurs dans le cas catégoriel dans le cas une base multi-tables par exemple des clients avec leurs achats une table analyse individus × variables est produite par construction automatique de variables le modèle de classification utilisé est un classifieur bayésien naïf avec sélection de variables et moyennage de modèles outil est adapté à analyse des grandes bases de données avec des millions d'individus des dizaines de milliers de variables et des centaines de millions enregistrements dans les tables secondaires\n",
      "analyse relationnelle de concepts pour la fouille de données temporelles – application à étude de données hydroécologiques\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cet article présente une méthode exploration de données temporelles fondée sur analyse relationnelle de concepts arc) et appliquée à des données séquentielles construites à partir échantillons physico-chimiques et biologiques prélevés dans des cours d'eau notre but est de mettre au jour des sous-séquences pertinentes et hiérarchisées associant les deux types de paramètres pour faciliter la lecture ces sous-séquences sont représentées sous la forme de motifs partiellement ordonnés (po-motifs) le processus de fouille de données se décompose en plusieurs étapes  construction un modèle temporel ad hoc et mise en oeuvre de arc  extraction des sous-séquences synthétisées sous la forme de po-motifs  sélection des po-motifs intéressants grâce à une mesure exploitant la distribution des extensions de concepts le processus a été testé sur un jeu de données réelles et évalué quantitativement et qualitativement\n",
      "la génération des résumés visuels de flux de données de capteurs météorologiques avec des chorèmes\n",
      "this paper describes a new approach for the automatic generation of visual summaries dealing with cartographic visualization methods and modeling of data coming from sensors in real time for meteorology indeed the concept of chorems seems to be an interesting candidate to visualize real time geographic database summaries\n",
      "la r-confiance pour identification de trajectoires de patients\n",
      "sequential patterns mining consist in identifying frequent sequences of ordered events to solve the problem of the large number of patterns obtained we extend the interest mesure called confidence conventionally used to select association rules to sequential patterns we focused on a case study myocardial infarction (mi) in order to predict the trajectory of patients with mi between 2009 and 2013 the results were submitted to an expert for discussion and validation\n",
      "la révolution de assurance par la donnée  défis scientifiques de extraction à la gestion de connaissances\n",
      "la quantité de données dans notre monde a explosé et analyse de grands ensembles de données – aussi connu dans industrie sous le nom « big data » – deviendra un atout majeur de compétitivité principalement dû à une croissance de productivité et surtout à grâce à plus d'innovation la croissance exponentielle de données est alimentée parla facilité de la captation et par la multiplication de canaux numériques d'acquisition on pense non seulement à tous les processus qui sont informatisés aujourd'hui mais aussi aux médias sociaux et aux objets connectés assurance vie une révolution tout particulière l'assureur traditionnellement gestionnaire du risque en s'appuyant sur une longue expérience qu'on traduirait aujourd'hui par une captation systématique de données est après la révolution numérique partiellement exclus de canaux digitaux ceci est en même temps une menace et une opportunité il s'agit un défi puisque industrie doit réaliser une forte mutation pour se positionner la où la donnée se trouve aujourd'hui ie dans le digital il s'agit une opportunité puisque ces nouvelles données permettront de mieux appréhender les risques et plus particulièrement permettront estimer au plus près les risques à la source plutôt que passer par de variables intermédiaires comme peut être âge pour le risque accident en conduite opportunité est autant plus grande qu'en accédant aux données au plus près des utilisateurs il est possible de faire de la prévention évitant ainsi des accidents coûteux pour l'assureur mais surtout désastreux pour les victimes une fois la révolution engagée ceci implique un certain nombre de transformations dans les processus extraction et gestion de connaissances les défis scientifiques sont nombreux allant de la captation non-intrusive de la donnée à la visualisation et gestion de connaissances extraites en passant par de apprentissage artificiel pour pouvoir servir à de millions utilisateurs simultanément dans cette présentation nous allons couvrir rapidement chacune de ces thématiques avec une attention particulière aux défis scientifiques sous-jacents nous allons illustrer notre propos par un exemple phare de cette révolution  la famille offres assurance dite « pay as you drive » où généralement on obtient une décote ou réduction en fonction de sa façon de conduire nous allons ce que ceci implique en termes extraction et de gestion de connaissances pour conclure il est important de mentionner que cette révolution implique autres challenges cruciaux qui dépassent ce qui est abordé ici en particulier pour ne mentioner que deux grands axes  la protection de la vie privée aussi bien du point de vue technique que juridique  et la transformation de métiers accompagné une pénurie de talents déjà entamé\n",
      "learning from massive incompletely annotated & structured data\n",
      "the maestra project http://maestra-project.eu/) addresses the ambitious task of predicting different types of structured outputs in several challenging settings such as semi-supervised learning mining data streams and mining network data it develops machine learning methods that work in each of these settings as well as combinaisons thereof the techniques developed are applied to problems from the area of biology and bioinformatics sensor data analysis multimedia annotation and retrieval and social network analysis the talk will give an introduction to the project and the topics it addresses an overview of the results of the project and a detailed description of selected techniques and applications semi-supervised learning for structured-output prediction sop) and sop on data streams will be discussed for the task of multitarget regression (mtr) as well as applications of mtr for the annotation/retrieval of images\n",
      "libre protocole de gestion de la cohérence dans les systèmes de stockage distribués\n",
      "nous présentons dans ce papier un protocole de gestion de la cohérence appelé libre adapté aux systèmes de stockage orientés cloud telles que les bases de données nosql) ce protocole garantit accès à la donnée la plus récente tout en ne consultant qu'une seule réplique cet algorithme est évalué par simulation et est également implémenté au sein du système de stockage cassandra les résultats de ces expérimentations ont démontré efficacité de notre approche\n",
      "manipulation interactive ensemble de motifs  application aux parcours hospitaliers\n",
      "dans cette démonstration nous proposons une application de visualisation des résultats de la fouille de données séquentielles pour illustrer le fonctionnement de cette application nous avons utilisé des données pmsi hospitalières plus précisément dans le cas de infarctus du myocarde (im) les résultats obtenus ont été soumis à un spécialiste pour discussion et validation\n",
      "nettoyage de données guidé par la sémantique inter-colonnes\n",
      "today the volume of unstructured and heterogeneous data is exploding coming from mutiple sources with different levels of quality therefore it is very likely to manipulate data without knowledge about their structures and their semantics in fact the meta-data may be insufficient or totally absent data anomalies may be due to the poverty of their semantic descriptions or even the absence of their descriptions we propose an approach to understand better the semantics and the structure of the data it helps to correct the intra-column anomalies homogenization) and then the inter-columns ones caused by the violation of semantic dependencies\n",
      "nouveaux algorithmes de fouilles de données relationnelles de clowdflows\n",
      "clowdflows est un logiciel open source qui permet à un utilisateur de réaliser des processus entiers de fouille de données à partir un navigateur et une connexion internet les calculs sont réalisés dans le “nuage” c'est-à-dire de façon transparente sur plusieurs serveurs exécutant les calculs ou hébergeant les données dans cet article nous rappelons les points forts de clowdflows et nous présentons trois familles algorithmes de fouille de données relationnelles que nous venons y intégrer en effet clowdflows est la seule plateforme web permettant d'exécuter voire comparer plusieurs techniques de fouille de données relationnelles souvent appelée programmation logique inductive\n",
      "nouvelle méthode de calcul de la réputation dans les forums de santé\n",
      "de plus en plus de forums tels que slashdot ou stack exchange proposent des systèmes de réputations qui se basent sur le vote collaboratif les utilisateurs peuvent ainsi donner un score à chaque message posté selon sa pertinence ou son utilité cependant ces fonctionnalités de vote sont rarement utilisées dans de nombreuses communautés en ligne tels que les forums de santé dans ces forums les utilisateurs préfèrent poster un nouveau message exprimant de accord ou du remerciement vis à vis des messages pertinents plutôt que de cliquer sur un bouton de vote dans ce travail nous proposons utiliser ces formes implicites expression de la confiance pour estimer la réputation des utilisateurs dans les forums de santé\n",
      "observations sur les distributions latentes aux matrices laplaciennes de graphes\n",
      "algorithme de clustering spectral permet en principe extraire des clusters de formes arbitraires à partir de données numériques cette propriété a contribué à sa popularité et même si ses bases théoriques sont établies depuis plus une décennie des variantes en ont été proposées jusqu'à récemment son fonctionnement repose sur une transformation vers un espace latent dans lequel des formes de clusters arbitraires sont converties en structures faciles à traiter par un algorithme tel que k-means toutefois les distributions dans cet espace latent n'ont été que peu discutées beaucoup auteurs supposant que les propriétés prédites par la théorie sont vérifiées cet article propose alternativement une approche qualitative pour vérifier si cette structure idéale est eddectivement obtenue en pratique le travail consiste également à identifier les paramètres de variabilité commandant à la transformation vers espace latent via un état de art synthétique de la théorie sous-jacente au clustering spectral les observations tirées de nos expériences permettent identifier les combinaisons de paramètres efficaces et les conditions de cette efficacité\n",
      "persorec  un système personnalisé de recommandations pour les folksonomies basé sur les concepts quadratiques\n",
      "nous proposons un nouveau système appelé persorec afin de personnaliser les recommandations (d'amis de tags ou de ressources faites aux utilisateurs dans les folksonomies la personnalisation des recommandations est réalisée en prenant en compte le profil des utilisateurs cette nouvelle donnée permet de proposer aux utilisateurs des tags ou/et ressources plus adaptées à leurs besoins en plus du profil des utilisateurs nous avons recours à leur historique de partage de tags et de ressources dans le but de regrouper les utilisateurs ayant partagé des tags et des ressources en commun tout en ayant des profils équivalents (i.e. des structures appelées concepts quadratiques) ces deux données prises en compte au moment du processus de recommandation a permis améliorer la qualité des recommandations faites aux utilisateurs persorec est donc capable de générer une recommandation personnalisée pour chaque utilisateur selon le mode de recommandation qu'il désire recommandation d'amis de tags ou de ressources et selon le profil qu'il possède\n",
      "plongement de métrique pour le calcul de similarité sémantique à échelle\n",
      "nous explorons le plongement de la métrique de plus court chemindans hypercube de hamming dans objectif améliorer les performances desimilarité sémantique dans wordnet subercaze et al (2015)) nous montronsque bien qu'un plongement isométrique est impossible en pratique nous obtenonsde très bons plongements non isométriques nous obtenons une améliorationdes performances de trois ordres de grandeur pour le calcul de la similaritéde leacock et chodorow (lch)\n",
      "prédiction de la qualité dans les plateformes collaboratives  une approche générique par les graphes hétérogènes\n",
      "la qualité des contenus sur les plateformes collaboratives est très hétérogène.dans la littérature scientifique les algorithmes analyse structurelleappliqués à la tâche de détection de contenu de qualité reposent généralement surdes graphes définis à partir un seul type de noeuds et de relations pourtant lesgraphes sur lesquels reposent ces récentes plateformes présentent de nombreusessémantiques de noeuds et relations différentes e.g. producteurs/consommateursquestions/réponses etc ces solutions souffrent un manque de généricité et nepeuvent s'adapter facilement à évolution des plateformes nous proposons unemodélisation générique de ces platformes par les graphes hétérogènes pouvantintégrer automatiquement de nouvelles sémantiques de noeuds et de relations unalgorithme de prédiction de qualité des contenus reposant sur ce modèle est proposé.nous montrons qu'il généralise plusieurs travaux de la littérature enfin,en intégrant certaines relations inter-utilisateurs nous montrons que notre solution,évaluée surwikipedia et stack exchange améliore la tâche de détection decontenu de qualité\n",
      "recherche de groupes parallèles en classification non-supervisée\n",
      "dans cet article nous nous intéressons à une situation de classificationnon supervisée dans laquelle nous souhaitons imposer une \"forme\" commune àtous les clusters dans cette approche la \"forme\" commune sera caractérisée parun hyperplan qui sera le même pour tous les groupes à une translation près.les points sont donc supposés être distribués autour hyperplans parallèles lafonction objectif utilisée peut naturellement s'exprimer comme la minimisationde la somme des distances de chaque point à son hyperplan comme pour le casde k-means la résolution est effectuée par alternance de phases affectationde chaque point à hyperplan le plus proche et de phases de calcul de hyperplanqui ajuste au mieux ensemble des points qui lui sont affectés objectifétant obtenir des hyperplans parallèles cette phase de calcul est menée simultanémentpour tous les hyperplans par une méthode de régression\n",
      "régression logistique pour la classification images à grande échelle\n",
      "nous présentons un nouvel algorithme parallèle de régression logistique(par-mc-lr pour la classification images à grande échelle nous proposonsplusieurs extensions de algorithme original de régression logistique àdeux classes pour en développer une version efficace pour les grands ensemblesde données images avec plusieurs centaines de classes nous présentons unnouvel algorithme lr-bbatch-sgd de descente de gradient stochastique de régressionlogistique en batch équilibré avec un apprentissage parallèle approcheun contre le reste multi-classes sur de multiples coeurs les résultats expérimentauxsur des ensembles de données imagenet montrent que notre algorithmeest efficace comparés aux algorithmes de classification linéaires de état de l'art\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "relaxation des requêtes skyline  une approche centrée utilisateur\n",
      "les requêtes skyline constituent un outil puissant pour analyse dedonnées multidimensionnelles et la décision multicritère en pratique le calculdu skyline peut conduire à deux scénarios  soit i) un nombre important objetssont retournés soit ii) un nombre réduit objets sont retournés ce qui peut êtreinsuffisant pour la prise de décisions dans cet article nous abordons le secondproblème et proposons une approche permettant de le traiter idée consiste àrendre le skyline plus permissive en lui ajoutant les objets non skyline les pluspréférés approche s'appuie sur une nouvelle relation de dominance floue appelée«much preferred» un algorithme efficace pour calculer le skyline relaxéest proposé une série expériences sont menées pour démontrer la pertinencede approche et la performance de algorithme proposé\n",
      "requêtes discriminantes pour exploration des données\n",
      "à ère du big data les profils utilisateurs deviennent de plus enplus diversifiés et les données de plus en plus complexes rendant souvent trèsdifficile exploration des données dans cet article nous proposons une techniquede réécriture de requêtes pour aider les analystes à formuler leurs interrogations,pour explorer rapidement et intuitivement les données nous introduisonsles requêtes discriminantes une restriction syntaxique de sql avecune condition de sélection qui dissocie des exemples positifs et négatifs nousconstruisons un ensemble de données apprentissage dont les exemples positifscorrespondent aux résultats souhaités par l'analyste et les exemples négatifs àceux qu'il ne veut pas en utilisant des techniques apprentissage automatique,la requête initiale est reformulée en une nouvelle requête qui amorce un processusitératif exploration des données nous avons implémenté cette idée dansun prototype isql) et nous avons mené des expérimentations dans le domainede l'astrophysique\n",
      "saffiet  un système extraction de règles associations spatiales et fonctionnelles dans les séries de données géographiques\n",
      "nous partons de hypothèse que les dynamiques spatiales et évolutiondes usages des objets géographiques peuvent en partie être explicitées(voire anticipées par leurs différentes évolutions précédentes et les configurationsspatiales dans lesquelles ils se situent aussi afin analyser et comprendreles changements de fonction des objets géographiques au cours du temps et endéduire un modèle prospectif et puis prédictif nous proposons outil saffietqui exploite la recherche des motifs fréquents et des règles d'associations pourextraire des règles évolution régissant les dynamiques spatiales\n",
      "sarem un méta-modèle pour la spécification des processus extraction architectures logicielles\n",
      "we propose a meta-model called sarem that specifies the basic elements of the softwarearchitecture extraction sarem serves as a tool to compare the different software architectureextraction approaches that aim to extract a system architecture from the source code\n",
      "segmentation comportementale à aide des réseaux communautaires\n",
      "la mise en place actions marketing efficaces passe par la segmentationde la clientèle c'est-à-dire que les clients sont regroupés en ensembles homogènesen fonction de leurs habitudes de consommation ce qui rend possibleles actions ciblées ces dernières en personnalisant offre permettent obtenirdes taux de transformation plus importants et de meilleures ventes.dans cet article une méthode originale de segmentation comportementale de laclientèle est présentée elle permet de visualiser les segments de clients à traversdes réseaux de communautés et de déceler aisément des mutations soudainesou graduelles dans les comportements de quelques individus ou un ensembleplus important analyste bénéficie alors une meilleure visibilité et peut adapteroffre à tout moment\n",
      "sélection topologique de variables dans un contexte de discrimination\n",
      "en apprentissage automatique la présence un grand nombre de variablesexplicatives conduit à une plus grande complexité des algorithmes et àune forte dégradation des performances des modèles de prédiction pour cela,une sélection un sous-ensemble optimal discriminant de ces variables s'avèrenécessaire dans cet article une approche topologique est proposée pour la sélectionde ce sous-ensemble optimal elle utilise la notion de graphe de voisinagepour classer les variables par ordre de pertinence ensuite une méthode pas à pasde type ascendante \"forward\" est appliquée pour construire une suite de modèlesdont le meilleur sous-ensemble est choisi selon son degré équivalence topologiquede discrimination pour chaque sous-ensemble le degré équivalence estmesuré en comparant la matrice adjacence induite par la mesure de proximitéchoisie à celle induite par la \"meilleure\" mesure de proximité discriminante ditede référence les performances de cette approche sont évaluées à aide de donnéessimulées et réelles des comparaisons de sélection de variables en discriminationavec une approche métrique montrent une bien meilleure sélection àpartir de approche topologique proposée\n",
      "slider  un raisonneur incrémental évolutif\n",
      "the main drawbacks of current reasoning methods over ontologies are they struggle toprovide scalability for large datasets the batch processing reasoners who provide the bestscalability so far are unable to infer knowledge from evolving data we contribute to solvingthese problems by introducing slider an efficient incremental reasoner slider exhibits a performanceimprovement by more than a 70% compared to the owlim-se reasoner slider isconceived to handle expanding data from streams with a growing background knowledge base.it natively supports \u001adf and rdfs and its architecture allows to extend it to more complexfragments with a minimal effort\n",
      "structures de haies dans un paysage agricole  une étude par chemin de hilbert adaptatif et chaînes de markov\n",
      "dans cet article nous présentons une approche couplant une courberemplissant espace et une chaîne de markov pour analyser des données spatialesconcernant la localisation de haies du fait de hétérogénéité spatiale desdonnées nous utilisons une courbe adaptative de hilbert qui permet de linéariserespace en s'ajustant localement à la densité des données pour ensuite exploiterla séquence produite il est nécessaire de caractériser la distance entre un pointet son prédecesseur sur la courbe ainsi que la densité locale nous proposonsde calculer un temps accès à un point à partir du point précédent en utilisantla notion de profondeur de découpe cette variable couplée avec les variablescaractérisant les haies est ensuite analysée avec un modèle de markov nousprésentons et interprétons les résultats obtenus sur un jeu de données environ10000 segments de haies une zone de la basse vallée de la durance\n",
      "supervision de comportements remarquables objets mobiles à partir du suivi et de analyse de leurs trajectoires spatio-temporelles\n",
      "we propose a new generic knowledge model dedicated to the consideration of temporaland spatial dimensions of moving objects we extend usual approaches to meet the specificityof the representation of moving objects and their trajectories an application on shipping andboat trip scenarii is done\n",
      "tom a library for topic modeling and browsing\n",
      "in this paper we present tom topic modeling) a python libraryfor topic modeling and browsing its objective is to allow for an efficient analysisof a text corpus from start to finish via the discovery of latent topics to thisend tom features advanced functions for preparing and vectorizing a text corpusit also offers a unified interface for two topic models namely lda usingeither variational inference or gibbs sampling and nmf using alternating leastsquarewith a projected gradient method) and implements three state-of-the-artmethods for estimating the optimal number of topics to model a corpus what ismore tom constructs an interactive web-based browser that makes exploringa topic model and the related corpus easy\n",
      "topic modeling and hypergraph mining to analyze the egc conference history\n",
      "dans le cadre du défi proposé à édition 2016 de la conférence egc nous exploitons lesarticles qui y ont été publiés de 2004 à 2015 avec pour but expliquer sa structure et sonévolution a partir des thématiques latentes découvertes et autres propriétés des articles (e.g.auteurs affiliations) nous mettons en lumière des caractéristiques intéressantes des structuresthématique et collaborative d'egc a aide une méthode extraction itemsets dans leshyper-graphes nous mettons aussi en avant des liens latents entre auteurs ou entre thématiquesde plus nous proposons des recommandations auteurs ou de thématiques enfin nous décrivonsune interface web pour explorer les connaissances découvertes\n",
      "towards generic and efficient constraint-based mining a constraint programming approach\n",
      "in today's data-rich world pattern mining techniques allow us to extract knowledge fromdata however such knowledge can take many forms and often depends on the application athand this calls for generic techniques that can be used in a wide range of settings in recentyears constraint programming has been shown to offer a generic methodology that fits manypattern mining settings including novel ones existing constraint programming solvers do notscale very well though in this talk i will review different ways in which this limitation hasbeen overcome often this is through principled integration of techniques and data structuresfrom pattern mining into the constraint solvers\n",
      "transmute  un outil interactif pour assister extraction de connaissances à partir de traces\n",
      "alors que extraction de connaissances à partir de données(ecd est un processus qualifié interactif et d'itératif interactivité desoutils est souvent limitée et son étude est relativement récente elle estpourtant déterminante lors de interprétation pour choisir les motifs quideviendront des connaissances nous proposons transmute un outild'assistance à interprétation dans le processus d'ecd dans le cadre dela recherche épisodes séquentiels à partir de traces la phase interprétationest itérative et à chaque itération les résultats de la fouille sontmis à jour dynamiquement en fonction des interactions avec analyste.des outils de visualisation et des mesures de qualité indépendantes dudomaine permettent de caractériser intérêt des motifs à interpréter pourfaciliter leur choix et accompagner le travail de analyste afin de aider àse focaliser plus rapidement sur les motifs potentiellement intéressants\n",
      "un cadre collaboratif pour la segmentation et la classification images de télédétection\n",
      "dans cet article nous présentons cosc un cadre collaboratif pour lasegmentation et la classification images de télédétection permettant extraireles objets une classe thématique donnée le processus de collaboration estguidé par la qualité des données évaluée par des critères homogénéité ainsique des critères implicitement liés à la sémantique des objets afin extraire uneclasse thématique donnée nos expériences montrent que cosc atteint des bonsrésultats en termes de classification et améliore notablement la segmentation del'image de manière globale\n",
      "un outil exploration pour le défi egc 2016\n",
      "dans le cadre du défi egc 2016 nous avons développé une applicationweb pour explorer les données décrivant les articles publiés depuis 2004 lorsdes conférences egc outil permet de découvrir les thèmes importants qui ontété abordés dans ces papiers de plus il permet de déterminer automatiquementles articles sémantiquement similaires à des thèmes donnés\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "un protocole expérimentation sur les propriétés graphémiques avec algorithme som\n",
      "nous présentons une recherche sur la distribution et la classificationnon-supervisée des graphèmes nous visons à réduire écart entre les résultatsde recherches récentes qui montrent la capacité des algorithmes apprentissageet de classification non-supervisée pour détecter les propriétés de phonèmes etles possibilités actuelles de la représentation textuelle d'unicode nos procéduresdoivent assurer la reproductibilité des expériences et garantir que informationrecherchée n'est pas implicitement présente dans le pré-traitement desdonnées notre approche est capable de catégoriser correctement de potentielsgraphèmes ce qui montre que les propriétés phonologiques sont présentes dansles données textuelles et peuvent être automatiquement extraites à partir desdonnées textuelles brutes en unicode sans avoir besoin de les traduire en représentationsphonologiques\n",
      "un regard lexico-scientométrique sur le défi egc 2016\n",
      "depuis 2001 les conférences egc ont rassemblé 1 782 chercheursautour de extraction et la gestion de connaissances en 2016 associationegc réfléchit à son histoire et se projette en lançant un défi à sa communauté.que peut-on révéler sur la communauté egc via des approches développées enegc  notre étude lexico-scientométrique apporte un éclairage sur les thématiquesdu congrès les lieux de publication investis par ses auteurs ou encore lesauteurs sollicitables comme évaluateurs les résultats sont intégrés à un site websous-tendu par un système information décisionnel\n",
      "une approche basée sur des données mixtes – mesures et estimations – pour la détection de défaillances un système robotisé\n",
      "mettre en place un dispositif de détection de pannes représente denos jours un des défis majeurs pour les constructeurs des systèmes robotisésle processus de détection nécessite utilisation un certain nombre de capteursafin de surveiller le fonctionnement de ces systèmes or le coût ainsi queles contraintes liées à la mise en place de ces capteurs conduisent souvent lesconcepteurs à optimiser leurs nombres ce qui mène à un manque de mesuresnécessaires pour la détection de défaillances une des méthodes pour comblerce manque est estimer les paramètres non mesurables à partir un modèlemathématique décrivant la dynamique du système réel cet article présente uneapproche basée sur des données mixtes données mesurées et données estimées)pour la détection de défaillances dans les systèmes robotisés cette détection esteffectuée en utilisant un classifieur de type arbre de décision les données utiliséespour son apprentissage proviennent des mesures prises sur le système réelces données sont ensuite enrichies par des données estimées en provenance unobservateur basé sur un modèle analytique cet enrichissement sous forme attributssupplémentaires a pour but augmenter la connaissance du classifieursur le fonctionnement du système et par conséquent améliorer le taux de bonnedétection de défaillances une expérience sur un système actionnement unsiège robotisé montrant intérêt de notre approche sera présentée à la fin del'article\n",
      "une approche combinée pour enrichissement ontologie à partir de textes et de données du lod\n",
      "cet article porte sur étiquetage automatique de documents décrivantdes produits avec des concepts très spécifiques traduisant des besoins précisd'utilisateurs la particularité du contexte est qu'il se confronte à une triple difficulté 1 les concepts utilisés pour étiquetage n'ont pas de réalisations terminologiquesdirectes dans les documents 2 leurs définitions formelles ne sontpas connues au départ 3 toutes les informations nécessaires ne sont pas forcémentprésentes dans les documents mêmes pour résoudre ce problème nousproposons un processus annotation en deux étapes guidé par une ontologie.la première consiste à peupler ontologie avec les données extraites des documentscomplétées par autres issues de ressources externes la deuxièmeest une étape de raisonnement sur les données extraites qui recouvre soit unephase apprentissage de définitions de concepts soit une phase applicationdes définitions apprises approche saupodoc est ainsi une approche originaled'enrichissement ontologie qui exploite les fondements du web sémantique,en combinant les apports du lod et outils analyse de texte apprentissageautomatique et de raisonnement l'évaluation sur deux domaines application,donne des résultats de qualité et démontre intérêt de approche.\n",
      "une approche évolution du web de données\n",
      "sharing knowledge and data coming from different sources is one of the biggest advantageof linked data keeping this knowledge graph up to date may take in account both ontologyvocabularies and data since they should be consistent our general problem is to deal with webof data evolution in particular we aim at modifing both levels  a-box and t-box\n",
      "une approche de réduction de dimensionnalité pour agrégation de préférences qualitatives\n",
      "nous présentons une méthode de réduction de dimensionnalité pour des données de préférences multicritères lorsque espace des évaluations est un treillis distributif borné cette méthode vise à réduire la complexité des procédures apprentissage un modèle agrégation sur des données qualitatives ainsi nous considérons comme modèle agrégation intégrale de sugeno apprentissage un tel modèle à partir de données empiriques est un problème optimisation à 2n paramètres où n est le nombre de critères considérés) la méthode de réduction que nous proposons s'appuie sur observation de certaines relations entre les éléments de ces données et nous donnons des premiers résultats d'applications\n",
      "une mesure de similarité entre phrases basée sur des noyaux sémantiques\n",
      "nous proposons une nouvelle approche pour le calcul de similarité sémantique entre phrases en utilisant les noyaux sémantiques qui les composent ces noyaux sous la forme de triplets (sujet verbe et objet sont supposés porteurs de information des phrases dont ils sont extraits sur la base de la comparaison sémantique de noyaux on extrait un ensemble indicateurs descriptifs nous utilisons ensuite un apprentissage automatique sur un benchmark contenant des phrases dont la similarité sémantique a été évaluée par des experts humains afin de déterminer importance de chaque indicateur et de construire ainsi un modèle capable de fournir une mesure de similarité sémantique entre phrases les expérimentations et les études comparatives effectuées avec autres approches permettant estimation des similarités sémantiques entre phrases montrent les bonnes performances de notre approche en se basant sur cette dernière un outil de navigation sémantique est en cours de développement\n",
      "une méthode de découverte de motifs contextualisés dans les traces de mobilité une personne\n",
      "les traces de mobilité générées par les divers capteurs qui nous entourent peuvent être analysées à des fins prédictives et explicatives pour répondre à divers problèmes du quotidien si de nombreuses méthodes ont été proposées pour décrire le comportement un individu de manière globale à partir des transitions entre ses différents points intérêts par exemple via un modèle de markov) peu de travaux cherchent à expliquer de manière locale nous proposons dans cet article une méthode qui permet extraire pour un individu dont on a une trace de mobilité conséquente des motifs de mobilité dits contextualisés chaque motif est composé une description sur ensemble des visites aux différents points intérêt de individu qui maximise une ou plusieurs mesures avec une sémantique particulière le motif décrit une phase sédentaire ou exceptionnel de la mobilité de individu). une expérimentation a été menée à partir de traces de mobilité de véhicules et donne des résultats encourageants\n",
      "une méthode supervisée pour initialiser les centres des k-moyennes\n",
      "au cours des dernières années la classification à base de clustering s'est imposée comme un sujet de recherche important cette approche vise à décrire et à prédire un concept cible une manière simultanée partant du fait que le choix des centres pour algorithme des k-moyennes standard a un impact direct sur la qualité des résultats obtenus cet article vise alors à tester à quel point une méthode initialisation supervisée pourrait aider algorithme des kmoyennes standard à remplir la tâche de la classification à base des k-moyennes\n",
      "vers une approche visual analytics pour explorer les variantes de sujets un corpus\n",
      "our purpose is to implement a visual analytics tool for exploring topic variants in text corpora the overlapping bi-clustering methods extract multiple topics from the documents but the interpretation of the results remains difficult we make the assumption that bi-cluster overlaps are articulation points between high-level topics and their multiple variants and viewpoints we propose to extract and visualize a hierarchical structure of bi-cluster overlaps allowing to explore the corpus and to discover unsuspected viewpoints\n",
      "visualisation interactive de métadonnées pour aider les utilisateurs un logiciel de cartographie statistique à concevoir de meilleures cartes\n",
      "cd7online est application saas de la 7ème version de cartes & données c & d) le logiciel de cartographie statistique décisionnelle et de géomarketing édité par articque c & d permet aux utilisateurs occasionnels de réaliser simplement des cartes à partir de données statistiques et géographiques 25 ans de retours utilisateurs nous ont permis de voir que la qualité des cartes repose en partie sur la bonne connaissance des données dont disposent les utilisateurs et sur leur capacité à choisir des outils analyse et de représentation pertinents pour aider les utilisateurs à mieux comprendre leurs données et à réaliser des cartes de meilleure qualité nous avons développé une brique sémantique avec un outil de visualisation interactif permettant de visualiser les connaissances extraites des espaces de travail des utilisateurs nous décrivons ici application cd7online ainsi que outil de visualisation que nous présenterons lors de la démonstration logicielle\n",
      "a clustering based approach for type discovery in rdf data sources\n",
      "rdf(s)/owl data sources are not organized according to a predefined schema as they are structureless by nature this lack of schema limits their use to express queries or to understand their content our work is a contribution towards the inference of the structure of rdf(s)/owl data sources we present an approach relying on density-based clustering to discover the types describing the entities of possibly incomplete and noisy data sets\n",
      "a framework for mesh segmentation and annotation using ontologies\n",
      "la segmentation et annotation de maillages utilisant la sémantique a été objet un intérêt grandissant avec la démocratisation des techniques de reconstruction 3d une approche classique consiste à réaliser cette tâche en deux étapes tout abord en segmentant le maillage puis en l'annotant cependant cette approche ne permet pas à chaque étape de profiter de l'autre en traitement images quelques méthodes combinent la segmentation et annotation mais ces approches ne sont pas génériques et nécessitent des ajustements implémentation ou des réécritures pour chaque modification des connaissances expertes dans ce travail nous décrivons un cadre de fonctionnement qui mélange segmentation et annotation afin de réduire le nombre étapes de segmentation et nous présentons des résultats préliminaires qui montrent la faisabilité de l'approche notre système fournit une ontologie générique qui décrit sous forme de concepts les propriétés un objet (géométrie topologie etc.) ainsi que des algorithmes permettant de détecter ces concepts cette ontologie peut être étendue par un expert pour décrire formellement un domaine spécifique la description formelle du domaine est alors utilisée pour réaliser automatiquement assemblage de la segmentation et de annotation objets et de leurs propriétés en sélectionnant à chaque étape algorithme le plus pertinent étant données les information sémantiques déjà détectées cette approche originale comporte plusieurs avantages tout abord, elle permet de segmenter et annoter des objets sans aucune connaissance en traitement images ou de maillages en décrivant uniquement les propriétés de objet en terme de concepts ontologiques de plus ce cadre de fontionnement peut facilement être réutilisé et appliqué à différents contextes dès lors qu'une ontologie de domaine a été définie finalement la réalisation conjointe de la segmentation et de annotation permet utiliser une manière efficace la connaissance experte en réduisant les erreurs de segmentation et le temps de calcul en lançant toujours algorithme le plus pertinent\n",
      "analyse des paramètres de recherche d'information etude de influence des paramètres sur les résultats\n",
      "cet article présente une analyse détaillée un ensemble de 2 millions de résultats de recherche information obtenus par différents paramétrages de systèmes de recherche information. plus spécifiquement nous avons utilisé la plateforme terrier et interface rungeneration pour créer différentes exécutions run en anglais en modifiant les modèles indexation et de recherche nous avons ensuite évalué chacun des résultats obtenus selon différentes mesures de performance de recherche information. une analyse systématique a été menée sur ces données afin de déterminer une part quels étaient les paramètres qui ont le plus d'influence autre part quels étaient les valeurs de ces paramètres les plus susceptibles de conduire à de bonnes performances du système\n",
      "analyse et visualisation opinions dans un cadre de veille sur leweb\n",
      "analyse opinions est une tâche qui consiste en identification et la classification de textes subjectifs dans ce travail nous nous intéressons au problème analyse opinions dans un contexte de veille sur le web nous proposons une approche pour visualiser les résultats analyse opinions, basée sur utilisation de termes clés nous décrivons également la plateforme de veille sur le web amiei au sein de laquelle notre approche a été implémentée la démonstration consistera en une expérimentation de la plateforme de veille amiei et du module analyse opinions sur un corpus de tweets politiques\n",
      "analyse olap sur des tweets et des blogs  un retour expérience\n",
      "le projet anr imagiweb dans lequel s'inscrit ce travail s'est donné pour mission étudier les images véhiculées sur internet en se basant sur la détection d'opinions deux cas étude ont été définis  1) le premier vise à répondre aux besoins analyse de chercheurs en science politique grâce à des données issues de twitter durant la campagne présidentielle de 2012  2) le second doit permettre à entreprise française edf évaluer opinion du public en matière de sécurité emploi et de prix à partir de billets de blogs dans cet article nous présentons un retour expérience sur usage de analyse en ligne olap online analytical processing pour des données textuelles mettant en avant intérêt de ce type analyse pour les membres du projet\n",
      "analyse visuelle pour la détection des intrusions\n",
      "la démocratisation d'internet couplée à effet de la mondialisation a pour résultat interconnecter les personnes les états et les entreprises le côté déplaisant de cette interconnexion mondiale des systèmes information réside dans un phénomène appelé \"cybercriminalité\" nous proposons une méthode de visualisation de grands \"graphes\" et exploitation analyses statiques des flux permettant de détecter les comportements anormaux et dangereux afin appréhender les risques une façon compréhensible par tous les acteurs\n",
      "approche extraction de classes interlangues à partir de documents multilingues à base de concepts fermés\n",
      "in this article we highlight the interest and usefulness of formal concept analysis fca) in multilingual document clustering we propose a statistical approach for clustering multilingual documents based on closed concepts and vector model partition the documents of one or more collections an experimental evaluation was conducted on the collection of bilingual documents french-english of clef 2 2003 and showed the merits of this method and the interesting degree of comparability of the obtained bilingual classes\n",
      "approche relationnelle de apprentissage de séquences\n",
      "we observe an increasing amount of sequential data for instance open data sources provide real-time information in order to apply classical learning algorithms sequential data are often modelled in an attribute-value setting using a sliding window in this paper we propose a relational approach a first advantage is to let the relational algorithm choose the length of the window a second advantage is to allow to consider conditions based on the existential quantifier and aggregates a third advantage is to be able to consider several granularities at the same time\n",
      "big data and the dawn of algorithms in everything\n",
      "the mainstream adoption of the internet as a source for knowledge and interaction for the past decades has given rise to new data sources that are characterized by large sizes and rapid creation in addition sensory data from mobile devices and machinery are on the rise with similar characteristics all these sources have the commonality that they will tell us something new or something more detailed than before from a business standpoint these data sources holds the opportunity to create more customized services and improved products in practically anything however they also present a challenge since they are big and typically residing outside the traditional server structure of organizations this talk will explore the challenges of integrating these new so-called big data in decision processes specifically we will explore the paradigm shifts when external data become equally or more important than internal data we will also explore the emerging shift in decision making becoming algorithmic as opposed to human discovery driven\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "big data is all about data that we don't have\n",
      "big data is now becoming a buzz word in information technology industry and research is big data only about large volume of data? and if it is yes why is it suddenly becoming a trend hasn't the growth of data volume been gigantic in the last decade from a research point of view it is not surprising to see researchers from all walks of computer science are trying to align their research to big data for the sake of being trendy the question remains whether it tackles the real big data problems in this talk i will describe the misconceptions of big data present motivating cases and discuss the unavoidable challenges faced by industry and research\n",
      "challenges and opportunities in hci visual analytics and knowledge management for the development of sustainable cities\n",
      "while overtly exposed in the media the challenges faced by our societies to transition towards sustainable energy use are quite formidable a simple visual refresher of the cold hard facts should amply reveal the importance of visualization to assess the situation private companies such as ibm and public research centers are joining forces and investing to design and evaluate novel approaches to build and manage cities defined as the rational organisation of dense human habitat information and communication technologies are certainly part of the answers in particular in areas related to knowledge management data mining hci and social computing illustrated with telltaling examples of research work carried at ibm the cstb and the efficacity institute i will argue that interactive information technologies can help managing the energy transition of cities in 3 key aspects   — to support the city design process notably computer supported tooling and information infrastructure that help taming the complexity of the intertwinning actors and interests at play   — to help understand better the city's dynamics identifiy inefficiencies and reveal optimization opportunities where knowledge management and extraction is crucial   — and foremost to ease the necessary changes that will have to happen in our mobility and housing habits with novel tools and services that alleviate our energy needs\n",
      "choix une mesure de proximité discriminante dans un contexte topologique\n",
      "les résultats de toute opération de classification ou de classement objets dépendent fortement de la mesure de proximité choisie utilisateur est amené à choisir une mesure parmi les nombreuses mesures de proximité existantes or selon la notion équivalence topologique choisie certaines sont plus ou moins équivalentes dans cet article nous proposons une nouvelle approche de comparaison et de classement de mesures de proximité dans une structure topologique et dans un objectif de discrimination le concept équivalence topologique fait appel à la structure de voisinage local nous proposons alors de définir équivalence topologique entre deux mesures de proximité à travers la structure topologique induite par chaque mesure dans un contexte de discrimination nous proposons également un critère pour choisir la \"meilleure\" mesure adaptée aux données considérées parmi quelques mesures de proximité les plus utilisées dans le cadre de données quantitatives le choix de la \"meilleure\" mesure de proximité discriminante peut être vérifié a posteriori par une méthode apprentissage supervisée de type svm analyse discriminante ou encore régression logistique appliquée dans un contexte topologique le principe de approche proposée est illustré à partir un exemple de données quantitatives réelles avec huit mesures de proximité classiques de la littérature des expérimentations ont permis évaluer la performance de cette approche topologique de discrimination en terme de taille et/ou de dimension des données considérées et de sélection de la \"meilleur\" mesure de proximité discriminante\n",
      "classification évidentielle avec contraintes étiquettes\n",
      "ce papier propose une version améliorée de algorithme de classification automatique évidentielle semi-supervisée secm celui-ci bénéficie de introduction de données étiquetées pour améliorer la pertinence de ses résultats et utilise la théorie des fonctions de croyance afin de produire une partition crédale qui généralise notamment les concepts de partitions dures et floues le pendant de ce gain expressivité est une complexité qui est exponentielle avec le nombre de classes ce qui impose en retour utilisation de schémas efficaces pour optimiser la fonction objectif nous proposons dans cet article une heuristique qui relâche la contrainte classique de positivité liée aux masses de croyances des méthodes évidentielles nous montrons sur un ensemble de jeux de données de test que notre méthode optimisation permet accélérer sensiblement algorithme secm avec un schéma optimisation classique tout en améliorant également la qualité de la fonction objectif\n",
      "classification multi-label par raisonnement logique pour indexation sémantique de documents\n",
      "cet article présente une solution centrée sur les ontologies pour la classification multi-label automatique information nécessaire à un système de recommandation informations économiques\n",
      "clustering topologique pour le flux de données\n",
      "actuellement le clustering de flux de données devient le moyen le plus efficace pour partitionner un très grand ensemble de données dans cet article nous présentons une nouvelle approche topologique appelée g-stream pour le clustering de flux de données évolutives la méthode proposée est une extension de algorithme gng growing neural gas pour gérer le flux de données g-stream permet de découvrir de manière incrémentale des clusters de formes arbitraires en ne faisant qu'une seule passe sur les données les performances de algorithme proposé sont évaluées à la fois sur des données synthétiques et réelles\n",
      "cohérence des données de bases rdf en évolution constante\n",
      "le maintien de la qualité et de la fiabilité de bases de connaissances rdf du web sémantique est un problème courant de nombreuses propositions pour intégration de « bonnes » données ont été faites se basant soit sur les ontologies de ces bases soit sur des méta-données additionnelles dans cet article nous proposons une approche originale basée exclusivement sur étude des données de la base le principe est de déterminer si les modifications apportées par la mise à jour candidate rendent la partie ciblée de la base plus similaire – selon certains critères – à autres parties existantes dans la base la mise à jour est considérée cohérente avec cette base et peut être appliquée\n",
      "comparison of linear modularization criteria using the relational formalism an approach to easily identify resolution limit\n",
      "la modularisation de grands graphes ou recherche de communautés est abordée comme optimisation un critère de qualité un des plus utilisés étant la modularité de newman-girvan autres critères ayant autres propriétés aboutissent à des solutions différentes dans cet article nous présentons une réécriture relationnelle de six critères linéaires zahn-condorcet owsi´nski- zadro&#729;zny ecart à uniformité, ecart à indétermination et la modularité equilibrée nous utilisons une version générique de algorithme optimisation de louvain pour approcher la partition optimale pour chaque critère sur des réseaux réels de différentes tailles les partitions obtenues présentent des caractéristiques différentes concernant notamment le nombre de classes le formalisme relationnel nous permet de justifier ces différences un point de vue théorique en outre cette notation permet identifier facilement les critères ayant une limite de résolution phénomène qui empêche en pratique la détection de petites communautés sur de grands graphes) une étude de la qualité des partitions trouvées dans les graphes synthétiques lfr permet de confirmer ces résultats\n",
      "compromis précision-rappel dans évaluation des performances \n",
      "dans de nombreux problèmes apprentissage automatique la performance des algorithmes est évaluée à aide des mesures précision et rappel or ces deux mesures peuvent avoir une importance très différente en fonction du contexte dans cet article nous étudions le comportement des principaux indices de performance en fonction du couple précision-rappel nous proposons un nouvel outil de visualisation de performances et définissons espace de compromis qui représente les différents indices en fonction du compromis précision-rappel nous analysons les propriétés de ce nouvel espace et mettons en évidence ses avantages par rapport à espace précision-rappel\n",
      "contribution au calcul du skyline par réduction de espace candidat\n",
      "opérateur skyline est devenu un paradigme dans les bases de données il consiste à localiser sky ensemble des points un espace vectoriel qui ne sont pas dominés cet opérateur est utile lorsqu'on n'arrive pas à se décider dans les situations conflictuelles le calcul des requêtes skyline est pénalisé par le nombre de points que peuvent contenir les bases de données dans ce papier nous présentons une solution analytique pour la réduction de espace candidat et nous proposons une méthode efficace pour le calcul de ce type de requêtes\n",
      "d113  une plateforme open-source dédiée à analyse des flux et à la détection des intrusions\n",
      "ce travail se situe dans le domaine de la \"cybersécurité\" le projet \"d113\" permet de visualiser en temps réel les flux transitant sur des équipements de filtrage sans avoir recours au traitement manuel des journaux d'événements nous centrerons notre démonstration sur la visualisation de grands \"graphes\" et exploitation analyses statiques des flux\n",
      "découverte de proportions analogiques dans les bases de données  une première approche\n",
      "cet article présente un nouveau cadre pour la découverte de connaissances basé sur la notion de proportion analogique qui exprime égalité des rapports entre les attributs de deux paires d'éléments cette notion est développée dans le contexte des bases de données pour découvrir des parallèles dans les données dans un premier temps nous donnons une définition formelle des proportions analogiques dans le cadre des bases de données relationnelles puis nous étudions le problème de extraction des proportions analogiques nous montrons qu'il est possible de suivre une approche de clustering pour découvrir les classes équivalence de paires de n-uplets dans le même rapport de proportion analogique ce travail constitue uncet article présente un nouveau cadre pour la découverte de connaissances basé sur la notion de proportion analogique qui exprime égalité des rapports entre les attributs de deux paires d'éléments cette notion est développée dans le contexte des bases de données pour découvrir des parallèles dans les données dans un premier temps nous donnons une définition formelle des proportions analogiques dans le cadre des bases de données relationnelles puis nous étudions le problème de extraction des proportions analogiques nous montrons qu'il est possible de suivre une approche de clustering pour découvrir les classes équivalence de paires de n-uplets dans le même rapport de proportion analogique ce travail constitue une première étape vers extension des langages interrogation de base de données avec des requêtes « analogiques »e première étape vers extension des langages interrogation de base de données avec des requêtes « analogiques »\n",
      "détection automatique de reformulations - correspondance de concepts appliquée à la détection du plagiat\n",
      "dans le cadre de la détection du plagiat la phase de comparaison de deux documents est souvent réduite à une comparaison mot à mot une recherche de « copier/coller » dans cet article nous proposons une approche naïve de comparaison de deux documents dans le but de détecter automatiquement aussi bien les phrases copiées de un des textes dans autre que les paraphrases et reformulations ceci en se focalisant sur existence des mots porteurs de sens ainsi que sur leurs mots de substitution possibles nous comparons trois algorithmes utilisant cette approche afin de déterminer la plus efficace pour ensuite évaluer face à des méthodes existantes objectif est de permettre la détection des similitudes entre deux textes en utilisant uniquement des mots clefs approche proposée permet de détecter des reformulations non paraphrastiques impossibles à détecter avec des approches conventionnelles faisant appel à une phase d'alignement\n",
      "détection et regroupement automatique de style écriture dans un texte\n",
      "la détection de plagiat extrinsèque devient vite inefficace lorsque on n'a pas accès aux documents potentiellement sources du plagiat ou lorsque on se confronte à un espace aussi vaste que leweb ce qui est souvent le cas dans les logiciels anti-plagiat actuels dès lors la détection intrinsèque devient nettement plus efficace dans cet article nous traitons justement de la détection automatique auteurs qui permet de savoir si un passage un texte n'appartient pas au même auteur que le reste du texte et donc en théorie de repérer les passages plagiés un document nous expliquons notre contribution aux procédures déjà existantes et évaluons les limites de notre approche objectif est de permettre la détection et le regroupement de passages un document par auteur\n",
      "deux approches pour catégoriser le risque\n",
      "le risque chimique ou alimentaire couvre les situations où les produits chimiques sont dangereux pour la santé et consommation humaine ou animale et pour l'environnement les experts qui assurent le contrôle et la gestion de ces substances se retrouvent face à de gros volumes de littérature scientifique qui doit être analysée pour appuyer la prise de décisions nous proposons une aide automatique pour analyse de cette littérature nous abordons la tâche comme une problématique de catégorisation il s'agit de catégoriser les phrases des textes dans les classes du risque lié aux substances nous utilisons deux approches par apprentissage supervisé et la recherche information les résultats obtenus avec apprentissage supervisé toute classe confondue f-mesure autour de 0,8 pour le risque alimentaire entre 0,61 et 0,64 pour le risque chimique sont meilleurs que ceux obtenus avec par recherche information toute classe confondue f-mesure entre 0,18 et 0,226 pour le risque alimentaire entre 0,20 et 0,32 pour le risque chimique. le rappel est compétitif avec les deux approches\n",
      "échantillonnage de flux de données sémantiques  une approche orientée graphe\n",
      "nowadays processing online massive data streams with special techniques like load shedding is an unavoidable alternative to optimize system resources use in this paper we propose a graph-oriented approach for load shedding semantic data streams our approach unlike the rdf triple based one preserves the semantic level of the data streams which improves the responses quality of the rdf data stream processing systems\n",
      "etude de la pertinence lors de la sélection de collections dans les systèmes distribués\n",
      "this paper presents a new function of collection selection our function is free of any extracollection parameter and is based on the documents relevance the ranking of a collection is proportional to its number of relevant documents\n",
      "extraction complète efficace de chemins pondérés dans un a-dag\n",
      "un nouveau domaine de motifs appelé chemins pondérés condensés a été introduit en 2013 lors de la conférence ijcai le contexte de fouille est alors un graphe acyclique orienté dag) dont les sommets sont étiquetés par des attributs nous avons travaillé à une implémentation efficace de ce type de motifs et nous montrons que algorithme proposé était juste mais incomplet nous établissons ce résultat incomplétude et nous expliquons avant de trouver une solution pour réaliser une extraction complète nous avons ensuite développé des structures complémentaires pour calculer efficacement tous les chemins pondérés condensés algorithme est amélioré en performance de plusieurs ordres de magnitude sur des jeux de données artificiels et nous appliquons à des données réelles pour motiver qualitativement usage des chemins pondérés\n",
      "extraction de intérêt implicite des utilisateurs dans les attributs des items pour améliorer les systèmes de recommandations\n",
      "les systèmes de recommandation ont pour objectif de sélectionner et présenter abord les informations susceptibles intéresser les utilisateurs ce travail expose un système de recommandation qui s'appuie sur deux concepts des relations sémantiques sur les données et une technique de filtrage collaboratif distribué basée sur la factorisation des matrices (mf) une part les techniques sémantiques peuvent extraire des relations entre les données et par conséquent améliorer la précision des recommandations autre part mf donne des prévisions très précises avec un algorithme facilement parralélisable notre proposition utilise cette technique en ajoutant des relations sémantiques au processus en effet nous analysons en profondeur les intérêts cachés des utilisateurs dans les attributs des items à recommander nous utilisons dans nos expérimentations le jeu de données movielens enrichi par la base de données imdb nous comparons notre travail à une technique mf classique les résultats montrent une précision dans les recommandations tout en préservant un niveau élevé abstraction du domaine en outre nous améliorons le passage à échelle du système en utilisant des techniques parallélisables\n",
      "feedback - study and improvement of the random forest of the mahout library in the context of marketing data of orange\n",
      "apprentissage automatique a fait son apparition dans écosystème hadoop créant de par la puissance promise une opportunité sans précédent pour ce domaine dans cet écosystème apache mahout est une réponse à la question du temps de calcul et/ou de la volumétrie il consiste en un entrepôt algorithmes apprentissage automatique tous portés afin de s'exécuter sur map/reduce ce rapport se concentre sur le portage et utilisation de algorithme des random forest dans mahout il montre à travers notre retour expérience les difficultés qui peuvent être rencontrées tant pratiques que théoriques et suggère une piste d'amélioration\n",
      "gapit  un outil visuel pour imputation de valeurs manquantes en hydrologie\n",
      "les données manquantes sont problématiques en hydrologie car elles gênent le calcul de statistiques interannuelles et sur de longues périodes ainsi que analyse et interprétation de la variabilité des données dans cet article nous présentons gapit une plateforme analyse de données permettant inspecter visuellement les données manquantes et ensuite de choisir la méthode de correction adéquate nous avons utilisé outil pour estimer les données manquantes dans des séries temporelles correspondant aux débits mesurés par des stations hydrométriques du luxembourg\n",
      "gestion de incertitude dans le cadre une extraction des connaissances à partir de texte\n",
      "the knowledge representation area needs some methods that allow to detect and handle uncertainty indeed a lot of text hold information whose the veracity can be called into question these information should be managed efficiently in order to represent the knowledge in an explicit way as first step we have identified the different forms of uncertainty during a knowledge extraction process then we have introduce an rdf representation for these kind of knowledge based on an ontologie that we developped for this issue\n",
      "heuristiques pour adaptation des mappings entre ontologies dynamiques\n",
      "les correspondances sémantiques entre ontologies mappings) jouent un rôle essentiel dans les systèmes d'information cependant en vertu de évolution des connaissances les éléments ontologiques sont sujets à modification invalidant potentiellement les alignements préalablement établis des techniques de maintenance sont donc nécessaires pour maintenir la validité des mappings dans cet article nous présentons un ensemble heuristiques guidant leur adaptation notre approche s'appuie sur explication des mappings existants les informations provenant de évolution des ontologies ainsi que les adaptations possibles applicables aux mappings nous proposons une validation expérimentale à partir ontologies du domaine médical et des mappings qui leur sont associés\n",
      "identification auteurs par apprentissage automatique\n",
      "etant donné un ensemble de documents rédigés par un même auteur le problème authentification auteurs consiste à décider si un nouveau texte a été rédigé ou non par cet auteur pour résoudre ce problème nous avons proposé et implémenté différentes approches  comptage de similarité techniques de vote et apprentissage supervisé qui exploitent différents modèles de représentation des documents les expérimentations réalisées à partir des collections de la compétition pan-clef 2013 et 2014 ont confirmé intérêt de nos approches et leur performance en termes de temps de traitement\n",
      "identification des utilisateurs atypiques dans les systèmes de recommandation sociale\n",
      "malgré des performances très satisfaisantes approche sociale de la recommandation ne fournit pas de bonnes recommandations à un sous-ensemble des utilisateurs nous supposons ici que certains de ces utilisateurs ont des préférences différentes de celles des autres nous les qualifions d'atypiques nous nous intéressons à leur identification en amont de la tâche de recommandation et proposons plusieurs mesures représentant atypicité des préférences un utilisateur évaluation de ces mesures sur un corpus de état de art montre qu'elles permettent identifier de façon fiable des utilisateurs recevant de mauvaises recommandations\n",
      "apport une approche symbolique pour le repérage des entités nommées en langue amazighe\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le repérage des entités nommées ren) en langue amazighe est un prétraitement éventuellement essentiel pour de nombreuses applications du traitement automatique des langues (tal) en particulier pour la traduction automatique dans cet article nous présentons une chaîne de repérage des entités nommées en amazighe fondée sur une étude synthétique des spécificités de la langue et des entités nommées en amazighe article met accent sur les choix méthodologiques à résoudre les ambiguïtés dues à la langue en exploitant les technologies existantes pour autres langues\n",
      "leveragingweb 2.0 for informed real-estate services\n",
      "the perception about real estate properties both for individuals and agents is not formed exclusively by their intrinsic characteristics such as surface and age but also from property externalities such as pollution traffic congestion criminality rates proximity to playgrounds schools and stimulating social interactions that are equally important in this paper we present the real-estate 2.0 system that in contrary to existing real-estate e-services and applications takes also into account important externalities by leveraging web 2.0 content from social networks poi listings applications and open data enables the thorough analysis of the current physical and social context of the property the context-based objective valuation of re properties along with an advanced property search and selection experience that unveils otherwise “hidden” property features and significantly reduces user effort and time spent in their re quest the system encompasses the above to provide services which assist individuals and agents in making more informed and sound re decisions\n",
      "linked data annotation and fusion driven by data quality evaluation\n",
      "dans cet article nous présentons une approche de fusion de données fondée sur utilisation informations sur la qualité des données pour résoudre les éventuels conflits entre valeurs\n",
      "managing big multidimensional data\n",
      "multidimensional database concepts such as cubes dimensions with hierarchies and measures have been a cornerstone of analytical business intelligence tools for decades however the standard data models and system implementations olap) for multidimensional databases cannot handle “big multidimensional data” very large amounts of complex and highly dynamic multidimensional data that occur in a number of emerging domains such as energy transport logistics as well as science this talk will discuss similarities and differences between traditional business intelligence bi) and big data present examples of big multidimensional data with the characteristics of large volume high velocity fast data and/or high variety complex data and discuss how to manage big multidimensional data including modeling algorithmic implementation as well as practical issues\n",
      "mesure influence via les indicateurs de centralité dans les réseaux sociaux\n",
      "for social network analysis existing centrality measures emphasize the importance of an actor considering only the structural position in the network regardless of a priori information on this actors such as popularity accessibility or behavior in this study new variants of centrality measures are proposed operating both the network structure and the specific attributes of an actor experiments have validated the contribution of valuations especially for the detection of broadcasters in social networks\n",
      "méthode alternative à la détection de « copier/coller »  intersection de textes et construction de séquences maximales communes \n",
      "la détection du plagiat passe le plus souvent par la phase de recherche de similitudes la plus naïve la détection de « copier/coller » dans cet article nous proposons une méthode alternative à approche standard de comparaison mot à mot le principe étant effectuer une intersection des deux textes à comparer récupérant ainsi un tableau des mots qu'ils ont en commun et de ne conserver que les séquences maximales des mots se suivant dans un des textes et existant également dans l'autre nous montrons que cette méthode est plus rapide et moins coûteuse en ressources que les méthodes de parcours de textes habituellement utilisées objectif étant de détecter les passages identiques entre deux textes plus rapidement que les méthodes de comparaison mot à mot tout en étant plus efficace que les méthodes n-grammes\n",
      "mining classes by multi-label classification\n",
      "we propose a new approach to mine potential classes in news documents by examining close relationship between new classes and probability vectors of multiple labeling of the documents using em algorithm to obtain the distribution over linear mixture models we make clustering and mine classes\n",
      "modèle de biclustering dans un paradigme \"mapreduce\"\n",
      "biclustering is a main task in a variety of areas of machine learning providing simultaneous observations and features clustering biclustering approches are more complex compared to the traditional clustering particularly those requiring large dataset and mapreduce platforms we propose a new approach of biclustering based on popular self-organizing maps for cluster analysis of large dataset we have designed scalable implementations of the new biclustering algorithm using mapreduce with the spark platform we report the experiments and demonstrated the performance public dataset using different cores using practical examples we demonstrate that our algorithm works well in practice the experimental results show scalable performance with near linear speedups across different data and 120 cores\n",
      "nouvelle approche de contextualisation de tweets basée sur les règles association inter-termes\n",
      "tweets are short messages that do not exceed 140 characters since they must be written respecting this limitation a particular vocabulary is used to make them understandable to a reader it is therefore necessary to know their context in this paper we describe our approach for the tweet contextualization this approach allows the extension of the tweet's vocabulary by a set of thematically related words using mining association rules between terms\n",
      "pour une meilleure exploitation de la classification croisée dans les systèmes de filtrage collaboratif\n",
      "pour la prédiction automatique des items préférés par des utilisateurs sur le web différents systèmes de filtrage collaboratif ont été proposés la plupart entre eux sont basés sur la factorisation matricielle et les approches de type k plus proches voisins malheureusement ces deux approches requièrent un temps de calcul important une partie de ces problèmes a pu être surmontée par la classification croisée ou co-clustering qui s'avère pertinente du fait qu'elle permet par nature une gestion simultanée des ensembles correspondant aux utilisateurs et aux items cependant des travaux doivent encore être menés pour une meilleure prise en compte des données manquantes dans ce travail nous proposons donc une gestion efficace des données non observées permettant une meilleure exploitation du potentiel de la classification croisée dans le domaine des systèmes de recommandation nous montrons de plus qu'elle permet obtenir des représentations à base de graphes bipartis facilitant interprétation interactive des affinités entre des groupes utilisateurs et des groupe d'items\n",
      "proposition outil de clustering visuel et interactif\n",
      "cet article présente un nouvel outil visuel de clustering interactif il utilise une technique de réduction de dimensionnalité pour permettre une représentation 2d des données et des classes associées initialement établies de manière non-supervisée originalité de outil consiste à autoriser des modifications itératives à la fois du clustering et de la projection 2d grâce à des contrôles adaptés utilisateur peut ainsi injecter ses préférences et observer le changement induit en temps réel la méthode de projection utilisée suit une métaphore physique qui facilite le suivi des changements par utilisateur. nous montrons un exemple illustrant intérêt pratique de outil.\n",
      "qualité et complexité en évaluation des mesures intérêt\n",
      "remplacer des hypothèses sur le modèle de données par des informations mesurées sur les données réelles est une des forces de la fouille de données cet article étudie cet ajustement entre les données et les méthodes de découverte de motifs pour en évaluer la qualité et la complexité nous formalisons ce lien entre données et mesures intérêt en identifiant les motifs liés qui sont ceux nécessaires pour évaluation une mesure ou une contrainte nous formulons alors trois axiomes que devraient satisfaire ces motifs liés pour qu'une méthode extraction se comporte bien en outre nous définissons la complexité en évaluation qui quantifie finement interrelation entre les motifs au sein une méthode extraction. a la lumière de ces axiomes et de cette complexité en évaluation nous dressons une typologie de multiples méthodes de découverte de motifs impliquant la fréquence\n",
      "rankmerging apprentissage supervisé de classements pour la prédiction de liens dans les grands réseaux sociaux\n",
      "trouver les liens manquants dans un grand réseau social est une tâche difficile car ces réseaux sont peu denses et les liens peuvent correspondre à des environnements structurels variés dans cet article nous décrivons rankmerging une méthode apprentissage supervisé simple pour combiner information obtenue par différentes méthodes de classement afin illustrer son intérêt nous appliquons à un réseau utilisateurs de téléphones portables pour montrer comment un opérateur peut détecter des liens entre les clients de ses concurrents nous montrons que rankmerging surpasse les méthodes à disposition pour prédire un nombre variable de liens dans un grand graphe épars\n",
      "réduction de la complexité spatiale et temporelle du compact prediction tree pour la prédiction de séquences\n",
      "la prédiction de séquences de symboles est une tâche ayant de multiples applications plusieurs modèles de prédiction ont été proposés tels que dg all-k-order markov et ppm récemment il a été montré qu'un nouveau modèle nommé compact prediction tree cpt) utilisant une structure en arbre et un algorithme de prédiction plus complexe offre des prédictions plus exactes que plusieurs approches de la littérature néanmoins une limite importante de cpt est sa complexité temporelle et spatiale élevée dans cet article nous pallions ce problème en proposant trois stratégies pour réduire la taille et le temps de prédiction de cpt les résultats expérimentaux sur 7 jeux de données réels montrent que le modèle résultant nommé cpt+ est jusqu'à 98 fois plus compact et est 4.5 fois plus rapide que cpt tout en conservant une exactitude très élevée par rapport à all-k-order markov dg lz78 ppm et tdag\n",
      "regroupement attributs par règles association dans les systèmes inférence floue\n",
      "dans les systèmes apprentissage supervisé par construction de règles de classification floues un nombre élevé attributs descriptifs conduit à une explosion du nombre de règles générées et peut affecter la précision des algorithmes apprentissage. afin de remédier à ce problème une solution est de traiter séparément des sous-groupes attributs cela permet de décomposer le problème apprentissage en des sous-problèmes de complexité inférieure et obtenir des règles plus intelligibles car de taille réduite nous proposons une nouvelle méthode de regroupement des attributs qui se base sur le concept des règles d'association ces règles découvrent des relations intéressantes entre des intervalles de valeurs des attributs ces liaisons locales sont ensuite agrégées au niveau des attributs mêmes en fonction du nombre de liaisons trouvées et de leur importance notre approche testée sur différentes bases apprentissage et comparée à approche classique permet améliorer la précision tout en garantissant une réduction du nombre de règles\n",
      "régularisation de noyaux temporellement élastiques et analyse en composantes principales non-linéaire pour la fouille de séries temporelles \n",
      "dans le domaine de la fouille de séries temporelles plusieurs travaux récents exploitent des noyaux construits à partir de distances élastiques de type dynamic time warping dtw) au sein approches à base de noyaux pourtant les matrices apparentées aux matrices de gram construites à partir de ces noyaux n'ont pas toujours les propriétés requises ce qui peut les rendre in fine impropres à une telle exploitation des approches émergeantes de régularisation de noyaux élastiques peuvent être mises à profit pour répondre à cette insuffisance nous présentons une de ces méthodes kdtw pour le noyau dtw puis autour une analyse en composantes principales non-linéaire (k-pca) nous évaluons la capacité de quelques noyaux concurrents élastiques v.s non élastiques définis vs non définis à séparer les catégories des données analysées tout en proposant une réduction dimensionnelle importante cette étude montre expérimentalement intérêt une régularisation de type kdtw\n",
      "requêtes skyline en présence des données évidentielles\n",
      "dans cet article nous nous intéressons à la recherche des points les plus intéressants au sens de ordre de pareto dans les bases de données évidentielles nous présentons le modèle skyline évidentiel qui est adapté à la nature des données incertaines ensuite nous présentons une évaluation expérimentale de notre approche\n",
      "to initiate a corporate memory with a knowledge compendium ten years of learning from experience with the ardans method\n",
      "ardans method ardanssas 2006b) and technology ardanssas 2006a) of knowledge capitalization and structuration are used with different industries (automotive aerospace energy defence steel health etc. for more than a decade in france and europe.the proposed solutions in knowledge management and especially in expertise capitalisation have set a lot of feedback over time with a view toward ongoing improvement what are the impacts of these feedbacks on the method nowadays put into practice into the industry the return of investment of a capitalization campaign is inferred from the quality of the knowledge base delivered at the end of the campaign therefore the method and the technology are intrinsically connected how it tools can assist with the quality diagnosis of the knowledge base?a comparative study was conducted on the basis of the method mariot et al 2007) exposed at egc'2007 this article sets out the results of the changes and improvements of the method in conjunction with the latest technical and scientific development on the one hand and the change of the industry needs on the other hand\n",
      "towards linked data extraction from tweets\n",
      "millions of twitter users post messages every day to communicate with other users in real time information about events that occur in their environment most of the studies on the content of tweets have focused on the detection of emerging topics however to the best of our knowledge no approach has been proposed to create a knowledge base and enrich it automatically with information coming from tweets the solution that we propose is composed of four main phases topic identification tweets classification automatic summarization and creation of an rdf triplestore the proposed approach is implemented in a system covering the entire sequence of processing steps from the collection of tweets written in english language based on both trusted and crowd sources to the creation of an rdf dataset anchored in dbpedia's namespace\n",
      "ultrametricity of dissimilarity spaces and its significance for data mining\n",
      "nous introduisons une mesure ultramétricité pour les dissimilaritées et examinons les transformations des dissimilaritées et leurs impact sur cette mesure ensuite nous étudions influence de ultramétricité sur la comportement de deux classes algorithmes exploration de données le knn algorithme de classification et algorithme de regroupement pam appliqués sur les espaces de dissimilarité on montre qu'il existe une variation inverse entre ultramétricité et la performance des classificateurs pour les clusters une augmentation ultramétricité genere regroupements avec une meilleure séparation une diminution de la ultramétricité produit groupes plus compacts\n",
      "un algorithme em pour une version parcimonieuse de analyse en composantes principales probabiliste\n",
      "nous considérons une version parcimonieuse de analyse en composantes principales probabiliste la pénalité `1 imposée sur les composantes principales rend leur interprétation plus aisée en ne faisant dépendre ces dernières que un nombre restreint de variables initiales un algorithme em simple de mise en oeuvre est proposé pour estimation des paramètres du modèle la méthode de heuristique de pente est finalement utilisée pour choisir le coefficient de pénalisation\n",
      "un algorithme icm basé sur la compacité pour la segmentation des images satellites à très haute résolution\n",
      "dans cet article nous proposons une modification pour algorithme \"iterated conditional modes\" icm) appliqué à la segmentation images à très haute résolution pour ce faire nous introduisons un nouveau critère de convergence basé sur la compacité des clusters et qui repose sur une fonction énergie adaptée aux modèles de voisinages irréguliers de ce type images. grâce à cette méthode nos premières expériences ont montré que nous obtenons des résultats plus fiables en terme de convergence et de meilleure qualité qu'en utilisant énergie globale comme critère d'arrêt\n",
      "un langage interrogation à la sparql pour les graphes conceptuels\n",
      "cet article propose un langage générique interrogation pour le modèle des graphes conceptuels d'abord nous introduisons les graphes interrogation. un graphe interrogation est utilisé pour exprimer un « ou » entre deux sous-graphes ainsi qu'une « option » sur un sous-graphe optionnel ensuite nous proposons quatre types de requêtes (interrogation sélection description et construction en utilisant les graphes interrogation. enfin les réponses à ces requêtes sont calculées à partir une opération basée sur homomorphisme de graphe\n",
      "une approche centrée graine pour la détection de communautés dans les réseaux multiplexes\n",
      "nous nous intéressons dans ce travail au problème de détection de communautés dans les réseaux multiplexes le modèle de réseau multiplexe a été récemment introduit afin de faciliter la modélisation des réseaux multirelationnels des réseaux dynamiques et/ou des réseaux attribués les approches existantes pour la détection de communautés dans ce genre de graphes sont pour la plupart basées sur des schémas agrégation de couches ou agrégation de partitions nous proposons ici une nouvelle approche centrée graine qui permet de prendre en compte directement la nature multi-couche un réseau multiplexe des expérimentations effectuées sur différents réseaux multiplexes montrent que notre approche surpasse les approches de état de art en termes de qualité des communautés identifiées\n",
      "une approche de visualisation analytique pour comparer les modèles de propagation dans les réseaux sociaux\n",
      "les modèles de propagation d'informations influence et actions dans les réseaux sociaux sont nombreux et diversifiés rendant le choix de celui approprié à une situation donnée potentiellement difficile la sélection un modèle pertinent pour une situation exige de pouvoir les comparer cette comparaison n'est possible qu'au prix une traduction des modèles dans un formalisme commun et indépendant de ceux-ci nous proposons utilisation de la réécriture de graphes afin exprimer les mécanismes de propagation sous la forme un ensemble de règles de transformation locales appliquées selon une stratégie donnée cette démarche prend tout son sens lorsque les modèles ainsi traduits sont étudiés et simulés à partir une plate-forme de visualisation analytique dédiée à la réécriture de graphe après avoir décrit les modèles et effectué différentes simulations nous exhibons comment la plate-forme permet interagir avec ces formalismes et comparer interactivement les traces exécution de chaque modèle grâce à diverses mesures soulignant leurs différences\n",
      "une nouvelle formalisation des changements ontologiques composés et complexes\n",
      "évolution une ontologie est un processus indispensable dans son cycle de vie elle est exprimée et définie par des changements ontologiques de différents types  élémentaires composés et complexes les changements complexes et composés sont très utiles dans le sens où ils aident utilisateur à adapter son ontologie sans se perdre dans les détails des changements élémentaires cependant ils cachent derrière une formalisation sophistiquée puisqu'ils affectent à la fois plusieurs entités ontologiques et peuvent causer des inconsistances à ontologie évoluée pour adresser cette problématique cet article présente une nouvelle formalisation des changements ontologiques composés et complexes basée sur les grammaires de graphes typés cette formalisation s'appuie sur approche algébrique simple pushout spo) de transformation de graphes et possède deux principaux avantages  1) fournir une nouvelle formalisation permettant de contrôler les transformations de graphes et éviter les incohérences une manière a priori 2) simplifier la définition des changements composés et complexes en réduisant le nombre de changements élémentaires nécessaires à leur application\n",
      "une nouvelle méthode de web usage mining basée sur une analyse sémiotique du comportement de navigation\n",
      "objectif de nos travaux est de proposer une méthode analyse automatique du comportement des utilisateurs à des fins de prédiction de leur propension à réaliser une action suggérée nous proposons dans cet article une nouvelle méthode de web usage mining basée sur une étude sémiotique des styles perceptifs considérant expérience de utilisateur comme élément déterminant de sa réaction à une sollicitation étude de ces styles nous a amené à définir de nouveaux indicateurs des descripteurs sémiotiques introduisant un niveau supplémentaire à approche sémantique annotation des sites nous proposons ensuite un modèle neuronal adapté au traitement de ces nouveaux indicateurs nous expliquerons en quoi le modèle proposé est le plus pertinent pour traiter ces informations\n",
      "une plateforme etl parallèle et distribuée pour intégration de données massives\n",
      "nous nous intéressons dans ce papier à impact des données massives dans un environnement décisionnel et plus particulièrement sur la phase intégration des données dans ce contexte nous avons développé une plateforme baptisée p-etl (parallel-etl) destinée à entreposage de données massives selon le paradigme mapreduce p-etl permet le paramétrage de processus etl workflow) et un paramétrage avancé relatif à environnement parallèle et distribué ce papier décrit la plateforme p-etl en vue une démonstration face à des jeux de données allant de 244 * 106 à 7 317 * 109 tuples les expérimentations menées ont montré amélioration significative des performances de p-etl lorsque la taille du cluster et le nombre des tâches parallèles augmentent\n",
      "using social conversational context for detecting users interactions on microblogging sites\n",
      "dans ce travail nous proposons une nouvelle méthode de détection des conversations sur les sites des réseaux sociaux cette méthode est basée sur analyse et enrichissement de contenu dans le but de présenter un résultat informatif basé sur les interactions des utilisateurs nous avons évalué notre méthode sur corpus recueillis de réseau social lié à des sujets spécifiques et nous avons obtenu des bons résultats\n",
      "utilisation des pyramides pour visualiser la contamination des manuscrits\n",
      "in this paper we present a new codicum stemma visualization method don quentin's modeling is usec to classify the textual tradition.we supplement the genealogical editor's information of betweenness triplets obtained directly from the corpus a pyramid depicting the family codicum stemma is then constructed on the basis of information obtained by the triplets\n",
      "vers la découverte de modèles exceptionnels locaux  des règles descriptives liant les molécules à leurs odeurs\n",
      "issue un phénomène complexe partant une molécule odorante jusqu'à la perception dans le cerveau olfaction reste le sens le plus difficile à appréhender par les neuroscientifiques enjeu principal est établir des règles sur les propriétés physicochimiques des molécules (poids nombre d'atomes etc. afin de caractériser spécifiquement un sous-ensemble de qualités olfactives (fruité boisé etc.. on peut trouver de telles règles descriptives grâce à la découverte de sous-groupes “subgroup discovery”) cependant les méthodes existantes permettent de caractériser soit une seule qualité olfactive  soit toutes les qualités olfactives à la fois “exceptional model mining” mais pas un sousensemble nous proposons alors une approche de découverte de sous-groupes caractéristiques de seulement certains labels par une nouvelle technique d'énumération issue de la fouille de redescriptions nous avons expérimenté notre méthode sur une base de données olfaction fournie par des neuroscientifiques et pu exhiber des premiers sous-groupes intelligibles et réalistes\n",
      "visualizing shooting spots using geo-tagged photographs from social media sites\n",
      "hotspots à laquelle de nombreuses photographies ont été prises pourraient être des lieux intéressants pour beaucoup de gens faire du tourisme visualisation des hotspots révèle les intérêts des utilisateurs ce qui est important pour les industries telles que la recherche et du marketing touristiques bien que plusieurs techniques basées sociaux-pour hotspots extraction indépendamment ont été proposés un hotspot a une relation à autres hotspots dans certains cas pour organiser ces hotspots nous proposons une méthode pour détecter et de visualiser les relations entre les hotspots notre méthode proposée détecte et évalue les relations de taches de tir et sujets photographiques notre approche extrait les relations à aide de sous-hotspots qui sont fendus un hotspot qui comprend des photographies de différents types\n",
      "xewgraph  outil de visualisation et analyse des hypergraphes pour un système intelligence economique\n",
      "the competitive intelligence system xplor everywhere helps searching visualizing and sharing useful data in this paper we will introduce xplor everywhere and its newest feature called xewgraph which is dedicated to the analysis of massive data and visualization of hypergraphs\n",
      "1d-sax  une nouvelle représentation symbolique pour les séries temporelles\n",
      "sax symbolic aggregate approximation est une des techniques majeures de symbolisation des séries temporelles la non prise en compte des tendances dans la symbolisation est une limitation bien connue de sax cet article présente 1d-sax une méthode pour représenter une série temporelle par une séquence de symboles contenant des informations sur la moyenne et la tendance des fenêtres successives de la série segmentée nous comparons efficacité de 1d-sax vs sax dans une tâche de classification de séries temporelles images satellites les résultats montrent que 1d-sax améliore les taux de classification pour une quantité information identique utilisée\n",
      "agrégation de sac-de-sacs-de-mots pour la recherche information par modèles vectoriels\n",
      "cet article étudie intérêt de représenter les documents textuels non plus comme des sacs-de-mots mais comme des sacs-de-sacs-de-mots au coeur de utilisation de cette représentation le calcul de similarité entre deux objets nécessite alors agréger toutes les similarités entre sacs de chacun des objets nous évaluons cette représentation dans un cadre de recherche d'information et étudions les propriétés attendues de ces fonctions agrégation les expériences rapportées montrent intérêt de cette représentation lorsque les opérateurs agrégation respectent certaines propriétés avec des gains très importants par rapport aux représentations standard\n",
      "alignement ontologies  exploitation des ontologies liées sur le web de données\n",
      "nous proposons dans cet article une méthode alignement une ontologie source avec des ontologies cibles déjà publiées et liées sur le web de données nous présentons ensuite un retour expérience sur alignement une ontologie dans le domaine des sciences du vivant et de environnement avec agrovoc et nalt\n",
      "annotation sémantique de documents administratifs\n",
      "la numérisation de documents administratifs est un enjeu économique et écologique prioritaire dans le contexte sociétal actuel la dématérialisation massive de document n'est pas sans conséquence et soulève les problèmes d'organisation de stockage et accès à l'information le défi n'est donc plus la numérisation du document mais extraction des informations qu'ils contiennent les documents sont produits par homme et pour homme. cette propriété permet de localiser des informations dans les zones saillantes du document (logos) la saillance et la reconnaissance sont deux éléments essentiels pour la classification rapide de documents a l'opposé la recherche un document ou un ensemble de documents repose presque toujours sur le texte brut il est donc nécessaire de faire une correspondance entre une requête textuelle et le documents cet article présente une nouvelle approche annotation automatique de documents administratifs qui utilise une approche visuel et une approche de fouille de texte\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "application du paradigme mapreduce aux données ouvertes cas  accessibilité des personnes à mobilité réduite aux musées\n",
      "le modèle mapreduce est aujourd'hui un des modèles de programmation parallèle les plus utilisés définissant une architecture maître-esclave il permet le traitement parallèle de grandes masses de données dans ce papier nous proposons un algorithme basé sur mapreduce qui permet à partir des données publiques du ministère français de la communication et de la culture de définir un classement des galeries et musées nationaux selon leurs degré accessibilité aux personnes handicapées tout en profitant de la puissance et de la flexibilité du paradigme mapreduce les décideurs pourront mettre en place des stratégies efficaces à moindre coût et avoir ainsi une vision plus précise sur les établissements culturels et leurs limites relatives à cette catégorie de personnes algorithme que nous proposons peut être exploité et appliqué à autres cas études avec des jeux de données plus volumineux\n",
      "apprentissage de fonctions de tri pour la prédiction interactions protéine-arn\n",
      "les fonctions biologiques dans la cellule mettent en jeu des interactions 3d entre protéines et arn les avancées des techniques expérimentales restent insuffisantes pour de nombreuse applications il faut alors pouvoir prédire in silico les interactions protéine-arn dans ce contexte nos travaux sont focalisés sur la construction de fonctions de score permettant ordonner les solutions générées par le programme amarrage protéine-arn rosettadock la méthodologie évaluation utilisée par rosettadock impose de trouver une fonction de score s'exprimant comme une combinaison linéaire de mesures physicochimiques avec une approche apprentissage supervisé par algorithme génétique nous avons appris différentes fonctions de score en imposant des contraintes sur la nature des poids recherchés les résultats obtenus montrent importance de la signification des poids à apprendre et de espace de recherche associé\n",
      "apprentissage incrémental anytime un classifieur bayésien naïf pondéré\n",
      "nous considérons le problème de classification supervisée pour des flux de données présentant éventuellement un très grand nombre de variables explicatives le classifieur bayésien naïf se révèle alors simple à calculer et relativement performant tant que hypothèse restrictive indépendance des variables conditionnellement à la classe est respectée la sélection de variables et le moyennage de modèles sont deux voies connues amélioration qui reviennent à déployer un prédicteur bayésien naïf intégrant une pondération des variables explicatives dans cet article nous nous intéressons à estimation directe un tel modèle bayésien naïf pondéré nous proposons une régularisation parcimonieuse de la log-vraisemblance du modèle prenant en compte informativité de chaque variable la log-vraisemblance régularisée obtenue étant non convexe nous proposons un algorithme de gradient en ligne qui post-optimise la solution obtenue afin de déjouer les minima locaux les expérimentations menées s'intéressent une part à la qualité de optimisation obtenue et autre part aux performances du classifieur en fonction du paramétrage de la régularisation\n",
      "apprentissage non supervisé de dépendances syntaxiques à partir de texte étiqueté plusieurs variantes de pcfg légères\n",
      "apprentissage de dépendances est une tâche consistant à établir à partir des phrases un texte un modèle de construction arbres traduisant une hiérarchie syntaxique entre les mots nous proposons un modèle intermédiaire entre analyse syntaxique complète de la phrase et les sacs de mots il est basé sur une grammaire stochastique hors-contexte se traduisant par des relations de dépendance entre les catégories grammaticales une phrase les résultats expérimentaux obtenus sur des benchmarks attestés dépassent pour cinq langues sur dix les scores de algorithme de référence dmv et pour la première fois des scores sont obtenus pour le français la très grande simplicité de la grammaire permet un apprentissage très rapide et une analyse presque instantanée\n",
      "approche formelle de fusion ontologies à aide des grammaires de graphes typés\n",
      "article propose une approche formelle de fusion ontologies se reposant sur les grammaires de graphes typés elle se décompose en trois étapes  1 la recherche de similarités entre concepts  2 la fusion des ontologies par approche algébrique spo simple push out  3 adaptation une ontologie globale par le biais de règles de réécriture de graphes contrairement aux solutions existantes cette méthode offre une représentation formelle de la fusion ontologies ainsi qu'une implémentation fonctionnelle basée sur outil agg\n",
      "approche par motifs pour analyse de données multi-résolution\n",
      "dans cet article nous nous intéressons aux approches pour analyse de graphes pouvant évoluer dans le temps et tel qu'un sommet à un temps donné peut correspondre à plusieurs sommets au temps suivant et où les sommets sont associés à un ensemble attributs catégoriels dans ce type de données nous proposons une nouvelle classe de motifs basée sur des contraintes permettant de décrire évolution de structures homogènes ce type approche est particulièrement adaptée pour analyse images multi-résolution sans perte d'information nous présentons un résultat qualitatif dans ce domaine\n",
      "automatic correction of svm for drifted data classification\n",
      "concept drift is an important feature of real-world data streams that can make usual machine learning techniques rapidly become unsuitable this paper addresses the problem of sudden concept drift in classification problems for which standard techniques may fail to this end support vector machines svms) are automatically corrected to cope with a new suddenly drifted dataset results on real-world datasets with several types of sudden drift indicate that the method is able to correct the svm in order to better classify the new data after the concept drift using a correction based on the difference between the initial dataset and the new drifted dataset even when the new dataset is small\n",
      "broad data what happens when the web of data becomes real\n",
      "“big data” is used to refer to the very large datasets generated by scientists to the many petabytes of data held by companies like facebook and google and to analyzing real-time data assets like the stream of twitter messages emerging from events around the world key area of interest include technologies to manage much larger datasets (cf nosql) technologies for the visualization and analysis of databases cloud-based data management and datamining algorithms recently however we have begun to see the emergence of another and equally compelling data challenge – that of the “broad data” that emerges from millions and millions of raw datasets available on the world wide web for broad data the new challenges that emerge include web-scale data search and discovery rapid and potentially ad hoc integration of datasets visualization and analysis of only-partially modeled datasets and issues relating to the policies for data use reuse and combination in this talk we present the broad data challenge and discuss potential starting points for solutions we illustrate these approaches using data froma “meta-catalog” of over 1,000,000 open datasets that have been collected from about two hundred governments from around the world\n",
      "classification des actions humaines basée sur les descripteurs spatio-temporels\n",
      "dans cet article nous proposons un nouveau descripteurs spatio-temporel appelé st-surf pour analyse et la reconnaissance actions dans des flux vidéo idée principale est enrichir le descripteur speed up robust feature surf) en intégrant information de mouvement issue du flot optique seuls les points intérêts qui ont subi un déplacement sont pris en compte pour générer un dictionnaire de mots visuels dmv) robuste basé sur algorithme des k-moyennes (k-means) le dictionnaire est utilisé lors du processus apprentissage et de reconnaissance actions basé sur la méthode des machines à vecteurs supports (svm) les résultats obtenus confirment intérêt du descripteur proposé st-surf pour analyse de scènes et en particulier pour la reconnaissance actions. la méthode atteind une précision de reconnaisssance de ordre de 80.7% équivalente aux performances des descripteurs spatio-temporels de état de l'art\n",
      "classification et prédiction du flux solaire\n",
      "la prédiction du rayonnement solaire horaire dans une journée est un enjeu primordial pour la production énergie de type photovoltaïque nous présentons deux stratégies de classification des jours selon leurs rayonnements solaires puis une méthode de prédiction du flux solaire cohérente avec la classification\n",
      "classifieur naïf de bayes pondéré pour flux de données\n",
      "un classifieur naïf de bayes est un classifieur probabiliste basé sur application du théorème de bayes avec hypothèse naïve c'est-à-dire que les variables explicatives xi) sont supposées indépendantes conditionnellement à la variable cible (c) malgré cette hypothèse forte ce classifieur s'est avéré très efficace sur de nombreuses applications réelles et est souvent utilisé sur les flux de données pour la classification supervisée le classifieur naïf de bayes nécessite simplement en entrée estimation des probabilités conditionnelles par variable p(xi|c et les probabilités a priori p(c) pour une utilisation sur les flux de données cette estimation peut être fournie à aide un « résumé supervisé en-ligne de quantiles » état de art montre que le classifieur naïf de bayes peut être amélioré en utilisant une méthode de sélection ou de pondération des variables explicatives la plupart de ces méthodes ne peuvent fonctionner que hors-ligne car elles nécessitent de stocker toutes les données en mémoire et/ou de lire plus une fois chaque exemple par conséquent elles ne peuvent être utilisées sur les flux de données cet article présente une nouvelle méthode basée sur un modèle graphique qui calcule les poids des variables entrée en utilisant une estimation stochastique la méthode est incrémentale et produit un classifieur naïf de bayes pondéré pour flux de données cette méthode est comparée au classique classifieur naïf de bayes sur les données utilisées lors du challenge « large scale learning »\n",
      "clustering de données relationnelles pour la structuration de flux télévisuels\n",
      "les approches existantes pour structurer automatiquement un flux de télévision (ie reconstituer un guide de programme exact et complet) sont supervisées elles requièrent de grandes quantités de données annotées manuellement et aussi de définir a priori les types émissions (publicités bandes annonces programmes sponsors) pour éviter ces deux contraintes nous proposons une classification non supervisée la nature multi-relationnelle de nos données proscrit utilisation des techniques de clustering habituelles reposant sur des représentations sous forme attributs-valeurs nous proposons et validons expérimentalement une technique de clustering capable de manipuler ces données en détournant la programmation logique inductive pli) pour fonctionner dans ce cadre non supervisé\n",
      "clustering de séquences évènements temporels\n",
      "nous proposons une nouvelle méthode de clustering et analyse de séquences temporelles basée sur les modèles en grille à trois dimensions les séquences sont partitionnées en clusters la dimension temporelle est discrétisée en intervalles et la dimension évènement est partitionnée en groupes la grille de cellules 3d forme ainsi un estimateur non-paramétrique constant par morceaux de densité jointe des séquences et des dimensions des évènements temporels les séquences un cluster sont ainsi groupés car elles suivent une distribution similaire évènements au cours du temps nous proposons aussi une méthode exploitation du clustering par simplification de la grille ainsi que des indicateurs permettant interpréter les clusters et de caractériser les séquences qui les composent les expériences sur des données artificielles ainsi que sur des données réelles issues de dblp démontrent le bien-fondé de notre approche\n",
      "clusters dans les réseaux sociaux  intersections entre liens conceptuels fréquents et communautés\n",
      "la recherche de liens conceptuels fréquents fcl) est une nouvelle approche de clustering de réseaux qui exploite à la fois la structure et les attributs des noeuds bien que les travaux récents se soient déjà intéressés à optimisation des algorithmes de recherche des fcl peu de travaux sont aujourd'hui menés sur la complémentarité qui existe entre les liens conceptuels et approche classique de clustering qui consiste en extraction de communautés ainsi dans ce papier nous nous intéressons à ces deux approches notre objectif est évaluer les relations potentiellement existantes entre les communautés et les fcl pour comprendre la façon dont les motifs obtenus par chacune des méthodes peuvent correspondre ou s'intersecter ainsi que la connaissance utile résultant de la prise en compte de ces deux types de connaissance nous proposons pour cela un ensemble de mesures originales basées sur la notion d'homogénéité visant à évaluer le niveau intersection des fcl et des communautés lorsqu'ils sont extraits un même jeu de données notre approche est appliquée à deux réseaux et démontre importance de considérer simultanément plusieurs types de connaissance et leur intersection\n",
      "comment devenir cybercondriaque \n",
      "nous avons tous déjà eu occasion effectuer des recherches ordre médical sur internet si certains sites spécialisés se refusent à tout diagnostic en ligne préférant le renvoi vers des professionnels de santé autres en revanche conduisent souvent à des déclarations alarmistes faisant état de situations humaines difficiles dans ce travail nous étudions ampleur de ce phénomène et montrons que quel que soit le syndrome recherché les résultats obtenus conduisent toujours à énoncé des mots \"cancer\" ou \"tumeur\"\n",
      "comparaison de bornes théoriques pour accélération du clustering incrémental en une passe\n",
      "le clustering incrémental en une passe repose sur affectation efficace de chaque nouveau point aux clusters existants dans le cas général où les clusters ne peuvent être représentés par une moyenne la détermination exhaustive du cluster le plus proche possède une complexité quadratique avec le nombre de données nous proposons dans ce papier une nouvelle méthode affectation stochastique à chaque cluster qui minimise le nombre de comparaisons à effectuer entre la donnée et chaque cluster pour garantir étant donné un taux erreur acceptable affectation au cluster le plus proche plusieurs bornes théoriques (bernstein hoeffding et student sont comparées dans ce papier les résultats sur des données artificielles et réelles montrent que la borne de bernstein donne globalement les meilleurs résultats notamment lorsqu'elle est réduite car elle permet une accélération forte du processus de clustering tout en conservant un nombre très faible erreurs.\n",
      "comparaison des chemins de hilbert adaptatif et des graphes de voisinage pour la caractérisation un parcellaire agricole\n",
      "cet article compare deux représentations de données spatiales les graphes de voisinages et les chemins de hilbert-peano utilisées par des algorithmes de fouille cette comparaison s'appuie sur la mise en oeuvre une méthode énumération de « sacs de noeuds » qui permet obtenir des caractérisations homogènes à partir des deux représentations la méthode est appliquée à la caractérisation de parcellaires agricoles et les résultats tendent à montrer que la linéarisation de espace capte la majorité de l'information à exception des éléments rares sur cet exemple particulier\n",
      "compréhension de recettes de cuisine utilisateurs par extraction de connaissances intrinsèques\n",
      "sur les sites web communautaires les utilisateurs échangent des connaissances en étant à la fois auteurs et lecteurs nous présentons une méthode pour construire notre propre compréhension de la sémantique de la communauté sans recours à une base de connaissances externe nous effectuons une extraction de la connaissance présente dans les contributions analysées nous proposons une évaluation de la confiance imputable à cette compréhension déduite afin évaluer la qualité du contenu avec application à un site web de partage de recettes de cuisine\n",
      "construction de cube olap à partir un entrepôt de données orienté colonnes\n",
      "optimisation de la construction de cubes olap 1 a été jusqu'à présent axée sur le développement algorithmes de calcul performants ces derniers opèrent sur des données extraites de entrepôt de données qui est généralement implémenté selon le modèle relationnel qui adopte architecture orientée lignes or pour les requêtes décisionnelles architecture orientée colonnes offre de meilleures performances cependant les sgbdr 2 selon cette architecture ne disposent pas opérateurs appropriés pour le calcul de cube olap nous proposons dans cet article une nouvelle méthode de calcul de cube olap les résultats obtenus à partir des expérimentations que nous avons menées démontrent que notre approche optimise considérablement le temps de construction de cube olap et réduit le temps de réponse relatif à exploitation du cube comparé à approche orientée lignes\n",
      "construction de profils de préférences contextuelles basée sur extraction de motifs séquentiels\n",
      "utilisation de préférences suscite un intérêt croissant pour personnaliser des réponses et effectuer des recommandations en amont étape essentielle est élicitation des préférences qui consiste à construire un profil de préférences en sollicitant le moins possible l'utilisateur dans cet article nous présentons une méthode basée sur extraction de motifs séquentiels afin de générer des règles de préférences contextuelles à partir une base de paires de transactions à partir de ces règles générées qui ont une expressivité plus riche que celle des approches existantes nous montrons comment construire et utiliser un profil modélisant les préférences de l'utilisateur de plus notre approche a avantage de bénéficier des nombreux algorithmes efficaces extraction de séquences fréquentes évaluation de notre méthode sur des données réelles montre que les modèles de préférences construits permettent effectuer des recommandations justes à un utilisateur\n",
      "de ombre à la lumière  plus de visibilité sur eclipse\n",
      "extraction de connaissances à partir de données issues du génie logiciel est un domaine qui s'est beaucoup développé ces dix dernières années avec notamment la fouille de référentiels logiciels mining software repositories et application de méthodes statistiques (partitionnement détection d'outliers à des thématiques du processus de développement logiciel cet article présente la démarche de fouille de données mise en oeuvre dans le cadre de polarsys un groupe de travail de la fondation eclipse de la définition des exigences à la proposition un modèle de qualité dédié et à son implémentation sur un prototype les principaux concepts adoptés et les leçons tirées sont également passés en revue\n",
      "de nouvelles pondérations adaptées à la classification de petits volumes de données textuelles\n",
      "un des défis actuels dans le domaine de la classification supervisée de documents est de pouvoir produire un modèle fiable à partir un faible volume de données avec un volume conséquent de données les classifieurs fournissent des résultats satisfaisants mais les performances sont dégradées lorsque celui-ci diminue nous proposons dans cet article de nouvelles méthodes de pondérations résistant à une diminution du volume de données leur efficacité évaluée en utilisant des algorithmes de classification supervisés existants naive bayes et class-feature-centroid sur deux corpus différents est supérieure à celle des autres algorithmes lorsque le nombre de descripteurs diminue nous avons étudié en parallèle les paramètres influençant les différentes approches telles que le nombre de classes de documents ou de descripteurs\n",
      "des humanités au numérique  interdisciplinarité et réciprocité\n",
      "les humanités numériques aussi contestable et critiquable que soit le terme font maintenant partie du paysage de la recherche en sciences humaines institutionnalisées par la très grande infrastructure de recherche huma-num du cnrs elles sont généralement définies comme la convergence de disciplines autour un matériau numérique matériau inévitablement accompagné un outillage tout aussi numérique ce matériau suivant la discipline qu'il observe pourra être considéré comme un objet éditorial un objet analysable ou un objet calculable nous tenterons de montrer que ce matériau peut aussi être perçu voire construit comme un dépôt voire un entrepôt de connaissances notre présentation s'appuiera sur divers projets de recherche en humanités numériques auxquels nous contribuons afin de mettre en exergue le lien qui peut être fait entre extraction et gestion de connaissances une part et humanités numériques autre part  le premier peut trouver un terrain expérimental dans le second tandis que le second peut tirer profit des méthodes et outils développés par le premier nous égrainerons par ailleurs autres problématiques inhérentes aux humanités numériques  de la constitution à analyse du corpus en passant par la formalisation et la normalisation des données enfin nous tenterons de montrer par exemple que les questions posées par les humanités numériques ne sont pas sans rappeler celles des industries de la connaissance\n",
      "détection opinions dans des tweets\n",
      "twitter est à heure actuelle un des réseau sociaux les plus utilisé au monde et analyser les opinions qui y sont contenues permet de fournir de précieuses informations notamment aux entreprises commerciales dans cet article nous décrivons une méthode permettant de déterminer opinion un tweet en détectant dans un premier temps sa subjectivité puis sa polarité\n",
      "détection de changements dans des flots de données qualitatives\n",
      "pour mieux analyser et extraire de la connaissance de flots de données des approches spécifiques ont été proposées ces dernières années un des challenges auquel elles doivent faire face est la détection de changement dans les données alors que de plus en plus de données qualitatives sont générées peu de travaux de recherche se sont intéressés à la détection de changement dans ce contexte et les travaux existants se sont principalement focalisés sur la qualité un modèle appris plutôt qu'au réel changement dans les données dans cet article nous proposons une nouvelle méthode de détection de changement non supervisée appelée cdcstream change detection in categorical datastreams) adaptée aux flux de données qualitatives\n",
      "détection de situations à risque basée sur des détecteurs de mouvement à domicile pour les personnes dépendantes\n",
      "avec le vieillissement de la population dans les décennies à venir la prise en charge de la dépendance est devenu un enjeu majeur les nouvelles technologies permettent améliorer le confort et la sécurité des personnes dépendantes à domicile dans cet article nous proposons une méthode de détection de situations à risques basée sur le seuillage automatique des intervalles incativité des capteurs de mouvement de type infrarouge passif notre contribution consiste à apprendre de façon automatique la durée maximale d'inactivité par pièce et par plage horaire la méthode est évaluée sur des données réelles provenant de activité une personne réelle dans un appartement équipé de capteurs domotiques notre approche permet de réduire le temps appel des secours\n",
      "du texte à la base de données géographiques\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avec la prolifération des données géographiques il y a un fort besoin de concevoir des outils automatiques pour exploitation des connaissances géographiques incarnées dans les documents textuels c'est dans ce contexte que nous proposons une approche permettant de générer une base de données géographiques bdg) à partir de textes notre approche s'articule autour de deux grandes phases  la génération du schéma de la bdg et la détermination des données qui serviront au remplissage de cette base implémentation de notre approche a donné naissance à un outil que nous avons baptisé gdb generator et que nous avons intégré dans le sig  openjump\n",
      "dynamique des communautés par prédiction interactions dans les réseaux sociaux\n",
      "dans cet article nous proposons une approche générale de prédiction des communautés basée sur un modèle apprentissage automatique pour la prédiction des interactions en effet nous pensons que si on peut prédire avec précision la structure du réseau alors on a juste à rechercher les communautés sur le réseau prédit des expérimentations sur des jeux de données réels montrent la faisabilité de cette approche\n",
      "evaluation de la pertinence dans un système de recommandation sémantique de nouvelles économiques\n",
      "de nos jours dans les secteurs commerciaux et financiers la veille est cruciale et complexe car la charge informations est importante pour répondre à cette problématique nous proposons un système novateur de recommandation article basé sur une modélisation ontologique des connaissances nous présentons également une nouvelle méthode évaluation de la pertinence utilisant le modèle vectoriel intrinsèquement efficace et adapté afin de pallier la confusion native de ces modèles entre les notions de similarité et de pertinence\n",
      "exploration une collection de chansons à partir une interface de visualisation basée sur une analyse des paroles\n",
      "dans cet article nous présentons une approche de fouille de texte ainsi qu'une interface de visualisation afin explorer une large collection de chansons françaises à partir des paroles dans un premier temps nous collectons paroles et métadonnées de différentes sources sur le web nous utilisons une approche combinant clustering et analyse sémantique latente afin identifier différentes thématiques et de déterminer différents descripteurs significatifs nous transformons par la suite le modèle afin obtenir une visualisation interactive permettant explorer la collection de chansons\n",
      "extension de étiquetage géographique des pixels une image par fouille de données\n",
      "les techniques de classification modernes permettent étiqueter les zones non couvertes des bases de données cartographiques mais souffrent un manque de robustesse important dans cet article nous proposons une méthode robuste extension étiquetage sur emprise une image satellite par analyse hiérarchique des données existantes notre approche est fondée sur une sélection attributs par thème de la base de données une sélection des pixels apprentissage et des classifications par objet de chaque thème la décision finale étiquetage est prise après fusion des classifications par thème notre méthode est appliquée avec succès et comparée à plusieurs méthodes de classification couplant données occupation du sol et imagerie spatiale très haute résolution\n",
      "extraction de motifs dans des graphes orientés attribués en présence automorphisme\n",
      "les graphes orientés attribués sont des graphes orientés dans lesquels les noeuds sont associés à un ensemble d'attributs de nombreuses données issues du monde réel peuvent être représentées par ce type de structure mais encore peu algorithmes sont capables de les traiter directement la fouille des graphes attribués est difficile car elle nécessite de combiner exploration de la structure du graphe avec identification itemsets fréquents de plus du fait de explosion combinatoire des itemsets les isomorphismes de sous-graphes dont la présence impacte énormément les performances des algorithmes de fouille sont beaucoup plus nombreux que dans les graphes étiquetés dans cet article nous présentons une nouvelle méthode de fouille de données qui permet extraire des motifs fréquents à partir un ou de plusieurs graphes orientés attribués nous montrons comment réduire explosion combinatoire provoquée par les isomorphismes de sous-graphes en traitant de manière particulière les motifs automorphes\n",
      "extraction de règles épisodes minimales dans des séquences complexes\n",
      "les messages déposés quotidiennement sur les réseaux sociaux et les blogs sont très nombreux et constituent une source informations précieuse leur fouille peut être utilisée dans un but de prédiction informations. notre objectif dans cet article est de proposer un algorithme permettant la prédiction informations au plus tôt et de façon fiable par le biais de identification de règles d'épisodes\n",
      "extraire les motifs minimaux efficacement et en profondeur\n",
      "les représentations condensées ont fait objet de nombreux travaux depuis 15 ans tandis que les motifs maximaux des classes équivalence ont reçu beaucoup d'attention les motifs minimaux sont restés dans ombre notamment à cause de la difficulté de leur extraction dans ce papier nous présentons un cadre générique concernant extraction de motifs minimaux en introduisant la notion de système minimisable d'ensembles il permet de considérer des langages variés comme les motifs ensemblistes ou les chaînes de caractères mais aussi différentes métriques dont la fréquence ensuite pour n'importe quel système minimisable d'ensembles nous introduisons un test de minimalité rapide permettant extraire en profondeur les motifs minimaux nous démontrons que algorithme proposé est polynomial-delay et polynomial-space des expérimentations sur les benchmarks traditionnels complètent notre étude\n",
      "fouille de données par programmation visuelle structurée avec kd-ariane\n",
      "nous présentons ici la plate-forme kd-ariane un déploiement outils pour la fouille de données dans environnement de programmation visuelle ariane ce déploiement facilite la conception de chaînes structurées de traitements pour extraction de connaissance dans les données\n",
      "fouille de motifs séquentiels pour élicitation de stratégies à partir de traces interactions entre agents en compétition\n",
      "pour atteindre un but tout agent en compétition élabore inévitablement des stratégies lorsque on dispose une certaine quantité de traces interactions entre agents il est naturel utiliser la fouille de motifs séquentiels pour découvrir de manière automatique ces stratégies dans cet article nous proposons une méthodologie qui permet élicitation de stratégies et leur capacité à discriminer une réussite ou un échec la méthodologie s'articule en trois étapes  i) les traces brutes sont transformées en une base de séquences selon des choix qui permettent ii) extraction de stratégies fréquentes iii) lesquelles sont munies une mesure originale d'émergence c'est donc une méthodologie de découverte de connaissances que nous proposons nous montrons intérêt des motifs extraits et la faisabilité de approche à travers des expérimentations quantitatives et qualitatives sur des données réelles issues du domaine émergent du sport électronique\n",
      "généralisation des k-moyennes pour produire des recouvrements ajustables\n",
      "la recherche de groupes non-disjoints à partir de données non-étiquetées est une problématique importante en classification non-supervisée la classification recouvrante overlapping clustering contribue à la résolution de plusieurs problèmes réels qui nécessitent la détermination de groupes qui se chevauchent cependant bien que les recouvrements entre groupes soient tolérés voire encouragés dans ces applications il convient de contrôler leur importance nous proposons dans ce papier des généralisations de k-moyennes offrant le contrôle et le paramétrage des recouvrements deux principes de régulation sont mis en place ils visent à contrôler les recouvrements relativement à leur taille et à la dispersion des classes les expérimentations réalisées sur des jeux de données réelles montrent intérêt des principes proposés\n",
      "génération un extrait textuel à partir de bases de données\n",
      "dans ce papier nous présentons une approche dédiée à la transformation une base de données en un extrait textuel idée sous-jacente à notre proposition est apporter plus de sémantique aux données de la base cet objectif est atteint moyennant utilisation des ontologies comme ressources sémantiques notre approche prend comme input un ensemble de bases de données et associe à chacune une ontologie une ontologie globale est générée à partir de laquelle des règles association sont proposées pour mieux expliciter sa sémantique enfin la génération un extrait textuel prend lieu\n",
      "granularité des motifs de co-variations dans des graphes attribués dynamiques\n",
      "découvrir des connaissances dans des graphes qui sont dynamiques et dont les sommets sont attribués est de plus en plus étudié par exemple dans le contexte de analyse interactions sociales il est souvent possible expliciter des hiérarchies sur les attributs permettant de formaliser des connaissances a priori sur les descriptions des sommets nous proposons étendre des techniques de fouille sous contraintes récemment proposées pour analyse de graphes attribués dynamiques lorsque on exploite de telles hiérarchies et donc le potentiel de généralisation/spécialisation qu'elles permettent nous décrivons un algorithme qui calcule des motifs de co-évolution multi-niveaux c'est à dire des ensembles de sommets qui satisfont une contrainte topologique et qui évoluent de la même façon selon un ensemble de tendances et de pas de temps nos expérimentations montrent que utilisation une hiérarchie permet extraire des collections de motifs plus concises sans perdre d'information\n",
      "identification de classes non-disjointes ayants des densités différentes\n",
      "la classification recouvrante correspond à un enjeu important en classification non-supervisée en permettant à une observation appartenir à plusieurs clusters plusieurs méthodes ont été proposées pour faire face à cette problématique en utilisant plusieurs approches usuelles de classification cependant malgré efficacité de ces méthodes à déterminer des groupes non-disjoints elles échouent lorsque les données comportent des groupes de densités différentes car elles ignorent la densité locale de chaque groupe et ne considèrent que la distance euclidienne entres les observations afin de détecter des groupes non-disjoints de densités différentes nous proposons deux méthodes de classification intégrant la variation de densité des différentes classes dans le processus de classification des expériences réalisées sur des ensembles de données artificielles montrent que les méthodes proposées permettent obtenir de meilleurs performances lorsque les données contiennent des groupes de densités différentes\n",
      "identification de rôles communautaires dans des réseaux orientés appliquée à twitter\n",
      "la notion de structure de communautés est particulièrement utile pour étudier les réseaux complexes car elle amène un niveau analyse intermédiaire par opposition aux plus classiques niveaux local voisinage des noeuds et global réseau entier) le concept de rôle communautaire permet de décrire le positionnement un noeud en fonction de sa connectivité communautaire cependant les approches existantes sont restreintes aux réseaux non-orientés utilisent des mesures topologiques ne considérant pas tous les aspects de la connectivité communautaire et des méthodes identification des rôles non-généralisables à tous les réseaux nous proposons de résoudre ces problèmes en généralisant les mesures existantes et en utilisant une méthode non-supervisée pour déterminer les rôles nous illustrons intérêt de notre méthode en appliquant au réseau de twitter nous montrons que nos modifications mettent en évidence les rôles spécifiques utilisateurs particuliers du réseau nommés capitalistes sociaux\n",
      "incremental learning with latent factor models for attribute prediction in social-attribute networks\n",
      "dans ce travail nous nous intéressons au problème de la prédiction attributs sur les noeuds dans un réseau social la plupart des techniques sont hors ligne et ne sont pas adaptées à des situations où les données arrivent massivement en flux comme dans le cas des médias sociaux dans ce travail nous utilisons les modèles de variables latentes pour prédire les attributs inconnus des noeuds dans un réseau social et proposer une méthode pour mettre à jour incrémentalement le modèle avec des nouvelles données des expérimentations sur un jeu de données issues des médias sociaux montrent que notre méthode est moins coûteuse en temps de calcul et peut garantir des performances acceptables en comparaison avec les techniques non-incrémentales de état de l'art\n",
      "intégration de plusieurs formes de représentations spatiales dans un modèle de simulation\n",
      "in this paper we focus on modeling expert knowledge for simulating complex landscape spatial dynamics one modeling tool to do that is the ocelet modeling language that user interaction graphs to describe spatial dynamics most present approaches impose an a prior choice of spatial format between i) a vector format representing the shapes of the entities or ii) a gridding of space into regular elements (raster) in this paper we show how ocelet was extended to support the interaction semantics between these two spatial formats vector and raster) as case study we present a run off model in a tropical insular environment\n",
      "intégration et visualisation de données liées thématiques sur un référentiel géographique\n",
      "de nombreuses ressources publiées sur le web des données sont décrites par une composante qui désigne une manière directe ou indirecte une localisation géographique comme toute autre propriété cette information de localisation peut être mise à profit pour permettre interconnexion des données avec autres sources elle permet en outre leur représentation cartographique cependant les informations de localisation utilisées dans les sources de données linked data peuvent parfois s'avérer imprécises ou hétérogènes une source à l'autre ceci rend donc leur exploitation pour réaliser une interconnexion difficile voire impossible dans cet article nous proposons de pallier ces difficultés en ancrant les données linked data thématiques aux objets un référentiel géographique nous mettons à profit le référentiel géographique afin de mettre en correspondance des données thématiques dotées indications de localisation hétérogènes nous exploitons enfin les relations de correspondance créées entre données thématiques et référentiel géographique dans une application de visualisation cartographique des données\n",
      "investigation visuelle événements dans un grand flot de liens\n",
      "nous présentons une nouvelle méthode analyse exploratoire de grands flots de liens que nous appliquons à la détection événements significatifs dans plus de 2 millions interactions pendant 4 mois entre utilisateurs du réseau social en ligne github nous combinons une méthode statistique de détection automatique événements dans une série temporelleoutskewer avec un système de visualisation de graphes outskewer identifie des instants de évolution du graphe interactions méritant être étudiés et un analyste peut valider et interpréter ces événements par la visualisation de motifs anormaux dans les sous-graphes correspondants nous montrons par de multiples exemples que cette approche 1 permet de détecter des événements pertinents et de rejeter ceux qui ne le sont pas 2 est adaptée à une démarche exploratoire car elle ne nécessite pas de connaissance a priori sur les données\n",
      "utilisation des entités nommées pour expansion sémantique des requêtes web\n",
      "les entités nommées sont des éléments intéressants pour les applications fondées sur le traitement du langage naturel dans le cas de la recherche d'information les entités nommées sont largement employées par les utilisateurs du web dans les requêtes de recherche soit pour définir un concept de base soit pour décrire un autre concept dans la requête du côté du modèle de recherche les entités nommées sont des éléments riches en information qui aident à mieux cibler les documents pertinents dans cet article nous étudions avantage étendre les entités nommées dans la requête idée est utiliser une technique expansion sémantique sur une ontologie générale yago) pour désambiguïser les entités nommées et pour trouver leurs différentes appelations que on intègre dans la requête en utilisant 3 approches  sac de mots dépendance séquentielle et concept clé nous mesurons efficacité de ces expériences en termes de précision et rappel et nous étudions effet du rôle des entités nommées sur expansion nous concluons que expansion des entités nommées est une méthode simple qui améliore significativement la qualité de la recherche quand elle est comparée à un modèle de référence sans expansion de plus cette méthode est assez compétitive par rapport à approche pseudo retour de pertinence souvent utilisée pour expansion de la requête\n",
      "la subjectivité dans le discours médical  sur les traces de incertitude et des émotions\n",
      "les acteurs et usagers du domaine médical (médecins infirmiers patientsinternes pharmaciens etc. ne sont pas issus de la même catégorie socioprofessionnelle et ne présentent pas le même niveau de maîtrise du domaine leurs écrits en témoignent et véhiculent de plus la subjectivité qui leur est propre nous nous intéressons à étude automatisée de la subjectivité dans le discours médical dans des textes en langue française nous confrontons le discours des médecins articles scientifiques rapports cliniques à celui des patients messages de forums de santé en analysant contrastivement les différences emploi des descripteurs tels que les marqueurs incertitude et de polarité les marques émotives non lexicales (smileys ponctuations répétées etc. et lexicales et les termes médicaux relatifs aux pathologies traitements et procédures nous effectuons une annotation et catégorisation automatiques des documents afin de mieux observer les spécificités que présentent les discours médicaux ciblés\n",
      "les nouvelles théories de incertain\n",
      "la notion incertitude a été longtemps un sujet de controverses en particulier la préémincence de la théorie des probabilités dans les sciences tend à gommer les différences présentes dans les premières tentatives de formalisation remontant au 17ème siècle entre incertitude due à la variabilité des phénomènes répétables et incertitude due au manque information dite épistémique) école bayésienne affirme que quelle que soit origine de incertitude, celle-ci peut être modélisée par une distribution de probabilité unique cette affirmation a été beaucoup remise en cause dans les trente dernières années en effet emploi systématique une distribution unique en cas information partielle mène à des utilisations paradoxales de la théorie des probabilités dans de nombreux domaines il est crucial de distinguer entre incertitude due à la variabilité observations et incertitude due à ignorance partielle cette dernière peut être réduite par obtention de nouvelles informations mais pas la première dont on ne se prémunit que par des actions concrètes dans le cas des bases de données il est souvent supposé qu'elles sont précises et incertitude correspondante est souvent négligée quant elle est abordée on reste souvent dans une approche probabiliste orthodoxe néanmoins les statisticiens ont développé des outils qui ne relèvent pas de la théorie de kolmogorov pour pallier le manque de données intervalles de confiance principe de maximum de vraisemblance) de nouvelles théories de incertain ont émergé qui offrent la possibilité de représenter les incertitudes épistémiques et aléatoires de façon distincte notamment incertitude épistémique en remplaçant la distribution de probabilité unique par une famille de distributions possibles cette famille étant autant plus grande que information est absente cette représentation complexe possède des cas particuliers plus simples à utiliser en pratique comme les ensembles aléatoires théorie des fonctions de croyance) les distributions de possibilité représentant des ensembles flous de valeurs possibles et les p-boxes notamment le but de cet exposé est de susciter intérêt pour ces nouvelles théories de incertain, en donner les bases formelles en discuter la philosophie sous-jacente de faire le lien avec certaines notions en statistique et de les illustrer sur des exemples\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local-generator  \"diviser pour régner\" pour extraction des traverses minimales un hypergraphe\n",
      "du fait qu'elles apportent des solutions dans de nombreuses applications les traverses minimales des hypergraphes ne cessent de susciter intérêt de la communauté scientifique et le développement algorithmes pour les calculer dans cet article nous présentons une nouvelle approche pour optimisation de extraction des traverses minimales basée sur les notions hypergraphe partiel et de traverses minimales locales selon une stratégie diviser pour régner nous introduisons aussi un nouvel algorithme appelé local-generator pour le calcul des traverses minimales les expérimentations effectuées sur divers jeux de données ont montré intérêt de notre approche notamment sur les hypergraphes ayant un nombre de transversalité élevé et renfermant un nombre très important de traverses minimales\n",
      "méthodologie 3-way extraction un modèle articulatoire de la parole à partir des données un locuteur\n",
      "pour parler le locuteur met en mouvement un ensemble complexe articulateurs  la mâchoire qu'il ouvre plus ou moins la langue à laquelle il fait prendre de nombreuses formes et positions les lèvres qui lui permettent de laisser air s'échapper plus ou moins brutalement etc le modèle articulatoire le plus connu est celui de maeda (1990) obtenu à partir analyses en composantes principales faites sur les tableaux de coordonnées des points des articulateurs un locuteur en train de parler nous proposons ici une analyse 3-way du même type de données après leur transformation en tableaux de distances nous validons notre modèle par la prédiction des sons prononcés qui s'avère presque aussi bonne que celle du modèle acoustique et même meilleure quand on prend en compte la co-articulation\n",
      "mining the crowd\n",
      "harnessing a crowd of web users for data collection has recently become a wide-spread phenomenon a key challenge is that the human knowledge forms an open world and it is thus difficult to know what kind of information we should be looking for classic databases have addressed this problem by data mining techniques that identify interesting data patterns these techniques however are not suitable for the crowd this is mainly due to properties of the human memory such as the tendency to remember simple trends and summaries rather than exact details following these observations we develop here a novel model for crowd mining we will consider in the talk the logical algorithmic and methodological foundations needed for such a mining process as well as the applications that can benefit from the knowledge mined from crowd\n",
      "modélisation de trajectoires cible/caméra  requêtes spatio-temporelles dans le cadre de la videosurveillance\n",
      "le nombre de caméras de vidéosurveillance installées dans le monde augmente chaque jour en france le système de la ratp déployé sur paris comprend 9000 caméras fixes et 19000 mobiles lors de faits particuliers (e.g. agressions vols) les opérateurs de vidéo surveillance se basent sur les indications spatiales et temporelles de la victime et sur leur connaissance de la localisation des caméras pour sélectionner les contenus intéressants pour l'enquête deux grands problèmes peuvent alors survenir  1) le temps de réponse est long jusqu'à plusieurs jours de traitement et 2) un risque important de perte de résultats à cause une mauvaise connaissance du terrain appel à des opérateurs extérieurs) le but de notre recherche est de définir des outils assistance aux opérateurs qui puissent à partir une trajectoire donnée sélectionner de façon automatique les caméras pertinentes par rapport à la requête\n",
      "motifs récursifs  extraction ascendante hiérarchique ensembles items ou évènements pour le résumé de données transactionnelles ou séquentielles\n",
      "nous proposons une méthode originale pour extraire un résumé compact représentatif et intelligible des motifs fréquents dans des données transactionnelles ou séquentielles notre approche consiste à extraire un nouveau type de motifs que nous appelons motifs récursifs ie des motifs de motifs à aide un algorithme hiérarchique agglomératif nommé repaminer nous générons non pas un simple ensemble de motifs mais une véritable structure dérivée de dendrogrammes le rpgraph\n",
      "passage aux noyaux en classification recouvrante\n",
      "la classification recouvrante correspond à un domaine étude très actif ces dernières années et dont objectif est organiser un ensemble de données en groupes individus similaires avec la particularité autoriser des chevauchements entre les groupes parmi les approches étudiées nous nous intéressons aux extensions recouvrantes des modèles de type moindres carrés et constatons les difficultés théoriques et pratiques liées à leur adaptation aux noyaux nous formulons alors une nouvelle définition ensembliste pour caractériser un recouvrement de plusieurs classes nous montrons que cette modélisation permet le recours aux noyaux et nous proposons une solution algorithmique efficace pour répondre au problème de la classification recouvrante à noyaux\n",
      "pondération de blocs de variables en bi-partitionnement topologique\n",
      "dans cet article nous proposons une nouvelle approche permettant à la fois le bi-partitionnement topologique bi-clustering) et la pondération de blocs variables le modèle que nous proposons fbr-bitm feature block relevance using bitm permet de découvrir un espace topologique un ensemble observations et de variables en associant un nouveau score de pondération à chaque sous ensemble de variables estimation des coefficients de pondération est réalisée dans le même processus apprentissage que le bi-partitionnement ces pondérations sont locales et associées à chaque prototype elles reflètent importance locale de chaque bloc de variables pour le bi-partitionnement évaluation montre que approche proposée comparée\n",
      "prédiction de valeurs manquantes dans les bases de données— une première approche fondée sur la notion de proportion analogique\n",
      "cet article présente une méthode originale de prédiction de valeurs manquantes dans les bases de données relationnelles fondée sur la notion de proportion analogique nous montrons en particulier comment un algorithme proposé dans le cadre de la classification automatique peut être adapté à cette fin deux cas sont considérés  celui une base de données transactionnelle attributs booléens) et celui où les valeurs manquantes peuvent être de type numérique\n",
      "que ressentent les patients \n",
      "les forums de santé en ligne sont des espaces échanges où les patients partagent leurs sentiments à propos de leurs maladies traitements etc sous couvert d'anonymat ils expriment très librement leurs expériences personnelles ces forums sont donc une source informations très utile pour les professionnels de santé afin de mieux identifier et comprendre les problèmes les comportements et les sentiments de leurs patients dans cet article nous proposons exploiter les messages des forums via des techniques de fouille de textes pour extraire des traces émotions (eg joie colère surprise  etc).\n",
      "réconciliation des profils dans les réseaux sociaux\n",
      "it is not uncommon that individuals create multiple profiles across several snss each containing partially overlapping sets of personal information as a result the creation of a global profile that gives an holistic view of the information of an individual requires methods that automatically match or reconciliates profiles across snss in this paper we focus on the problem of identifying or matching the profiles of any individual across social networks\n",
      "reconstruction et analyse sémantique de chronologies cybercriminelles\n",
      "la reconstruction de chronologies évènements cybercriminels ou reconstruction évènements) est une étape primordiale dans une investigation numérique cette phase permet aux enquêteurs avoir une vue des évènements survenus durant un incident la reconstruction évènements requiert étude importants volumes de données en raison de omniprésence des nouvelles technologies dans notre quotidien de plus les conclusions produites se doivent de respecter les critères fixés par la justice afin de répondre à ces challenges nous proposons une nouvelle méthodologie basée sur une ontologie permettant assister les enquêteurs tout au long du processus d'enquête\n",
      "règles association inter-langues au service de la recherche information multilingue\n",
      "dans cet article nous proposons de montrer intérêt et utilité de déploiement des règles association inter-langues rails) dans le domaine de la recherche information multilingue (rim) ces règles sont des connaissances additionnelles résultantes un processus de fouille de grands corpus parallèles alignés au niveau de la phrase en effet leurs conclusions exprimées dans une langue cible représentent des traductions potentielles de leurs prémisses exprimées dans une langue source nous illus trons utilisation des rails dans le contexte de la rim à travers deux propositions à savoir  i) la traduction des requêtes et ii) la traduction des termes de l'index évaluation expérimentale a été menée sur la collection de documents muchmore les résultats ont montré une amélioration significative de la pertinence système\n",
      "representative training sets for classification and the variability of empirical distributions\n",
      "we propose a novel approach for the estimation of the size of training sets that are needed for constructing valid models in machine learning and datamining we aim to provide a good representation of the underlying population without making any distributional assumptions our technique is based on the computation of the standard deviation of the \u001f2-statistics of a series of samples when successive statistics are relatively close we assume that the samples produced represent adequately the true underlying distribution of the population and the models learned from these samples will behave almost as well as models learned on the entire population we validate our results by experiments involving classifiers of various levels of complexity and learning capabilities\n",
      "requêtes skyline en présence exceptions\n",
      "dans cet article nous nous intéressons à la recherche des points les plus intéressants au sens de ordre de pareto i.e. à évaluation de requêtes « skyline »  dans des jeux de données présentant des anomalies il n'est pas rare que les données de petites annonces par exemple soient peuplées erreurs ou exceptions qui peuvent perturber la recherche des meilleurs points car celles-ci sont susceptibles de dominer les autres points approche présentée vise à calculer les requêtes skyline malgré la présence de ces exceptions sans pour autant les écarter définitivement et à présenter graphiquement les résultats de façon à identifier rapidement les points intérêt et les anomalies potentielles\n",
      "sélection une méthode de classification multi-label pour un système interactif\n",
      "objectif de cet article est évaluer la capacité de 12 algorithmes de classification multi-label à apprendre en peu de temps avec peu exemples d'apprentissage les résultats expérimentaux montrent des différences importantes entre les méthodes analysées pour les 3 mesures évaluation choisies  log-loss ranking-loss et temps d'apprentissage/prédiction et les meilleurs résultats sont obtenus avec multi-label k nearest neighbours (ml-knn) suivi de ensemble de classifier chains ecc) et ensemble de binary relevance (ebr)\n",
      "sélection de prototypes en vue une catégorisation de textes avec les k plus proches voisins  étude comparative\n",
      "la technique des k plus proches voisins knn) est une méthode apprentissage à base d'instances elle a été appliquée dans la catégorisation de textes depuis de nombreuses années en contraste avec ses performances de classification il est reconnu que cet algorithme est lent pendant la classification un nouveau document les techniques de sélection de prototypes sont apparues comme des méthodes très compétitives pour améliorer le knn grâce à la réduction des données étude contenue dans ce papier a pour objectif analyser impact de ces méthodes sur la performance de la classification de textes avec algorithme knn\n",
      "sous échantillonnage et machine à noyaux élastiques pour la classification de données de mouvement capturé\n",
      "dans le domaine de la reconnaissance de gestes isolés bon nombre de travaux se sont intéressés à la réduction de dimension sur axe spatial pour réduire à la fois la complexité algorithmique et la variabilité des réalisations gestuelles il est assez étonnant de constater que peu de ces méthodes se sont explicitement penchées sur la réduction de dimension sur axe temporel en matière de complexité la réduction de dimension sur cet axe est un enjeu majeur quant à utilisabilité de distances élastiques en complexité quadratique par ailleurs la prise en compte de la variabilité sur cet axe demeure une source avérée de gain de performance pour tenter apporter un éclairage en matière de réduction de dimension sur axe temporel nous présentons dans cet article une approche basée sur un sous échantillonnage temporel associé à exploitation un apprentissage automatique à base de noyaux élastiques nous montrons expérimentalement sur deux jeux de données très référencés dans la communauté et très opposés en matière de qualité de capture de mouvement qu'il est possible de réduire sensiblement le nombre de postures sur les trajectoires temporelles tout en conservant grâce à des noyaux élastiques des performances de reconnaissance au niveau de état de art du domaine le gain de complexité obtenu rend une telle approche éligible pour des applications temps-réel\n",
      "stratégies argumentatives pour la classification collaborative multicritères des connaissances cruciales\n",
      "dans cet article nous proposons une approche argumentative visant à automatiser la résolution des conflits entre les décideurs qui ont des préférences contradictoires lors une classification multicritères collaborative des connaissances cruciales notre étude expérimentale a prouvé que cette approche peut résoudre jusqu'à 81% des conflits et améliorer la qualité approximation de décideurs un taux de 0.62 pour un récepteur et de 0.15 pour un initiateur\n",
      "symétries et extraction de motifs ensemblistes\n",
      "les symétries sont des propriétés structurelles qu'on détecte dans un grand nombre de bases de données dans cet article nous étudions exploitation des symétries pour élaguer espace de recherche dans les problèmes extraction de motifs ensemblistes notre approche est basée sur une intégration dynamique des symétries dans les algorithmes de type a priori permettant de réduire espace des motifs candidats en effet pour un motif donné les symétries nous permettent de déduire les motifs qui lui sont symétriques et vérifiant par conséquent les mêmes propriétés nous détaillons notre approche en utilisant exemple des motifs fréquents ensuite nous la généralisons au cadre unificateur de mannila et toivonen pour extraction des motifs ensemblistes les expériences menées montrent la faisabilité et apport de notre approche élagage basé sur les symétries\n",
      "the hitchhiker's guide to ontology\n",
      "artificial intelligence has long had the dream of making computers smarter for quite sometime this vision has remained just that a dream with the development of large knowledge bases though we now have large amounts of semantic information at our hands this changes the game of ai computers have indeed become smarter in this talk we present the latest developments in the field the construction of general purpose knowledge bases including yago and dbpedia as well as nell and textrunner) and their applications to tasks that were previously out of scope the extraction of fine-grained information from natural language texts semantic query answering and the interpretation of newspaper texts at large scale\n",
      "un système de détection de thématiques populaires sur twitter\n",
      "with the ever-growing amount of messages exchanged via twitter there is an increasing interest in filtering this information which is delivered under the form of a stream of messages in this paper we present a system for detecting popular topics in twitter the system can be applied to static corpora and can also handle the live twitter stream\n",
      "une approche algébrique au problème du consensus de partitions\n",
      "en classification non-supervisée le consensus de partitions a pour objectif de produire une partition unique représentant le consensus à partir un ensemble de partitions où chacune est engendrée indépendamment des autres voire avec des méthodologies différentes en complément des techniques ayant leur qualité propre en terme de robustesse ou de passage à l'échelle nous apportons un point de vue original sur le consensus de partitions c'est-à-dire par le biais de définitions algébriques qui permettent établir la nature des déductions pouvant être réalisées dans une approche systématique (pex un système à base de connaissances) nous fondons notre approche sur le treillis des partitions pour lequel nous montrons comment peuvent être adjoint des opérateurs dans le but de formuler une expression caractérisant le consensus à partir un ensemble de partitions\n",
      "une approche basée sur statis pour la fusion de cartes topologiques auto-organisées\n",
      "dans le cadre des cartes topologiques nous proposons une nouvelle approche ensemble clusters basée sur la méthode statis les méthodes ensemble clusters visent à améliorer la qualité de la partition un jeu de données à travers la combinaison de plusieurs partitions les différentes partitions peuvent être obtenues en faisant varier les paramètres un algorithme choix des centres initiaux du voisinage initial et final des cellules dans le cas des cartes topologiques auto-organisée som etc) approche présentée dans cette communication repose sur la méthode analyse de données multi-tableaux statis pour déterminer une matrice compromis représentant au mieux la similarité entre les partitions issues des cartes topologiques la fusion des cartes topologiques est alors obtenue à travers une classification basée sur cette matrice compromis la méthode proposée est illustrée sur des données réelles issues de uci et sur des données simulées\n",
      "une approche ppc pour la fouille de données séquentielles\n",
      "nous proposons dans cet article une nouvelle approche croisant des techniques de programmation par contraintes et de fouille pour extraction de motifs séquentiels le modèle que nous proposons offre un cadre générique et déclaratif pour modéliser et résoudre des contraintes de nature hétérogène\n",
      "une approcheweb sémantique et combinatoire pour un système de recommandation sensible au contexte appliqué à apprentissage mobile\n",
      "au vu de émergence rapide des nouvelles technologies mobiles et la croissance des offres et besoins une société en mouvement les travaux se multiplient pour identifier de nouvelles plateformes apprentissage pertinentes afin améliorer et faciliter apprentissage à distance la prochaine étape de apprentissage à distance est naturellement le port de e-learning apprentissage électronique vers les nouveaux systèmes mobiles on parle de m-learning apprentissage mobile) nos travaux portent sur le développement une nouvelle architecture pour le m-learning dont objectif est adapter et recommander des parcours de formations selon les contraintes contextuelles de l'apprenant\n",
      "une heuristique pour le paramétrage automatique de algorithme de clustering spectral\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trouver le nombre optimal de groupes dans le contexte un algorithme de clustering est un problème notoirement difficile dans cet article nous en décrivons et évaluons une solution approchée dans le cas de algorithme spectral notre méthode présente avantage être déterministe et peu coûteuse nous montrons qu'elle fonctionne de manière satisfaisante dans beaucoup de cas même si quelques limites amènent des perspectives à ce travail\n",
      "une méthode hybride pour la prédiction du profil des auteurs\n",
      "dans cet article nous nous intéressons à la détection du profil des auteurs (âge genre à travers leurs discussions la méthode proposée s'appuie sur la classification automatique qui utilise certaines données extraites une manière statisque à partir de corpus source nous présentons une méthode hybride qui combine analyse de surface dans les textes avec une méthode apprentissage automatique a fin obtenir une meilleure gestion de ces données nous nous sommes basés sur utilisation des arbres de décision notre méthode a donné des résultats intéressants pour la détection du genre\n",
      "une méthode pour caractériser les communautés des réseaux dynamiques à attributs\n",
      "de nombreux systèmes complexes sont étudiés via analyse de réseaux dits complexes ayant des propriétés topologiques typiques parmi celles-ci les structures de communautés sont particulièrement étudiées de nombreuses méthodes permettent de les détecter y compris dans des réseaux contenant des attributs nodaux des liens orientés ou évoluant dans le temps la détection prend la forme une partition de ensemble des noeuds qu'il faut ensuite caractériser relativement au système modélisé nous travaillons sur assistance à cette tâche de caractérisation nous proposons une représentation des réseaux sous la forme de séquences de descripteurs de noeuds qui combinent les informations temporelles les mesures topologiques et les valeurs des attributs nodaux les communautés sont caractérisées au moyen des motifs séquentiels émergents les plus représentatifs issus de leurs noeuds ceci permet notamment la détection de comportements inhabituels au sein une communauté nous décrivons une étude empirique sur un réseau de collaboration scientifique\n",
      "une méthode pour la détection de thématiques populaires sur twitter\n",
      "explosion du volume de messages échangés via twitter entraîne un phénomème de surcharge informationnelle pour ses utilisateurs il est donc crucial de doter ces derniers de moyens les aidant à filtrer information brute laquelle est délivrée sous la forme un flux de messages dans cette optique nous proposons une méthode basée sur la modélisation de anomalie dans la fréquence de création de liens dynamiques entre utilisateurs pour détecter les pics de popularité et extraire une liste ordonnée de thématiques populaires les expérimentations menées sur des données réelles montrent que la méthode proposées est capable identifier et localiser efficacement les thématiques populaires\n",
      "une nouvelle approche pour la sélection de variables basée sur une métrique estimation de la qualité\n",
      "la maximisation étiquetage f-max) est une métrique non biaisée estimation de la qualité une classification non supervisée clustering) qui favorise les clusters ayant une valeur maximale de f-mesure étiquetage. dans cet article nous montrons qu'une adaptation de cette métrique dans le cadre de la classification supervisée permet de réaliser une sélection de variables et de calculer pour chacune elles une fonction de contraste la méthode est expérimentée sur différents types de données textuelles dans ce contexte nous montrons que cette technique améliore les performances des méthodes de classification de façon très significative par rapport à état de art des techniques de sélection de variables notamment dans le cas de la classification de données textuelles déséquilibrées fortement multidimensionnelles et bruitées\n",
      "utilisation de relations ontologiques pour la comparaison images décrites par des annotations sémantiques\n",
      "face à la complexité des nouvelles générations images médicales les processus de recherche images basés sur leurs contenus visuels peuvent s'avérer insuffisants cet article propose une nouvelle approche basée sur annotation des images via des termes sémantiques pouvant pallier ce problème elle repose sur la combinaison une distance hiérarchique permettant de comparer les images en considérant les corrélations entre les termes utilisés pour les décrire et une mesure de similarité permettant évaluer la proximité sémantique entre des termes ontologiques cette approche est validée dans le cadre de la recherche images tomodensitométriques\n",
      "vectorisation paramétrée des données textuelles\n",
      "automatic processing of textual data enables users to analyze semi-automatically and on a large scale the data this analysis is based on two successive processes i) representation of texts ii) gathering of textual data (clustering) the software described in this paper focuses on the first step of the process by offering expert a parameterized representation of textual data\n",
      "vers une classification non supervisée adaptée pour obtenir des arbres de décision simplifiés\n",
      "induction arbre de décision est une technique puissante et populaire pour extraire de la connaissance néanmoins les arbres de décision obtenus depuis des données issues du monde réel peuvent être très complexes et donc difficiles à exploiter dans ce cadre cet article présente une solution originale pour adapter le résultat une classification non supervisée quelconque afin obtenir des arbres de décision simplifiés pour chaque cluster\n",
      "vers une modularité pour données vectorielles\n",
      "la modularité introduite par newman pour mesurer la qualité une partition des sommets un graphe ne prend pas en compte éventuelles valeurs associées à ces sommets dans cet article nous introduisons une mesure de modularité complémentaire basée sur inertie et adaptée pour évaluer la qualité une partition éléments représentés dans un espace vectoriel réel cette mesure se veut un pendant pour la classification non supervisée de la modulatié de newman nous présentons également 2mod-louvain une méthode utilisant ce critère de modularité basée sur inertie conjointement à la modularité de newman pour détecter des communautés dans des réseaux d'information les expérimentations que nous avons menées ont montré qu'en exploitant à la fois les données relationnelles et vectorielles 2mod-louvain détectait plus efficacement les communautés que des méthodes utilisant un seul type de données et qu'elle était robuste face à des dégradations des données\n",
      "visualisation de données de prosopographie pour la reconstruction de carrières de personnages et de réseaux socio-professionnels\n",
      "dans cet article nous présentons deux approches de visualisation développées dans le cadre un projet collaboratif sur accès et exploitation des données prosopographiques de la renaissance en france objectif du projet est de modéliser et réaliser un portail sémantique assurant accès à différentes bases de données prosopographiques existantes afin de permettre une meilleure exploration et exploitation de ces données dans ce cadre nous avons proposé deux interfaces de visualisation prosograph et prosomap qui s'appuient respectivement sur la visualisation de graphes de réseaux sociaux et la visualisation de lieux géographiques et de trajectoires spatio-temporelles les deux interfaces communiquent avec le portail via une couche sémantique et lui offrent des fonctionnalités interrogation supplémentaires\n",
      "20 ans de découverte de motifs  une étude bibliographique quantitative\n",
      "depuis deux décennies la découverte de motifs a été un des champs de recherche les plus actifs de exploration de données cet article en établit une étude bibliographique quantitative en nous appuyant sur 1030 publications issues de 5 conférences internationales majeures  kdd pkdd pakdd icdm et sdm nous avons abord mesuré depuis 2005 un sévère ralentissement de activité de recherche dédiée à la découverte de motifs puis nous avons quantifié les principales contributions en terme de langages de contraintes et de représentations condensées de sorte à comprendre ce ralentissement et à esquisser les directions actuelles\n",
      "3d  de nouvelles perspectives en fouille exploratoire avec la stéréoscopie\n",
      "si la 3d est un sujet de débat dans la communauté les expériences sur lesquelles s'appuient les discussions concernent le plus souvent des restitutions visuelles basées sur une projection classique en perspective linéaire objectif de cette communication est de renouveler le cadre expérimental en étudiant impact de ajout de la disparité binoculaire nous nous focalisons ici sur une tâche importante en analyse de réseaux  identification de communautés et nous comparons la 3d monoscopique et la 3d stéréoscopique à la fois pour la performance de résolution de la tâche et pour le comportement exploratoire à travers analyse du mouvement du pointeur de la souris et de la dynamique des modifications de points de vue sur les graphes nos résultats expérimentaux mettent en évidence des performances significativement meilleures pour la 3d stéréoscopique et des différences comportementales dans exploration avec un centrage plus important sur des zones restreintes en stéréoscopie\n",
      "a pos tagger analysed in collaboration environments and literary texts\n",
      "part-of-speech pos) tagging is often used in other modules of natural language processing and therefore the results of this process should be as precise as possible many different types of taggers have been developed to improve the accuracy of the results in the field of literature or newspapers nowadays when the internet is widespread the environments for online collaboration as chats forums blogs wikis have become important means of communication the purpose of this research is to analyse the results of tagging the words obtained from the labelling of the words from the online collaboration environments and literary texts with the corresponding parts of speech in the case of pos tagging the ambiguities arise due to the fact that a word may have multiple morphological values depending on context\n",
      "accélération de la méthode des k plus proches voisins pour la catégorisation de textes\n",
      "parmi la panoplie de classificateurs utilisés dans la catégorisation de textes nous nous intéressons à algorithme des k-voisins les plus proches ces performances le situent parmi les meilleures méthodes de catégorisation de textes toutefois il présente certaines limites i) coût mémoire car il faut stocker ensemble apprentissage en entier et ii) coût élevé de calcul car il doit explorer ensemble apprentissage pour classer un nouveau document dans ce papier nous proposons une nouvelle démarche pour réduire ce temps de classification sans dégrader les performances de classification\n",
      "analyse conceptuelle de données de simulation de systèmes complexes pour aide à la décision  application à la conception une cabine avion\n",
      "dans cet article nous présentons une approche conceptuelle aide à la décision dans la conception de systèmes complexes cette approche s'appuie sur le formalisme de analyse de concepts formels par similarité acfs) pour la classification la visualisation et exploration de données de simulation afin aider les concepteurs de systèmes complexes à identifier les choix de conception les plus pertinents approche est illustrée sur un cas test de conception de cabine un avion de ligne fourni par les partenaires industriels et qui consiste à étudier les données de simulation de différentes configurations du système de ventilation de la cabine afin identifier celles qui assurent un confort convenable pour les passagers la cabine la classification des données de simulation avec leurs scores de confort en utilisant acfs permet identifier pour chaque paramètre de conception simulé la plage de valeurs possibles qui assure un confort convenable pour les passagers les résultats obtenus ont été confirmés et validés par de nouvelles simulations\n",
      "analyse de réseaux sociaux par analyse formelle de concepts\n",
      "analyse formelle de concepts afc) est un formalisme de représentation et extraction de connaissance fondé sur les notions de concepts et de treillis de concepts galois).afc a été exploitée avec succès dans plusieurs domaines en informatique tels le génie logiciel les bases et entrepôts de données extraction et la gestion de la connaissance et dans plusieurs applications du monde réel comme la médecine la psychologie la linguistique et la sociologie.dans cette présentation nous allons explorer le potentiel de afc et de quelques extensions de cette théorie (ex analyse triadique de concepts dans analyse de réseaux sociaux en vue de découvrir des connaissances à partir de réseaux homogènes simples (ex détection de communautés et individus influents à partir un réseau d'amis ou même de réseaux hétérogènes (ex extraction de règles association un réseau bibliographique)\n",
      "analyse des réclamations allocataires de la caf  un cas étude en fouille de données\n",
      "la gestion des réclamations est un élément fondamental dans la relation client c'est le cas en particulier pour la caisse nationale des allocations familiales qui veut mettre en place une politique nationale pour faciliter cette gestion dans cet article nous décrivons la démarche que nous avons adoptée afin de traiter automatiquement les réclamations provenant allocataires de la caf du rhône les données brutes mises à notre disposition nécessitent une série importante de prétraitements pour les rendre utilisables une fois ces données correctement nettoyées des techniques issues de analyse des données et de apprentissage non supervisé nous permettent extraire à la fois une typologie des réclamations basée sur leur contenu textuel mais aussi une typologie des allocataires réclamants après avoir présenté ces deux typologies nous les mettons en correspondance afin de voir comment les allocataires se distribuent selon les différents types de réclamation\n",
      "analyse relationnelle de concepts pour exploration de données relationnelles\n",
      "analyse relationnelle de concepts arc) est une extension de analyse formelle de concepts (afc) une méthode de classification non supervisée objets sous forme de treillis de concepts arc supporte en plus la gestion de relations entre objets de différents contextes ce qui permet établir des liens entre les concepts des différents treillis cette particularité lui permet être plus intuitive à utiliser pour extraire des connaissances à partir de données relationnelles et de donner des résultats plus riches malheureusement lorsque les jeux de données présentent de nombreuses relations les résultats obtenus sont difficilement exploitables et des problèmes de passages à échelle se posent nous proposons dans cet article une adaptation possible de arc pour explorer les relations de manière supervisée pour augmenter la pertinence des résultats obtenus et réduire le temps de calcul nous prenons pour exemple des données hydrobiologiques ayant trait à la qualité des milieux aquatiques\n",
      "approche orientée objet sémantique et coopérative pour la classification des images de zones urbaines à très haute résolution\n",
      "la classification orientée objet coo) prend de plus en plus de dimension dans les travaux de télédétection grâce à sa capacité intégrer des connaissances de haut niveau telles que la taille la forme et les informations de voisinage cependant les approches existantes restent tributaires de étape de construction des objets à cause de absence interaction entre celle-ci et celle de leur identification dans cet article nous proposons une approche sémantique hiérarchique et collaborative entre les algorithmes de croissances de régions et une classification orientée objet supervisée permettant une coopération entre extraction et identification des objets de l'image les expériences menées sur une image de très haute résolution de la région de strasbourg ont confirmé intérêt de approche introduite\n",
      "classification multi-étiquettes pour alignement multiple de séquences protéiques\n",
      "cet article présente une application de classification multi-étiquettes permettant de déterminer le programme à utiliser pour construire un alignement multiple un ensemble de séquences protéiques donné dans un premier temps nous avons réussi à améliorer le système existant alexsys en ajoutant des attributs dans un second temps nous déterminons pour un ensemble de séquences protéiques donné le ou les aligneurs capable de produire les alignements de meilleur score à epsilon près les mesures de performances propres à la classification multi-étiquette nous permettent analyser influence de epsilon et de choisir une valeur assez petite pour distinguer les meilleurs aligneurs des autres\n",
      "classifications croisées de données de trajectoires contraintes par un réseau routier\n",
      "le clustering ou classification non supervisée de trajectoires a fait objet un nombre considérable de travaux de recherche la majorité de ces travaux s'est intéressée au cas où les objets mobiles engendrant ces trajectoires se déplacent librement dans un espace euclidien et ne prennent pas en compte les contraintes liées à la structure sous-jacente du réseau qu'ils parcourent (ex réseau routier) dans le présent article nous proposons au contraire la prise en compte explicite de ces contraintes nous représenterons les relations entre trajectoires et segments routiers par un graphe biparti et nous étudierons la classification de ses sommets nous illustrerons sur un jeu de données synthétiques utilité une telle étude pour comprendre la dynamique du mouvement dans le réseau routier et analyser le comportement des véhicules qui l'empruntent\n",
      "comprendre et interpréter les données  enjeux et implantations un système de codage dans des gisements de données historiques\n",
      "accès croissant à une information pléthorique et le développement de gisements de données ambitieux posent aujourd'hui deux grands types de difficultés aux historiens le premier consiste à mettre en relation des gisements qui ont été développés de manière indépendante c'est par exemple le cas pour intégration un ensemble de bases de données prosopographiques développées entre 1980 et 2010 au lamop ou même dans le cadre un projet dont le seul lien est une problématique spatiale et temporelle projet anr-dfg euroscientia) le deuxième tient en la nature des données introduites dans ces différents systèmes  elles sont souvent hétérogènes ambiguës floues pour que le chercheur puisse se les approprier les données doivent faire objet un véritable travail afin de comprendre comment elles ont été obtenues structurées historien doit donc les évaluer et les valider s'il souhaite les mettre en relation cette évaluation nécessitant elle-même de pouvoir être commentée partagée et critiquée par autres chercheurs dans les deux cas il est nécessaire de développer des outils d'appropriation qui permettent entrer dans le réel historique contenu dans les stocks de données c'est là la fonction du projet histobase un système permettant entrer dans la structuration des gisements en évaluer information ajouter des couches interprétation qualification de information historique de les évaluer et de partager les données « obtenues » chacune des analyses individuelles et collectives fait objet une mémorisation il faut pour cela laisser une place importante aux historiens en tant qu'expert en prêtant une attention particulière aux processus métiers qu'ils mettent en oeuvre\n",
      "construction de descripteurs à partir du coclustering pour la classification supervisée de séries temporelles\n",
      "nous présentons un processus de construction de descripteurs pour la classification supervisée de séries temporelles ce processus est libre de tout paramétrage utilisateur et se décompose en trois étapes  i) à partir des données originales nous générons de multiples nouvelles représentations simples  ii) sur chacune de ces représentations nous appliquons un algorithme de coclustering  iii) à partir des résultats de co-clustering nous construisons de nouveaux descripteurs pour les séries temporelles nous obtenons une nouvelle base de données objets-attributs dont les objets identifiant les séries temporelles sont décrits par des attributs issus des diverses représentations générées nous utilisons un classifieur bayésien sur cette nouvelle base de données nous montrons expérimentalement que ce processus offre de très bonnes performances prédictives comparées à état de l'art\n",
      "découverte des soft-skypatterns avec une approche ppc\n",
      "les skypatterns sont des motifs traduisant des préférences de utilisateur selon une relation de dominance dans cet article nous introduisons la notion de souplesse dans la problématique des skypatterns et nous montrons comment celle-ci permet de découvrir des motifs intéressants qui seraient manqués autrement nous proposons une méthode efficace extraction de skypatterns ainsi que de soft-skypatterns méthode fondée sur la programmation par contraintes la pertinence de notre approche est illustrée à travers une étude de cas en chémoinformatique pour la découverte de toxicophores\n",
      "detecting academic plagiarism with graphs\n",
      "in this paper we tackle the problem of detecting academic plagiarism which is considered as a severe problem owing to the convenience of online publishing typical information retrieval methods stopword-based methods and \u0002ngerprinting methods are commonly used to detect plagiarism by using the sequence of words as they appear in the article as such they fail to detect plagiarism when an author reconstructs a source article by re-ordering and recombining phrases because graph structure \u0002ts for representing relationships between entities we propose a novel plagiarism detection method in which we use graphs to represent documents by modeling grammatical relationships between words experimental results show that our proposed method outperforms two n-gram methods and increases recall values by 10 to 20%\n",
      "détection efficace des traverses minimales un hypergraphe par élimination de la redondance\n",
      "extraction des traverses minimales un hypergraphe est une problématique réputée comme particulièrement difficile et qui a fait objet de plusieurs travaux dans la littérature dans cet article nous établissons un lien entre les concepts de la fouille de données et ceux de la théorie des hypergraphes proposant ainsi un cadre méthodologique pour le calcul des traverses minimales le nombre de ces traverses minimales étant souvent exponentiel même pour des hypergraphes simples nous proposons en représenter ensemble de manière concise et exacte pour ce faire nous introduisons la notion de traverses minimales irrédondantes à partir desquelles nous pouvons retrouver ensemble global de toutes les traverses minimales à aide de algorithme imt-extractor une étude expérimentale de ce nouvel algorithme a confirmé intérêt de approche introduite\n",
      "détection précoce de tendances produits dans le cadre des activités commerciales de la grande distribution\n",
      "dans ce papier nous présentons une nouvelle approche qui permet la détection précoce de tendances \"produits\" dans le cadre des activités commerciales de la grande distribution s'agissant un domaine où la concurrence est très vive entre les différentes enseignes avec des enjeux financiers colossaux les stratégies commerciales ont pour principal objectif de fidéliser la clientèle pour limiter leur défection c'est là qu'intervient la détection des changements de tendances produits qui va permettre anticiper attrition de la clientèle déceler des tendances suffisamment tôt permettra aux décideurs de mettre en place des stratégies préventives efficaces à moindre coût notre objectif est donc analyser et de modéliser clairement les changements de tendances et leurs impacts potentiels globaux sur les achats des clients nous illustrerons notre approche sur des données réelles achats de clients une grande enseigne\n",
      "enrichissement ontologies grâce à annotation sémantique de pages web\n",
      "nous présentons une approche pour enrichir automatiquement une ontologie à partir un ensemble de pages web structurées cette approche s'appuie sur un noyau ontologie initial son originalité est exploiter conjointement la structure des documents et des annotations sémantiques produites à aide du noyau ontologie pour identifier de nouveaux concepts et des spécialisations de relations qui enrichissent l'ontologie nous avons implémenté et évalué ce processus en réalisant une ontologie de plantes à partir de fiches de jardinage\n",
      "étude des corrélations spatio-temporelles des appels mobiles en france\n",
      "nous proposons dans cet article de présenter une application analyse une base de données de grande taille issue du secteur des télécommunications le problème consiste à segmenter un territoire et caractériser les zones ainsi définies grâce au comportement des habitants en terme de téléphonie mobile nous disposons pour cela un réseau appels inter-antennes construit pendant une période de cinq mois sur ensemble de la france nous proposons une analyse en deux phases la première couple les antennes émettrices dont les appels sont similairement distribués sur les antennes réceptrices et vice versa une projection de ces groupes antennes sur une carte de france permet une visualisation des corrélations entre la géographie du territoire et le comportement de ses habitants en terme de téléphonie la seconde phase découpe année en périodes entre lesquelles on observe un changement de distributions appels sortant des groupes antennes. on peut ainsi caractériser évolution temporelle du comportement des usagers de mobiles dans chacune des zones du pays\n",
      "étude des techniques oubli dans les moindres carrés récursifs pour apprentissage incrémental de systèmes inférence floue évolutifs  application à la reconnaissance de formes\n",
      "cet article étudie les possibilités utilisation oubli dans apprentissage incrémental en-ligne de classifieurs évolutifs basés sur des systèmes inférence floue pour cela nous étudions différentes possibilités existant dans la littérature dédiée au contrôle pour introduire de oubli dans algorithme des moindres carrés récursifs nous présentons impact de ces différentes techniques dans le contexte de apprentissage incrémental de classifieurs évolutifs en environnement non stationnaire ces approches sont évaluées pour optimisation des systèmes inférence floue sur la problématique de la reconnaissance de gestes manuscrits sur surface tactile\n",
      "évolution une ontologie dédiée à la représentation de relations n-aires\n",
      "nous nous intéressons dans cet article à la problématique évolution une ontologie permettant de représenter des relations n-aires nous présentons la représentation formelle des changements applicables à notre ontologie permettant de modifier sa structure tout en maintenant sa cohérence structurelle nous illustrerons nos propos sur une ontologie dédiée à la représentation de relations n-aires entre des données expérimentales quantitatives\n",
      "extraction de motifs condensés dans un unique graphe orienté acyclique attribué\n",
      "les graphes orientés acycliques attribués peuvent être utilisés dans beaucoup de domaines applicatif dans ce papier nous étudions un nouveau domaine de motif pour permettre leur analyse  les chemins pondérés fréquents nous proposons en conséquence des contraintes primitives permettant évaluer leur pertinence par exemple les contraintes de fréquence et de compacité) et un algorithme extrayant ces solutions nous aboutissons à une représentation condensée dont efficacité et le passage à échelle sont étudiés empiriquement\n",
      "extraction de motifs fréquents dans des arbres attribués\n",
      "extraction de motifs fréquents est une tâche importante en fouille de données initialement centrés sur la découverte ensembles items fréquents les premiers travaux ont été étendus pour extraire des motifs structurels comme des séquences des arbres ou des graphes dans cet article nous proposons une nouvelle méthode de fouille de données qui consiste à extraire de nouveaux types de motifs à partir une collection arbres attribués les arbres attribués sont des arbres dans lesquels les noeuds sont associés à des ensembles d'attributs extraction de ces motifs appelés sous-arbres attribués combine une recherche ensembles items fréquents à une recherche de sous-arbres et nécessite explorer un immense espace de recherche nous présentons plusieurs nouveaux algorithmes extraction arbres attribués et montrons que leurs implémentations peuvent efficacement extraire des motifs fréquents à partir de grands jeux de données\n",
      "extraction des nombres de betti avec un modèle génératif\n",
      "analyse exploratoire de données multidimensionnelles est un problème complexe nous proposons extraire certains invariants topologiques appelés nombre de betti pour synthétiser la topologie de la structure sous-jacente aux données nous définissons un modèle génératif basé sur le complexe simplicial de delaunay dont nous estimons les paramètres par optimisation du critère information bayésien (bic) ce complexe simplicial génératif nous permet extraire les nombres de betti de données jouets et images objets en rotation comparé à la technique géométrique des witness complex le csg apparait plus robuste aux données bruitées\n",
      "extraction et filtrage de syntagmes nominaux pour la recherche information\n",
      "nous proposons dans cet article un système de recherche information sri) qui se base sur des techniques indexation de textes en langue naturelle nous présentons une méthode indexation de documents qui repose sur une approche hybride pour la sélection de descripteurs textuels cette approche emploie des traitements du langage naturel pour extraction des syntagmes nominaux et sur un filtrage statistique basé sur information mutuelle pour sélectionner les syntagmes nominaux les plus informatifs pour le processus indexation. nous effectuons des expérimentations en utilisant le corpus le monde 94 de la collection clef 2001 et sur le sri lemur pour évaluer approche proposée\n",
      "extraction optimisée de règles association positives et négatives rapn)\n",
      "la littérature s'est beaucoup intéressée à extraction de règles association positives et peu à extraction de règles négatives en raison essentiellement du coût de calculs et du nombre prohibitif de règles extraites qui sont pour la plupart redondantes et inintéressantes dans cet article nous nous sommes intéressés aux algorithmes extraction de rapn règles association positives et négatives reposant sur algorithme fondateur apriori nous avons fait une étude de ceux-ci en mettant en évidence leurs avantages et leurs inconvénients a issue de cette étude nous avons proposé un nouvel algorithme qui améliore cette extraction au niveau du nombre et de la qualité des règles extraites et au niveau du parcours de recherche des règles étude s'est terminée par une évaluation de cet algorithme sur plusieurs bases de données\n",
      "grille bivariée pour la détection de changement dans un flux étiqueté\n",
      "nous présentons une méthode en-ligne de détection de changement de concept dans un flux étiqueté notre méthode de détection est basée sur un critère supervisé bivarié qui permet identifier si les données de deux fenêtres proviennent ou non de la même distribution notre méthode a intérêt de n'avoir aucun a priori sur la distribution des données ni sur le type de changement et est capable de détecter des changements de différentes natures changement dans la moyenne dans la variance) les expérimentations montrent que notre méthode est plus performante et robuste que les méthodes de état de art testées de plus à part la taille des fenêtres elle ne requiert aucun paramètre utilisateur\n",
      "identification de compatibilités entre descripteurs de lieux et apprentissage automatique\n",
      "les travaux présentés dans cet article s'inscrivent dans le paradigme des recherches visant à acquérir des relations sémantiques à partir de folksonomies ensemble de tags attribués à des ressources par des utilisateurs) nous expérimentons plusieurs approches issues de état de art ainsi que apport de apprentissage automatique pour identification de relations entre tags nous obtenons dans le meilleur des cas un taux erreur de 23,7 % relations non reconnues ou fausses) ce qui est encourageant au vu de la difficulté de la tâche les annotateurs humains ont un taux de désaccord de 12%)\n",
      "identification de complexes protéine-protéine par combinaison de classifieurs application à escherichia coli\n",
      "nous proposons une approche permettant de prédire des complexes impliquant trois protéines appelés trimères à partir de combinaison de classifieurs appris sur des complexes n'impliquant que deux protéines (dimères) la prédiction de ces trimères repose sur deux hypothèses biologiques  i) deux protéines orthologues présentent des caractéristiques fonctionnelles similaires ii) deux protéines interagissant sous la forme un complexe sous-tendent une fonction biologique essentielle à espèce concernée ces deux hypothèses sont exploitées pour décrire chaque paire de protéines par ensemble des espèces pour lesquelles elles possèdent un orthologue un ensemble de mesures de qualité classiquement utilisées pour évaluer intérêt des règles association est utilisé pour évaluer la force du lien entre les deux protéines organisme modèle escherichia coli a été utilisé pour évaluer notre approche\n",
      "inférence de réseaux biologiques  un défi pour la fouille de données structurées\n",
      "la réponse cellulaire un organisme vivant à un signal donné hormone stress ou médicament met en jeu des mécanismes complexes interaction et de régulation entre les gènes les arn messagers les protéines et autres éléments tels que les micro-arns on parle de réseau interaction pour décrire ensemble des interactions possibles entre protéines et de réseau de régulation génique pour représenter un ensemble de régulations entre gènes identifier ces interactions et ces régulations ouvre la porte à une meilleure compréhension du vivant et permet envisager de mieux soigner par le biais du ciblage thérapeutique puisque les techniques expérimentales de mesure à grande échelle récemment développées fournissent des données observation de ces réseaux ce problème identification de réseau généralement appelé inférence de réseau en biologie des systèmes s'inscrit dans le cadre général de la fouille de données et plus particulièrement de apprentissage artificiel voilà maintenant quelques années que cette problématique a été posée à notre communauté et durant lesquelles les échanges entre biologistes et informaticiens ont non seulement permis aux biologistes étoffer leurs boîtes à outils mais aussi aux informaticiens de concevoir de nouvelles méthodes de fouille de données.en partant des deux problématiques distinctes que sont inférence de réseau interaction et inférence de réseau de régulation je montrerai que ces deux tâches apprentissage posent chacune de manière différente la problématique de la prédiction de sorties structurées inférence de réseau interaction entre protéines vue comme un problème transductif de prédiction de liens peut être résolue comme un problème apprentissage un noyau de sortie à partir un noyau d'entrée inférence de réseau de régulation impliquant la modélisation un système dynamique peut être abordée par approximation parcimonieuse et structurée de fonctions à valeurs vectorielles je présenterai un ensemble de nouveaux outils de régression à sortie dans un espace de hilbert fondés sur des noyaux à valeur opérateur qui fournissent excellents résultats en inférence de réseaux biologiques des expériences in silico sur des données artificielles chez la levure du boulanger ou chez homme illustreront mes propos en fin d'exposé je tracerai quelques perspectives concernant les \" nouveaux \" défis dans le domaine de la bioinformatique et dans celui de la prédiction de sorties structurées\n",
      "les capitalistes sociaux sur twitter  détection via des mesures de similarité\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "les réseaux sociaux tels que twitter font partie du phénomène de déluge des données expression utilisée pour décrire apparition de données de plus en plus volumineuses et complexes pour représenter ces réseaux des graphes orientés sont souvent utilisés dans cet article nous nous focalisons sur deux aspects de analyse du réseau social de twitter en premier lieu notre but est de trouver une méthode efficace et haut niveau pour stocker et manipuler le graphe du réseau social en utilisant des ressources informatiques raisonnables cet axe de recherche constitue un enjeu majeur puisqu'il est ainsi possible de traiter des graphes à échelle réelle sur des machines potentiellement accessibles par tous ensuite nous étudions les capitalistes sociaux un type particulier utilisateurs de twitter observé par ghosh et al (2012) nous proposons une méthode pour détecter et classifier efficacement ces utilisateurs\n",
      "modèle de recherche information sociale centré utilisateur\n",
      "émergence des réseaux sociaux a révolutionné leweb en permettant notamment aux individus de prolonger leur connexion virtuelle en une relation plus réelle et de partager leurs connaissances ce nouveau contexte de diffusion de information sur le web peut constituer un moyen efficace pour cerner les besoins en information des utilisateurs du web et permettre à la recherche information ri) de mieux répondre à ces besoins en adaptant les modèles indexation et d'interrogation exploitation des réseaux sociaux confronte la ri à plusieurs défis dont les plus importants concernent la représentation de information dans ce modèle social de ri et son évaluation en absence de collections de test et de compétitions dédiées dans cet article nous présentons un modèle de ri sociale dans lequel nous proposons de modéliser et exploiter le contexte social de l'utilisateur nous avons évalué notre modèle à aide une collection de test de ri sociale construite à partir des annotations du réseau social de bookmarking collaboratif delicious\n",
      "non-disjoint grouping of text documents based word sequence kernel\n",
      "this paper deals with two issues in text clustering which are the detection of non disjoint groups and the representation of textual data in fact a text document can discuss several themes and then it must belong to several groups the learning algorithm must be able to produce non disjoint clusters and assigns documents to several clusters the second issue concerns the data representation textual data are often represented as a bag of features such as terms phrases or concepts this representation of text avoids correlation between terms and doesn't give importance to the order of words in the text we propose a non supervised learning method able to detect overlapping groups in text document by considering text as a sequence of words and using the word sequence kernel as similarity measure the experiments show that the proposed method outperforms existing overlapping methods using the bag of word representation in terms of clustering accuracy and detect more relevant groups in textual documents\n",
      "nouvelle approche de bi-partitionnement topologique\n",
      "dans ce papier nous proposons une nouvelle approche topologique de bi-partitionnement bi-clustering) appelée bitm en utilisant les cartes autoorganisatrices idée principale de approche est utiliser une seule carte pour le partitionnement simultané des lignes observations) et des colonnes (variables) contrairement aux approches utilisant les cartes topologiques notre modèle ne nécessite pas de pré-traitement de la base de données ainsi une nouvelle fonction de coût est proposée de plus bitm fournit une visualisation topologique des blocs ou bi-clusters facilement interprétable les résultats obtenus sont très encourageants et prometteurs pour continuer dans cette optique\n",
      "paramétrage intelligent de alignement ontologies par intégrale de choquet\n",
      "le nombre croissant ontologies rend le processus alignement une composante essentielle du web sémantique plusieurs outils ont été conçus dans le but de produire des alignements la qualité des alignements fournis par ces outils est étroitement liée à certains paramètres qui régissent leurs traitements dans ce papier nous proposons une nouvelle approche permettant adaptation automatique des paramètres alignement ontologies par utilisation de intégrale de choquet comme un opérateur d'agrégation les expérimentations montrent une nette amélioration des résultats par rapport à un paramétrage statique et figé\n",
      "processus itératif extraction de classes en non supervisée\n",
      "nous proposons dans cet article une nouvelle approche de classification non supervisée où les classes sont obtenues les unes après les autres suivant un processus itératif approche utilise une méthode extraction de classes basée sur la détection de limite de classe chaque classe étant définie par son centre nous avons également défini des critères évaluation adaptés à la méthode proposée plusieurs expérimentations ont montré intérêt de approche dans divers problèmes\n",
      "ré-écriture de requêtes dans un système intégration sémantique\n",
      "nous décrivons la deuxième phase de réalisation un système intégration qui minimise intervention humaine habituellement nécessaire après la phase de construction semi-automatique du schéma ontologie) global décrite dans de précédents articles nous présentons ici le processus de ré-écriture de requêtes globales en des requêtes adressées aux sources\n",
      "recherche de documents similaires sur le web par segmentations hiérarchiques et extraction de mots-clés\n",
      "la recherche de documents similaires est un processus qui consiste à trouver les documents présentant des similitudes comme la copie ou la reformulation sur des bases documentaires ou sur internet elle est utilisée notamment pour protéger la propriété intellectuelle de productions issues de l'enseignement de la recherche ou de l'industrie dans cet article nous définissons une approche automatique pour permettant extraire des mots-clés un document en effectuant un bouclage sur une succession de découpage de plus en plus petit cette approche permet obtenir des mots-clés impossibles à obtenir par une approche globale notamment quand la thématique le style ou le contenu un document varient dans le document objectif est de permettre la détection des documents présentant des similitudes en utilisant uniquement des mots-clés\n",
      "réutiliser les connaissances expert pour assister analyse de activité sur simulateur pleine échelle de conduite de centrale nucléaire - approche à base de m-trace\n",
      "notre travail porte sur aide à observation de activité dans les simulateurs pleine échelle de centrale nucléaire pour assister les formateurs pendant les simulations notre approche consiste à représenter activité sous la forme de trace modélisée et à les transformer afin extraire et de visualiser des informations de haut niveau permettant aux formateurs de mieux retracer et analyser les simulations afin de valider notre approche nous avons conçu le prototype d3kode que nous avons évalué avec des experts formateurs d'edf\n",
      "sélection de variables non supervisée sous contraintes hiérarchiques\n",
      "la sélection des variables a un rôle très important dans la fouille de données lorsqu'un grand nombre de variables est disponible ainsi certaines variables peuvent être peu significatives corrélées ou non pertinentes une méthode de sélection a pour objectif de mesurer la pertinence un ensemble utilisant principalement un critère d'évaluation nous présentons dans cet article un critère non supervisé permettant de mesurer la pertinence un sous-ensemble de variables ce dernier repose sur utilisation du score laplacien auquel nous avons ajouté des contraintes hiérarchiques travailler dans le cadre non supervisé est un vrai challenge dans ce domaine dû à absence des étiquettes de classes les résultats obtenus sur plusieurs bases de tests sont très encourageants et prometteurs\n",
      "snow un algorithme exploratoire pour le subspace clustering\n",
      "cet article propose un nouvel algorithme pour le problème de subspace clustering dénommé snow contrairement aux approches descendantes classiques il ne repose pas sur hypothèse de localité et permet affectation une donnée à plusieurs clusters dans des sous-espaces différents les expérimentations préliminaires montrent que notre approche obtient de meilleurs résultats que algorithme copac sur une base de référence et a été appliquée sur une base de données réelles\n",
      "technique de factorisation multi-biais pour des recommandations dynamiques\n",
      "la factorisation de matrices offre une grande qualité de prédiction pour les systèmes de recommandation mais sa nature statique empêche de tenir compte des nouvelles notes que les utilisateurs produisent en continu ainsi la qualité des prédictions décroît entre deux factorisations lorsque de nombreuses notes ne sont pas prises en compte la quantité de notes écartées est autant plus grande que la période entre deux factorisation est longue ce qui accentue la baisse de qualité.nos travaux visent à améliorer la qualité des recommandations nous proposons une factorisation de matrices utilisant des groupes de produits et intégrant en ligne les nouvelles notes des utilisateurs nous attribuons à chaque utilisateur un biais pour chaque groupe de produits similaires que nous mettons à jour ainsi nous améliorons significativement les prédictions entre deux factorisations nos expérimentations sur des jeux de données réels montrent efficacité de notre approche\n",
      "text2geo  des données textuelles aux informations géospatiales\n",
      "dans cet article nous nous intéressons aux méthodes extraction informations spatiales dans des documents textuels nous présentons la méthode hybride text2geo qui combine une approche extraction informations, fondée sur des patrons avec une approche de classification supervisée permettant explorer le contexte associé nous discutons des résultats expérimentaux obtenus sur le jeu de données de étang de thau\n",
      "totem une méthode de détection de communautés adaptées aux réseaux information\n",
      "alors que les réseaux sociaux s'attachaient à représenter des entités et les relations qui existaient entre elles les réseaux information intègrent également des attributs décrivant ces entités  ce qui conduit à revisiter les méthodes analyse et de fouille de ces réseaux dans cet article nous proposons une méthode de classification des sommets un graphe qui exploite une part leurs relations et autre part les attributs les caractérisant cette méthode reprend le principe de la méthode de louvain en étendant de façon à permettre la manipulation attributs continus une manière symétrique à ce qui existe pour les relations\n",
      "towards a new science of big data analytics based on the geometry and the topology of complex hierarchic systems\n",
      "my work is concerned with pattern recognition knowledge discovery computer learning and statistics i address how geometry and topology can uncover and empower the semantics of data in addition to the semantics of data that can be explored using correspondence analysis and related multivariate data analyses hierarchy is a fundamental concept in this work i address not only low dimensional projection for display purposes but carry out search and pattern recognition whenever useful in very high dimensional spaces high dimensional spaces present very different characteristics from low dimensions i have shown that in a particular sense very high dimensional space becomes as dimensionality increases hierarchical i have also shown how in hierarchy and hence in an ultrametric topological mapping of information space we track change or anomaly or rupture.in this presentation the first theme discussed is that of linear time hierarchical clustering with application to sky survey data in astronomy and to chemo-informatics the second theme discussed is computational text analysis it is interesting to note that jp benzécri's original motivation was in language and linguistics in my text analysis work i have taken the dictum of mckee story  substance structure style and the principles of screenwriting methuen 1999 that \"text is the sensory surface of a work of art\" and show just how this insight can be rendered in computational terms this leads to demarcating tracking statistical modelling visualizing and pattern recognition of narrative in an application to collaborative writing i developed an interactive framework for critiquing and assessing fit and appropriateness of content on the basis of semantics leading to books that were published as e-books having been written by school children in a few days of collaborative class work in many aspects of this work hierarchy expresses both continuity and change in the textual narrative or in the narrative of chronological events\n",
      "un critère évaluation pour la construction de variables à base itemsets pour apprentissage supervisé multi-tables\n",
      "dans le contexte de la fouille de données multi-tables les données sont représentées sous un format relationnel dans lequel les individus de la table cible sont potentiellement liés à plusieurs enregistrements dans des tables secondaires en relation un-à-plusieurs dans cet article nous proposons un framework basé sur des itemsets pour la construction de variables à partir des tables secondaires informativité de ces nouvelles variables est évaluée dans le cadre de la classification supervisée au moyen un critère régularisé qui vise à éviter le surapprentissage pour ce faire nous introduisons un espace de modèles basés sur des itemsets dans la table secondaire ainsi qu'une estimation de la densité conditionnelle des variables construites correspondantes une distribution a priori est définie sur cet espace de modèles pour obtenir ainsi un critère sans paramètres permettant évaluer la pertinence des variables construites des expérimentations préliminaires montrent la pertinence de l'approche\n",
      "un système hybride de recherche information intégrant le raisonnement à partir de cas et la composition ontologies\n",
      "la croissance des informations disponibles sur le web nécessite des outils de recherche de plus en plus performants permettant de répondre efficacement aux besoins des utilisateurs dans ce contexte utilisation des ontologies présente des atouts importants cependant la construction manuelle ontologies est très coûteuse ceci a poussé à proposer des approches permettant automatiser cette construction cet article présente un système de recherche information hybride basée sur le raisonnement à partir de cas ràpc) et la composition ontologies. ce système vise à combiner la construction automatique ontologies modulaires et le ràpc qui a pour but améliorer les résultats de recherche information (ri) des expérimentations ont été menées et les résultats obtenus montrent une amélioration de la précision dans le cas une recherche information sur le web\n",
      "une approche en programmation par contraintes pour la classification non supervisée\n",
      "dans cet article nous abordons le problème de classification non supervisée sous contraintes fondé sur la programmation par contraintes (ppc) nous considérons comme critère optimisation la minimisation du diamètre maximal des clusters nous proposons un modèle pour cette tâche en ppc et nous montrons aussi importance des stratégies de recherche pour améliorer son efficacité notre modèle basé sur la distance entre les objets permet de traiter des données qualitatives et quantitatives des contraintes supplémentaires sur les clusters et les instances peuvent directement être ajoutées des expériences sur des ensembles de données classiques montrent intérêt de notre approche\n",
      "une nouvelle mesure pour évaluation des méthodes extraction de thématiques  la vraisemblance généralisée\n",
      "les méthodes dédiées à extraction automatique de thématiques sont issues de domaines variés  linguistique computationnelle tal algèbre linéaire statistique etc a ces méthodes spécifiques peuvent s'ajouter des méthodes adaptées autres domaines notamment de apprentissage automatique non supervisé les résultats produits par ensemble de ces méthodes prennent des formes hétérogènes  partitions de documents distributions de probabilités sur les mots matrices cela pose clairement un problème pour les comparer de manière uniforme dans cet article nous proposons une nouvelle mesure de qualité intitulée vraisemblance généralisée pour permettre une évaluation et ainsi la comparaison de différentes méthodes extraction de thématiques les résultats obtenus sur un corpus de documents web autour des élections présidentielles françaises de 2012 ainsi que sur le corpus associated press montrent la pertinence de la mesure proposée\n",
      "unsupervised video tag correction system\n",
      "we present a new system for video auto tagging which aims at correcting and completing the tags provided by users for videos uploaded on the internet unlike most existing systems we do not learn any tag classifiers or use the questionable textual information to compare our videos we propose to compare directly the visual content of the videos described by different sets of features such as bag-of-visual-words or frequent patterns built from them then we propagate tags between visually similar videos according to the frequency of these tags in a given video neighborhood we also propose a controlled experimental set up to evaluate such a system experiments show that with suitable features we are able to correct a reasonable amount of tags in web videos\n",
      "validation une carte cognitive\n",
      "les cartes cognitives sont un modèle graphique représentant des influences entre des concepts malgré le fait qu'une carte cognitive soit relativement simple à construire certaines influences peuvent se contredire une l'autre cet article propose différents critères pour valider une carte cognitive c'est-àdire indiquer si la carte contient ou non des contradictions nous distinguons deux types de critères  les critères de vérification qui valident une carte cognitive en déterminant sa cohérence interne et les critères de test qui valident une carte à partir un ensemble de contraintes choisies par le concepteur\n",
      "vers un cadre évolutif de classification non supervisée\n",
      "la classification non supervisée clustering) évolutive surpasse généralement par celle statique en produisant des groupes de données clusters) qui reflètent les tendances à long terme tout en étant robuste aux variations à court terme dans ce travail nous présentons un cadre différent pour le clustering évolutif une manière incrémentale par un suivi précis des variables de proximité temporelles entre les objets suivis par un clustering statique ordinaire\n",
      "vers une architecture multicouche ontologies dédiée à la résolution mixte de problèmes\n",
      "dans cet article nous nous intéressons à la gestion expériences générées au sein des processus de résolution mixte individuelle et/ou collective de problèmes afin assister la capitalisation et le partage des connaissances dans les environnements collaboratifs dans ce contexte nous proposons un cadre ontologique générique par rapport au domaine dédié à la modélisation formelle et consensuelle de ces expériences en adoptant une architecture multicouche basée sur quatre strates la première strate est basée sur la spécialisation ontologies fondationnelles la deuxième strate est basée sur la conception de trois patrons conceptuels ontologiques pco) noyaux le pco organisationnel le pco téléologique et le pco argumentatif modélisant respectivement les acteurs le problème et les solutions proposées) la troisième strate est basée sur la spécialisation des pco noyaux dans un domaine particulier et la dernière strate est basée sur instanciation du modèle ontologique de domaine pour la représentation une situation du monde réel\n",
      "vers une automatisation de la construction de variables pour la classification supervisée\n",
      "dans cet article nous proposons un cadre visant à automatiser la construction de variables pour apprentissage supervisé en particulier dans le cadre multi-tables la connaissance du domaine est spécifiée une part en structurant les données en variables tables et liens entre tables autre part en choisissant des règles de construction de variables espace de construction de variables ainsi défini est potentiellement infini ce qui pose des problèmes exploration combinatoire et de sur-apprentissage nous introduisons une distribution de probabilité a priori sur espace des variables constructibles ainsi qu'un algorithme performant de tirage échantillons dans cette distribution des expérimentations intensives montrent que approche est robuste et performante\n",
      "vers une mesure de similarité pour les séquences complexes\n",
      "le calcul de similarité entre les séquences est une extrême importance dans de nombreuses approches explorations de données il existe une multitude de mesures de similarités de séquences dans la littérature or la plupart de ces mesures sont conçues pour des séquences simples dites séquences d'items dans ce travail nous étudions un point de vue purement combinatoire le problème de similarité entre des séquences complexes (i.e. des séquences ensembles ou itemsets) nous présentons de nouveaux résultats afin de compter efficacement toutes les sous-séquences communes à deux séquences ces résultats théoriques sont la base une mesure de similarité calculée efficacement grâce à une approche de programmation dynamique\n",
      "visualisation radiale  approche parallèle entre cpu et gpu\n",
      "dans cet article nous proposons une parallélisation sur cpu et gpu une méthode de visualisation radiale à base de points d'intérêt nous montrons que cette approche peut visualiser avec des temps très courts des millions de données sur des dizaines de dimensions et nous étudions efficacité de la parallélisation dans différentes configurations\n",
      "antipattern detection inweb ontologies an experiment using sparql queries\n",
      "ontology antipatterns are structures that reflect ontology modelling problems because they lead to inconsistencies bad reasoning performance or bad formalisation of domain knowledge we propose four methods for the detection of antipatterns using sparql queries we conduct some experiments to detect antipattern in a corpus of owl ontologies\n",
      "apprentissage ensemble opérateurs de projection orthogonale pour la détection de nouveauté\n",
      "dans ce papier nous proposons une approche de détection de nouveauté fondée sur les opérateurs de projection orthogonale et idée de double bootstrap (bi-bootstrap) notre approche appelée random subspace novelty dectection filter (rs-ndf) combine une technique de rééchantillonnage et idée apprentissage d'ensemble rs-ndf est un ensemble de filtres ndf novelty detection filter) induits à partir échantillons bootstrap des données apprentissage, en utilisant une sélection aléatoire des variables pour apprentissage des filtres rs-ndf utilise donc un double bootstrap c'est à dire un réechantillonnage avec remise sur les observations et un rééchantillonnage sans remise sur les variables la prédiction est faite par agrégation des prédictions de ensemble des filtres rs-ndf présente généralement une importante amélioration des performances par rapport au modèle de base ndf unique grâce à son algorithme apprentissage en ligne approche rs-ndf est également en mesure de suivre les changements dans les données au fil du temps plusieurs métriques de performance montrent que approche proposée est plus efficace robuste et offre de meilleures performances pour la détection de nouveauté comparée aux autres techniques existantes\n",
      "apprentissage par analyse linéaire discriminante des paramètres de fusion pour la recherche information multimédia texte-image\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "avec le développement du numérique des quantités très importantes de documents composés de texte et images sont échangés ce qui nécessite le développement de modèles permettant exploiter efficacement ces informations multimédias dans le contexte de la recherche d'information un modèle possible consiste à représenter séparément les informations textuelles et visuelles et à combiner linéairement les scores issus de chaque représentation cette approche nécessite le paramétrage de poids afin équilibrer la contribution de chaque modalité le but de cet article est de présenter une nouvelle méthode permettant apprendre ces poids basée sur analyse linéaire discriminante de fisher (ald) des expérimentations réalisées sur la collection imageclef montrent que apprentissage des poids grâce à ald est pertinent et que la combinaison des scores correspondante améliore significativement les résultats par rapport à utilisation une seule modalité\n",
      "biological event extraction using svm and composite kernel function\n",
      "with an overwhelming of experimental and computational results in molecular biology there is an increasing interest to provide tools that will automatically extract structured biological information recorded in freely available text extraction of named entities such as protein gene or disease names and of simple relations of these entities such as statements of protein-protein interactions has gained certain success and now the new focus research has been moving to higher level of information extraction such as co-reference resolution and event extraction it is precisely the last of these tasks which will be focused in this paper the biological event template allows detailed representations of complex natural language statements which is specified by a trigger and arguments labeled by semantic roles in this paper we have developed a biological event extraction approach which uses support vector machines svm) and a suitable composite kernel function to identify triggers and to assign the corresponding arguments also we make use of a number of features based on both syntactic and contextual information which where automatically learned from the training data we implemented our event extraction system using the state-of-the-art of nlp tools we achieved competitive results compared to the bionlp'09 shared task benchmark\n",
      "caractérisation et extraction de biclusters de valeurs similaires avec analyse de concepts triadiques\n",
      "le biclustering de données numériques est devenu depuis le début des années 2000 une tâche importante analyse de données particulièrement pour étude de données biologiques expression de gènes un bicluster représente une association forte entre un ensemble objets et un ensemble attributs dans une table de données numériques les biclusters de valeurs similaires peuvent être vus comme des sous-tables maximales de valeurs proches seules quelques méthodes se sont penchées sur une extraction complète (ie non heuristique) excate et non redondante de tels motifs qui reste toujours un problème difficile tandis qu'aucun cadre théorique fort ne permet leur caractérisation dans le présent article nous introduisons des liens importants avec analyse formelle de concepts plus particulièrement nous montrons de manière originale que analyse de concepts triadiques tca) propose un cadre mathématique intéressant et puissant pour le biclustering de données numériques de cette manière les algorithmes existants de la tca qui s'appliquent habituellement à des données binaires peuvent être utilisés directement ou après quelques modifications après un prétraitement des données pour extraction désirée\n",
      "classification conceptuelle avec généralisation par intervalles\n",
      "nous nous intéressons aux méthodes de classification hiérarchique ou pyramidale où chaque classe formée correspond à un concept ie une paire (extension intension) considérant des données décrites par des variables quantitatives à valeurs réelles ou intervalles ordinales et/ou prenant la forme de distribution de probabilités/fréquences sur un ensemble de catégories les concepts sont obtenus par une correspondance de galois avec généralisation par intervalles ce qui permet de traiter les données de différents types dans un cadre commun une mesure de la généralité un concept est alors calculée sous une forme commune pour les différents types de variables un exemple illustre la méthode proposée\n",
      "classification de données eeg par algorithme évolutionnaire pour étude états de vigilance\n",
      "objectif de ce travail est de prédire état de vigilance un individu à partir de étude de son activité cérébrale signaux électro-encéphalographie eeg) la variable à prédire est binaire état de vigilance \"normal\" ou \"relaxé\").des eeg de 44 participants dans les deux états 88 enregistrements) ont été recueillis via un casque à 58 électrodes après une étape de prétraitement et de validation des données un critère nommé \"critère des pentes\" a été choisi des méthodes de classification supervisée usuelles k plus proches voisins arbres binaires de décision (cart) forêts aléatoires pls et sparse pls discriminante ont été appliquées afin de fournir des prédictions de état des participants le critère utilisé a ensuite été raffiné grâce à un algorithme génétique ce qui a permis de construire un modèle fiable taux de bon classement moyen par cart égal à 86.68 ± 1.87% et de sélectionner une électrode parmi les 58 initiales\n",
      "classification des données catégorielles via la maximisation spectrale de la modularité\n",
      "ce papier présente un algorithme spectrale pour maximiser le critère de la modularité étendu à la classification des données catégorielles il met en evidence la connexion formelle entre la maximisation de la modularité et la classification spectrale il présente en particulier le problème de maximisation de la modularité sous forme un problème algèbrique de maximisation de la trace nous développons ensuite un algorithme efficace pour trouver la partition optimale maximisant le critère de modularité les résultats expérimentaux montrent efficacité de notre approche\n",
      "classification probabiliste non supervisée et visualisation des données séquentielles\n",
      "nous proposons dans ce papier un nouvel algorithme de classification non-supervisée à base de modèle de mélange topologique pour des données non i.i.d non independently and identically distributed) ce nouveau paradigme probabiliste plonge les cartes topologiques probabilistes dans une formulation sous forme de chaînes de markov cachées dans cette formulation la génération une observation à un instant donné du temps est conditionnée par les états voisins au même instant du temps ainsi une grande proximité impliquera une grande probabilité pour la contribution à la génération approche proposée est évaluée en utilisant des données séquentielles réelles issues des bases de données de institut nationale de audiovisuel (ina) les résultats obtenus sont très encourageants et prometteurs\n",
      "classification topologique probabiliste pour des données catégorielles\n",
      "cet article présente une carte auto-organisatrice probabiliste pour analyse et la classification topologique des données catégorielles en considérant un modèle de mélanges parcimonieux nous introduisons une nouvelle carte auto-organisatrice som) probabiliste estimation des paramètres de notre modèle est réalisée à aide de algorithme em classique contrairement à som algorithme apprentissage proposé optimise une fonction objective ces performance sont été évaluées sur des données réelles et les résultats obtenus sont encouragemeants et prometteurs à la fois pour la classification et pour la modélisation\n",
      "clustering de séquences activités pour étude de procédures neurochirurgicales\n",
      "utilisation de modèles de procédure chirurgicale surgical process model spm a récemment émergé dans le domaine de la conception outils intervention chirurgicale assistée par ordinateur ces modèles qui sont utilisés pour analyser et évaluer les interventions représentent des procédures chirurgicales surgical process sp qui sont formalisées comme des structures symboliques décrivant une chirurgie à un niveau de granularité donné un enjeu important réside dans la définition de métriques permettant la comparaison et évaluation de ces procédures ainsi les relations entre ces métriques et des données pré-opératoires permettent de classer les chirurgies pour mettre en lumière des informations sur la procédure elle-même mais également sur le comportement du chirurgien dans ce papier nous étudions la classification automatique un ensemble de procédures chirurgicales en utilisant algorithme dynamic time warping dtw) pour calculer une mesure de similarité entre procédures chirurgicales utilisation de dtw permet de se concentrer sur les différents types activité effectués pendant la procédure ainsi que sur leur séquencement tout en réduisant les différences temporelles des expériences ont été menées sur 24 procédures chirurgicales hernie discale lombaire dans le but de discriminer le niveau expertise des chirurgiens à partir une classification connue a aide un algorithme de clustering hiérarchique utilisant dtw nous avons retrouvé deux groupes de chirurgiens présentant des niveaux expertise différents junior et senior)\n",
      "clustering hiérarchique non paramétrique de données fonctionnelles\n",
      "dans cet article il est question de clustering de courbes nous proposons une méthode non paramétrique qui segmente les courbes en clusters et discrétise en intervalles les variables continues décrivant les points de la courbe le produit cartésien de ces partitions forme une grille de données qui est inférée en utilisant une approche bayésienne de sélection de modèle ne faisant aucune hypothèse concernant les courbes enfin une technique de post-traitement visant à réduire le nombre de clusters dans le but améliorer interprétabilité des clusters est proposée elle consiste à fusionner successivement et de façon optimale les clusters ce qui revient à réaliser une classification hiérarchique ascendante dont la mesure de dissimilarité correspond à la variation du critère de manière intéressante cette mesure est en fait une somme pondérée de divergences de kullback-leibler entre les distributions des clusters avant et après fusion intérêt de approche dans le cadre de analyse exploratoire de données fonctionnelles est illustré par un jeu de données artificiel et réel\n",
      "clustering multi-niveaux de graphes  hiérarchique et topologique\n",
      "nan\n",
      "combinaison de classificateurs simples pour une sélection rapide de caractéristiques\n",
      "la sélection de caractéristiques est une technique permettant de choisir les caractéristiques les plus pertinentes celles adaptées à la résolution un problème particulier les méthodes classiques présentent certains inconvénients par exemple elles peuvent être trop complexes elles peuvent faire dépendre les caractéristiques sélectionnées du classificateur utilisé elles risquent de sélectionner des caractéristiques redondantes dans le but de limiter ces inconvénients nous proposons dans cet article une nouvelle méthode rapide de sélection de caractéristiques basée sur la construction et la sélection de classificateurs simple associés à chacune des caractéristiques une optimisation par un algorithme génétique est proposée afin de trouver la meilleure combinaison des classificateurs différentes méthodes de combinaison sont considérées et adaptées à notre problème cette méthode a été appliquée sur différents ensembles de caractéristiques de tailles variées et construite à partir de la base de chiffres manuscrits mnist les résultats obtenus montrent la robustesse de approche ainsi que efficacité de la méthode en moyenne le nombre de caractéristiques sélectionnées a diminué de 69,9% tout en conservant le taux de reconnaissance\n",
      "combinaison de classification supervisée et non-supervisée par la théorie des fonctions de croyance\n",
      "nous proposons dans cet article une nouvelle approche de classification fondée sur la théorie des fonctions de croyance cette méthode repose sur la fusion entre la classification supervisée et la classification non supervisée en effet nous sommes face à un problème de manque de données apprentissage pour des applications dont les résultats de classification supervisée et non-supervisées ont très variables selon les classificateurs employés les résultats ainsi obtenus sont par conséquent considérés comme invertains notre approche se propose de combiner les résultats des deux types de classification en exploitant leur complémentarité via la théorie des fonctions de croyance celle-ci permet de tenir compte de aspect incertitude et d'imprécision après avoir dresser les différentes étapes de notre nouveau schéma de classification nous détaillons la fusion de classificateurs cette nouvelle approche est appliquée sur des données génériques issues une vingtaine de bases de données les résultats obtenus ont montré efficacité de approche proposée\n",
      "community detection in social networks with attribute and relationship data\n",
      "nan\n",
      "découverte de règles association pour aide à la prévision des accidents maritimes\n",
      "les systèmes de surveillance maritime permettent la récupération et la fusion des informations sur les navires (position vitesse etc. à des fins de suivi du trafic maritime sur un dispositif d'affichage aujourd'hui identification des risques à partir de ces systèmes est difficilement automatisable compte-tenu de expertise à formaliser du nombre important de navires et de la multiplicité des risques (collision échouement etc) de plus le remplacement périodique des opérateurs de surveillance complique la reconnaissance événements anormaux qui sont éparses et parcellaires dans le temps et l'espace dans objectif de faire évoluer ces systèmes de surveillance maritime nous proposons dans cet article une approche originale fondée sur le data mining pour extraction de motifs fréquents cette approche se focalise sur des règles de prévision et de ciblage pour identification automatique des situations induisant ou constituant le cadre des accidents maritimes\n",
      "détection de groupes outliers en classification non supervisée\n",
      "nous proposons dans ce papier une nouvelle méthode de détection de groupes outliers notre mesure nommée gof group outlier factor est estimée par apprentissage non-supervisé nous avons intégré dans apprentissage des cartes topologiques notre approche est basée sur la densité relative de chaque groupe de données et fournit simultanément un partitionnement des données et un indicateur quantitatif gof) sur \"la particularité\" de chaque cluster ou groupe les résultats obtenus sont très encourageants et prometteurs pour continuer dans cette optique\n",
      "détection non supervisée une sous-population par méthode ensemble et changement de représentation itératif\n",
      "apprentissage non supervisé a classiquement pour objectif la détection de sous-populations homogènes classes) considérées de manière équivalente sans information a priori sur celles-ci le problème étudié dans cet article est quelque peu distinct on se focalise ici uniquement sur une sous-population intérêt que on cherche à identifier avec un rappel et une précision optimales nous proposons pour cela une méthode s'appuyant sur les principes suivants  1) travailler dans espace de représentation fourni par des experts faibles pour cette tâche 2) confronter ces experts pour détecter des seuils de sélection plus pertinents et 3) les combiner itérativement afin de converger vers expert idéal cette méthode est éprouvée et comparée sur des données synthétiques\n",
      "development of a distributed recommender system using the hadoop framework\n",
      "producing high quality recommendations has become a challenge in the recent years indeed the growth in the quantity of data involved in the recommendation process pose some scalability and effectiveness problems these issues have encouraged the research of new technologies instead of developing a new recommender system we improve an already existing method a distributed framework was considered based on the known quality and simplicity of the mapreduce project the hadoop open source project played a fundamental role in this research it undoubtedly encouraged and facilitated the construction of our application supplying all tools needed our main goal in this research was to prove that building a distributed recommender system was not only possible but simple and productive\n",
      "evaluating bayesian networks by sampling with simplified assumptions\n",
      "the most common fitness evaluation for bayesian networks in the presence of data is the cooper-herskovitz criterion this technique involves massive amounts of data and therefore expansive computations we propose a cheaper alternative evaluation method using simplified assumptions which produces evaluations that are strongly correlated with the cooper-herskovitz criterion\n",
      "evaluation rapide du diamètre un graphe\n",
      "lors de analyse de graphes il est important de connaître leurs propriétés afin de pouvoir par exemple identifier leur structure et les comparer une des caractérisations importante de ces graphes repose sur le fait de déterminer s'il s'agit ou non un \"petit monde\" pour ce faire la valeur du diamètre du graphe est essentielle or la mesure du diamètre est pour un très grand graphe une opération extrêmement longue nous proposons un algorithme en deux phases qui permet obtenir rapidement une estimation du diamètre un graphe avec une proportion erreur faible en réduisant cet algorithme à une seule phase et en acceptant une marge erreur plus élevée nous obtenons une estimation très rapide du diamètre nous testons cet algorithme sur deux grands graphes de terrain plus un million de noeuds et comparons ses performances avec celles un algorithme de référence bfs breadth-first search) les résultats obtenus sont décrits et commentés\n",
      "exploitation de asymétrie entre termes pour extraction automatique de taxonomies à partir de textes\n",
      "nous présentons dans cet article une nouvelle approche pour la génération automatique de structures lexicales ou taxonomies à partir de textes cette tâche est fondée sur hypothèse forte selon laquelle accumulation defaits statistiques simples sur les usages en corpus permet approximer des informations de niveau sémantique sur le lexique nous utilisons la prétopologie comme cadre de travail afin de formaliser et de combiner plusieurs hypothèses sur les usages terminologiques et enfin de structurer le lexique sous la forme une taxonomie nous considérons également le problème de évaluation des taxonomies résultantes et proposons un nouvel indice afin de les comparer et de positionner notre approche par rapport à la littérature\n",
      "extraction opinions appliquée à des critères\n",
      "les technologies de information et le succès des services associés (e.g. blogs forums ... ont ouvert la voie à un mode expression massive opinions sur les sujets les plus variés récemment de nouvelles techniques de détection automatique opinions opinion mining ont fait leur apparition et via des analyses statistiques des avis exprimés tendent à dégager une tendance globale des opinions exprimées par les internautes néanmoins une analyse plus fine de celle-ci montre que les arguments avancés par les internautes relèvent de critères de jugement distincts ici un film sera décrié pour un scénario décousu là il sera encensé pour une bande son époustouflante dans cet article nous proposons après avoir caractérisé automatiquement des critères dans un document en extraire opinion relative a partir un ensemble restreint de mots clés opinions, notre approche construit automatiquement une base apprentissage de documents issus du web et en déduit un lexique de mots ou expressions opinions spécifiques au domaine d'application des expériences menées sur des jeux de données réelles illustrent efficacité de l'approche\n",
      "extraction de co-variations entre des propriétés de sommets et leur position topologique dans un graphe attribué\n",
      "analyse de grands réseaux est très étudiée en fouille de données toutefois les approches existantes proposent une analyse soit à un niveau macroscopique étude des propriétés globales comme la distribution des degrés) soit à un niveau microscopique extraction de sous-graphes fréquents ou denses) nous proposons une nouvelle méthode qui effectue une analyse intermédiaire permettant de découvrir des motifs regroupant des propriétés microscopiques et macroscopiques du réseau ces motifs capturent des co-variations entre des propriétés numériques relatives aux sommets par exemple un motif mésoscopique dans un réseau de co-auteurs peut être plus le nombre de publications à egc est important plus la centralité des sommets correspondants dans le réseau est également notre contribution est multiple d'abord ce travail est le premier à exploiter conjointement des propriétés locales et des propriétés topologiques de plus nous produisons de nouvelles avancées dans le domaine de extraction de co-variations en revisitant les motifs émergents dans ce contexte enfin nous rapportons une analyse un réseau bibliographique réel issu de dblp\n",
      "extraction de dépendances fonctionnelles approximatives\n",
      "la découverte de dépendances fonctionnelles df) à partir une relation existante est une technique importante pour analyse de bases de données ensemble des df exactes ou approximatives extraites par les algorithmes existants est valide tant que la relation n'est pas modifiée ceci est insuffisant pour des situations réelles où les relations sont constamment mises à jour nous proposons une approche incrémentale qui maintiens à jour ensemble des df valides exactes ou approximatives selon une erreur donnée quand des tuples sont insérés et supprimés les résultats expérimentaux indiquent que lors de extraction de df à partir une relation continuellement modifiée les algorithmes existants sont sensiblement dépassés par notre stratégie incrémentale\n",
      "extraction de liens fréquents dans les réseaux sociaux\n",
      "cet article présente flmin une nouvelle méthode extraction de motifs fréquents dans les réseaux sociaux contrairement aux méthodes traditionnelles qui s'intéressent uniquement aux régularités structurelles originalité de notre approche réside dans sa capacité à exploiter la structure et les attributs des noeuds pour extraire des régularités que nous appelons “liens fréquents” dans les liens entre des noeuds partageant des caractéristiques communes\n",
      "extraction de séquences fréquentes avec intervalles incertitude\n",
      "lors de extraction des séquences la granularité temporelle est plus ou moins importante selon les besoins des utilisateurs et les contraintes du domaine d'application nous proposons un algorithme extraction de séquences fréquentes par intervalles à partir de séquences à estampilles temporelles discrètes nous intégrons une relaxation des contraintes temporelles en introduisant la définition de \"séquences temporelles par intervalles\" (sti) ces intervalles reflètent une incertitude sur les occurrences précises des évènements nous formalisons ce nouveau concept en exhibant certaines de ses propriétés et nous menons quelques expériences afin de comparer qualitativement) nos résultats avec une autre proposition assez proche de la nôtre\n",
      "extraction de sous-parties ciblées une ontologie généraliste pour enrichir une ontologie particulière\n",
      "différentes ressources ontologiques généralistes de très grande taille ont été développées de façon collective et sont aujourd'hui disponibles sur le web ainsi ontologie yago est une énorme base de connaissances décrivant plus de 2 millions d'entités afin de tirer parti de ce gigantesque travail collectif nous montrons comment en extraire des sous-parties thématiquement focalisées pour enrichir une autre ontologie dite cible de taille plus limitée mais de domaine centré sur une application particulière 1\n",
      "extraction et gestion informations pour la construction une base vidéo apprentissage\n",
      "indexer une vidéo consiste à rattacher un ou plusieurs concepts à des segments de cette vidéo un concept étant défini comme une représentation intellectuelle une idée abstraite indexation automatique se base sur extraction automatique de caractéristiques fournies par un système de traitement d'images cependant il est nécessaire de définir les index ou concepts pour cela il faut définir le lien qui existe entre ces caractéristiques et ces concepts ce qui sépare les caractéristiques extraites sur lesquelles se base indexation automatique et les concepts est appelé fossé sémantique qui est le manque de concordance entre les informations que les machines peuvent extraire depuis les documents numériques et les interprétations que les humaines en font la définition un concept peut être faite automatiquement si on dispose une base apprentissage liée au concept dans ce cas il est possible \"d'apprendre\" le concept de manière statistique mais la construction de cette base apprentissage nécessite de faire intervenir un utilisateur ou un expert applicatif en fait il s'agit de s'appuyer sur ses connaissances pour extraire des segments vidéo représentatifs du concept que on souhaite définir on peut lui demander indexer manuellement la base apprentissage, mais cette opération est longue et fastidieuse dans cet article nous proposons une méthode qui permet extraire expertise pour que implication de expert soit la plus simple et la plus limitée possible\n",
      "extraction incrémentale de séquences fréquentes dans un flux itemsets\n",
      "nan\n",
      "human detection by a small autonomous mobile robot\n",
      "nous proposons une méthode utilisant les histogrammes de gradient orienté hog) et les séparateurs à vaste marge svm) pour la détection de personnes à partir images prises depuis un petit robot mobile autonome les travaux antérieurs réalisés dans le domaine de la détection êtres humains à partir images ne peuvent pas être employés pour ce type application car ils supposent que les images sont prises à partir une position élevée au moins la hauteur un petit enfant alors que la taille de notre robot n'est que de 15cm nous employons à la fois les hog et les svm car cette combinaison de métodes est reconnue comme étant celle ayant le plus de succès pour la détection de personnes pour traiter une grande variété de formes humaines principalement en raison de la distance existant entre les personnes et le robot nous avons développé une nouvelle méthode de prédiction à deux étapes utilisant deux types de classificateurs svm qui reposent sur une estimation de la distance estimation est basée sur une proportion de pixels de couleur de peau dans l'image ce qui nous permet de clairement séparer notre problème de la détection de corps entier et de celle de corps partiel les essais réalisés dans un bureau ont montré des résultats prometteurs de notre méthode avec une valeur de f de 0,93\n",
      "identification et caractérisation de différents types de boycott par des méthodes analyse de données\n",
      "nan\n",
      "k-moyennes contraintes par un classifieur application à la personnalisation de scores de campagnes\n",
      "lorsqu'on désire contacter un client pour lui proposer un produit on calcule au préalable la probabilité qu'il achètera ce produit cette probabilité est calculée à aide un modèle prédictif pour un ensemble de clients le service marketing contacte ensuite ceux ayant la plus forte probabilité acheter le produit en parallèle et avant le contact commercial il peut être intéressant de réaliser une typologie des clients qui seront contactés idée étant de proposer des campagnes différenciées par groupe de clients cet article montre comment il est possible de contraindre la typologie réalisée à aide des k-moyennes à respecter la proximité des clients vis-à-vis de leur score d'appétence\n",
      "extraction de règles de dépendance bien définies entre ensembles de variables multivaluées\n",
      "cet article étudie la faisabilité et intérêt de extraction de règles de dépendance entre ensembles de variables multivaluées en comparaison du problème bien connu de extraction des règles association fréquentes une règle de dépendance correspond à une dépendance fonctionnelle approximative caractérisée principalement par entropie conditionnelle associée article montre comment établir une analogie formelle entre les deux familles de règles et comment adapter à aide de cette analogie algorithme « eclat » afin extraire un jeu de données les règles de dépendance dites bien définies une étude expérimentale conclut sur les forces et inconvénients des règles de dépendance bien définies vis-à-vis des règles association fréquentes\n",
      "méta-règles pour la génération de règles négatives\n",
      "la littérature s'est beaucoup intéressée à extraction de règles classiques ou positives et peu à extraction des règles négatives en raison essentiellement une part du coût de calculs et autre part du nombre prohibitif de règles redondantes et inintéressantes extraites la démarche que nous avons retenue est de dégager les règles négatives lors de extraction des règles positives et pour cela nous recherchons les règles négatives que on peut inférer ou pas à partir de la pertinence une règle positive ces différentes inférences vont être formalisées par un ensemble de méta-règles\n",
      "mining genetic interactions in genome-wide association study\n",
      "advanced biotechnologies have rendered feasible high-throughput data collecting in human and other model organisms the availability of such data holds promise for dissecting complex biological processes making sense of the flood of biological data poses great statistical and computational challenges i will discuss the problem of mining gene-gene interactions in high-throughput genetic data finding genetic interactions is an important biological problem since many common diseases are caused by joint effects of genes previously it was considered intractable to find genetic interactions in the whole-genome scale due to the enormous search space the problem was commonly addressed using heuristics which do not guarantee the optimality of the solution i will show that by utilizing the upper bound of the test statistic and effectively indexing the data we can dramatically prune the search space and reduce computational burden moreover our algorithms guarantee to find the optimal solution in addition to handling specific statistical tests our algorithms can be applied to a wide range of study types by utilizing convexity a common property of many commonly used statistics\n",
      "modèle de supervision interactions non-intrusif basé sur les ontologies\n",
      "automatisation et la supervision des systèmes pervasifs est à heure actuelle principalement basée sur utilisation massive de capteurs distribués dans l'environnement dans cet article nous proposons un modèle de supervision interactions basé sur analyse sémantique des logs domotiques commandes émises par l'utilisateur) visant à limiter utilisation de ces capteurs  le principe est utiliser des outils inférences avancés afin de déduire les informations habituellement captées pour cela une ontologie automatiquement dérivée un processus dirigé par les modèles définit les interactions utilisateur système utilisation un système de règles permet ensuite inférer des informations sur la localisation et intention de l'utilisateur dans le but de réaliser du monitoring et de proposer des services domotiques adaptés\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pls path modeling and regularized generalized canonical correlation analysis for multi-block data analysis\n",
      "regularized generalized canonical correlation analysis rgcca) is a generalization of regularized canonical correlation analysis to three or more sets of variables it constitutes a general framework for many multi-block data analysis methods it combines the power of multi-block data analysis methods maximization of well identified criteria and the flexibility of pls path modeling the researcher decides which blocks are connected and which are not) searching for a fixed point of the stationary equations related to rgcca a new monotone convergent algorithm very similar to the pls algorithm proposed by herman wold is obtained finally a practical example is discussed\n",
      "prétraitement supervisé des variables numériques pour la fouille de données multi-tables\n",
      "le pré-traitement des variables numériques dans le contexte de la fouille de données multi-tables diffère de celui des données classiques individu variable la difficulté vient principalement des relations un-à-plusieurs où les invidus de la table cible sont potentiellement associés à plusieurs enregistrements dans des tables secondaires dans cet article nous décrivons une méthode de discrétisation des variables numériques situées dans des tables secondaires nous proposons un critère qui évalue les discrétisations candidates pour ce type de variables nous décrivons un algorithme optimisation simple qui permet obtenir la meilleure discrétisation en intervalles de fréquence égale pour le critère proposé idée est de projeter dans la table cible information contenue dans chaque variable secondaire à aide un vecteur attributs un attribut par intervalle de discrétisation) chaque attribut représente le nombre de valeurs de la variable secondaire appartenant à intervalle correspondant ces attributs effectifs sont conjointement partitionnés à aide de modèles en grille de données afin obtenir une meilleure séparation des valeurs de la classe des expérimentations sur des jeux de données réelles et artificielles révèlent que approche de discrétisation permet de découvrir des variables secondaires pertinentes\n",
      "raisonner sur une ontologie cartographique pour concevoir des légendes de cartes\n",
      "concevoir une carte géographique plus particulièrement sa légende exige des compétences spécifiques objectif de ce papier est de présenter une base de connaissances destinée à aider tout utilisateur à concevoir une ou plusieurs légendes adaptées à son besoin et conformes aux règles de cartographie la base de connaissances est formée une ontologie de la cartographie nommée ontocarto un corpus de règles  ontocartorules et un moteur de raisonnement corese dans ce papier chaque demande de conception de légende est vue comme une instanciation particulière de l'ontologie associée à une sélection de règles pertinentes dans le corpus de règles sur laquelle corese va raisonner pour construire des légendes adaptées à la configuration spécifique traitée la conception de la légende s'appuie sur la définition de deux hiérarchies objets géographiques et cartographiques les principes de fonctionnement de corese sont présentés un prototype a été implémenté et des extraits des résultats sont montrés\n",
      "recherche information agrégée dans des documents xml basée sur les réseaux bayésiens\n",
      "dans cet article nous nous intéressons à la recherche agrégée dans des documents xml pour cela nous proposons un modèle basé sur les réseaux bayésiens les relations de dépendances entre requête-termes indexation et termes indexation-éléments sont quantifiées par des mesures de probabilité dans ce modèle la requête de utilisateur déclenche un processus de propagation pour trouver des éléments ainsi au lieu de récupérer une liste des éléments qui sont susceptibles de répondre à la requête notre objectif est agréger dans un agrégat des éléments pertinents non-redondants et complémentaires nous avons évalué notre approche dans le cadre de la compagne évaluation inex2009 et avons présenté quelques résultats expérimentaux mettant en évidence impact de agrégation de tels éléments\n",
      "relational learning from spatial data retrospect and prospect\n",
      "learning from spatial data is characterized by two main features first spatial objects have a locational property which implicitly defines several spatial relationships (topological directional distance based between objects second attributes of spatially related units tend to be statistically correlated these two features argue against the assumption of the independent generation of data samples (iid assumption underlying classic machine learning algorithms and motivate the application of relational learning algorithms whose inferences are based on both instance properties and relations between data this relational learning approach to spatial domains has already been investigated in the last decade and important accomplishments in this direction have already been performed in this talk we retrospectively survey major achievements on relational learning from spatial data and we report open problems which still challenges researchers and prospectively suggest important topics for incorporation into a research agenda\n",
      "réorganisation hiérarchique de visualisations dans olap\n",
      "dans cet article nous proposons un nouvel algorithme pour la régorganisation hiérarchique des cubes olap on-line analytical processing ayant pour objectif améliorer leur visualisation cet algorithme se caractérise par le fait qu'il peut traiter des dimensions organisées hiérarchiquement et optimiser conjointement les dimensions du cube contrairement aux autres approches il utilise un algorithme génétique qui réorganise des arbres n-aires quelconques il a été intégré dans une interface olap puis testé en comparaison avec autres approches de réorganisation et fournit des résultats très positifs a ce titre nous avons également généralisé algorithme heuristique classique bea \"bond energy algorithm\" au cas de hiérarchies olap enfin notre approche a été évaluée par des utilisateurs et les résultats soulignent intérêt de la réorganisation dans des exemples de tâches à résoudre pour olap\n",
      "représentations de services web  impact sur la découverte et la recommandation\n",
      "nan\n",
      "ricsh  recherche information contextuelle par segmentation thématique de documents\n",
      "le but principal des systèmes de recherche informations sri) classiques est de retrouver dans un corpus de documents information considérée comme pertinente pour une requête utilisateur cette pertinence est souvent liée à la fréquence apparition des termes dans le texte par rapport au corpus sans tenir compte du contexte de la recherche partant de ce constat nous proposons dans cet article une approche pour la recherche information contextuelle par segmentation thématique de documents (ricsh) cette approche s'appuie sur la méthode de pondération tf-idf que nous avons adaptée dans notre cas pour indexer le corpus cette adaptation se situe au niveau de importance du terme et de son pouvoir de discrimination par rapport aux fragments de textes et non au corpus ces fragments sont obtenus grâce à un processus identification des unités thématiques les plus pertinentes pour chaque document\n",
      "sélection bayésienne de modèles avec prior dépendant des données\n",
      "cet article analyse la consistance asymptotique des modèles en grille appliqués à estimation de densité jointe de deux variables catégorielles les modèles en grille considèrent un partitionnement des valeurs de chacune des variables le produit cartésien des partitions formant une grille dont les cellules permettent de résumer la table de contingence des deux variables le meilleur modèle de co-partitionnement est recherché au moyen une approche map maximum a posteriori) présentant la particularité peu orthodoxe exploiter une famille de modèles et une distribution a priori de ces modèles qui dépendent des données ces modèles sont par nature des modèles de échantillon d'apprentissage et non de la distribution sous-jacente nous démontrons la consistance de l'approche qui se comporte comme un estimateur universel de densité jointe convergeant asymptotiquement vers la vraie distribution jointe\n",
      "solving problems with visual analytics challenges and applications\n",
      "never before in history data is generated and collected at such high volumes as it is today as the volumes of data available to business people scientists and the public increase their effective use becomes more challenging keeping up to date with the flood of data using standard tools for data analysis and exploration is fraught with difficulty the field of visual analytics seeks to provide people with better and more effective ways to understand and analyze large datasets while also enabling them to act upon their findings immediately visual analytics integrates the analytic capabilities of the computer and the abilities of the human analyst allowing novel discoveries and empowering individuals to take control of the analytical process visual analytics enables unexpected and hidden insights which may lead to beneficial and profitable innovation the talk presents the challenges of visual analytics and exemplifies them with application examples illustrating the exiting potential of current visual analysis techniques\n",
      "structuration des décisions de jurisprudence basée sur une ontologie juridique en langue arabe\n",
      "informatique juridique est un domaine en évolution constante le contexte général de notre travail est élaboration un système de recherche de jurisprudence tunisienne en langue arabe objectif opérationnel de ce système est de fournir une aide aux juristes pour résoudre une situation juridique donnée en mettant à leur disposition une collection de situations similaires ce qui améliorera leur raisonnement futur une ontologie du domaine juridique construite à partir des documents des décisions juridiques est nécessaire dans notre contexte cette ontologie a pour but  i) la structuration des décisions ii) la formulation des requêtes interrogation de la base des décisions et iii) la recherche des décisions dans cet article nous présentons architecture de notre système de recherche de jurisprudence nous nous focalisons sur ontologie du domaine de jurisprudence que nous avons élaborée ainsi que sur le module destructuration des décisions\n",
      "sweetdeki  le wiki sémantique couteau suisse du réseau social isicil\n",
      "le projet anr isicil 1 mixe les nouvelles applications virales du web avec des représentations formelles et des processus entreprise pour les intégrer dans les pratiques de veille en entreprise les outils développés s'appuient sur les interfaces avancées des applications du web 2.0 (blog wiki social bookmarking extensions de navigateurs pour les interactions et sur les technologies du web sémantique pour interopérabilité et le traitement de l'information le présent article décrit plus précisément le wiki sémantique développé dans le cadre de ce projet et son intégration au coeur du framework isicil\n",
      "tmd-miner  une nouvelle approche pour la détection des diffuseurs dans un système communautaire\n",
      "plusieurs méthodes ont été développées ces dernières années pour détecter dans un réseau social les membres qualifiés selon les auteurs d'influenceurs de médiateurs ambassadeurs ou encore d'experts dans cet article nous proposons un nouveau cadre méthodologique permettant identifier des diffuseurs dans le contexte où seule information sur appartenance des membres du réseau à des communautés est disponible ce cadre basé sur une représentation du réseau sous forme d'hypergraphe nous a permis de formaliser la notion de diffuseur et introduire algorithme tmd-miner dédié à la détection des diffuseurs et basé sur les itemsets essentiels\n",
      "topological decomposition and heuristics for high speed clustering of complex networks\n",
      "with the exponential growth in the size of data and networks development of new and fast techniques to analyze and explore these networks is becoming a necessity moreover the emergence of scale free and small world properties in real world networks has stimulated lots of activity in the field of network analysis and data mining clustering remains a fundamental technique to explore and organize these networks a challenging problem is to find a clustering algorithm that works well in terms of clustering quality and is efficient in terms of time complexity in this paper we propose a fast clustering algorithm which combines some heuristics with a topological decomposition to obtain a clustering the algorithm which we call topological decomposition and heuristics for clustering tdhc) is highly efficient in terms of asymptotic time complexity as compared to other existing algorithms in the literature we also introduce a number of heuristics to complement the clustering algorithm which increases the speed of the clustering process maintaining the high quality of clustering we show the effectiveness of the proposed clustering method on different real world data sets and compare its results with well known clustering algorithms\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformation de espace de description pour apprentissage par transfert\n",
      "dans ce papier nous proposons une étude sur utilisation de apprentissage topologique pondéré et les méthodes de factorisation matricielle pour transformer espace de représentation un jeu de données \"sparse\" afin augmenter la qualité de apprentissage, et de adapter au cas de apprentissage par transfert la factorisation matricielle nous permet de trouver des variables latentes et apprentissage topologique pondéré est utilisé pour détecter les plus pertinentes parmi celles-ci la représentation de nouvelles données est basée sur leurs projections sur le modèle topologique pondéré pour apprentissage par transfert nous proposons une nouvelle méthode où la représentation des données est faite de la même manière que dans la première phase mais en utilisant un modèle topologique élagué les expérimentations sont présentées dans le cadre un challenge international où nous avons obtenu des résultats prometteurs 5ieme rang de la conpétition internationale) 1 introduction\n",
      "un algorithme de classification automatique pour des données relationnelles multi-vues\n",
      "classification automatique de carvalho et al. 2012 capable de partitinner des objets en prenant en compte de manière simultanée plusieurs matrices de dissimilarité qui les décrivent ces matrices peuvent avoir été générées en utilisant différents ensembles de variables et de fonctions de dissimilarité cette méthode basée sur algorithme de nuées dynamiques est conçu pour fournir une partition et un prototype pour chaque classe tout en découvrant une pondération pertinante pour chaque matrice de dissimilarité en optimisant un critère adéquation entre les classes et leurs représentants ces pondérations changent à chaque itération de algorithme et sont différentes pour chacune des classes nous présentons aussi plusieurs outils aide à interprétation des groupes et de la partition fournie par cette nouvelle méthode deux exemples illustrent intérêt de la méthode le premier utilise des données concernant des chiffres manuscrits 0 à 9 numérisés en images binaires provenant de l'uci le second utilise un ensemble de rapports dont nous connaissons une classification experte donnée à priori\n",
      "un assistant utilisateur pour le choix et le paramétrage des méthodes de fouille visuelle de données\n",
      "nous nous intéressons dans cet article au problème de automatisation du processus de choix et de paramétrage des visualisations en fouille visuelle de données pour résoudre ce problème nous avons développé un assistant utilisateur qui effectue deux étapes  à partir des objectifs annoncés par utilisateur et des caractéristiques de ses données le système commence par proposer à utilisateur différents appariements entre la base de données à visualiser et les visualisations qu'il gère ces appariements sont générés par une heuristique utilisant une base de connaissances sur les visualisations et la perception visuelle ensuite afin affiner les différents paramétrages suggérés par le système nous utilisons un algorithme génétique interactif qui permet aux utilisateurs évaluer et ajuster visuellement ces paramétrages nous présentons une évaluation utilisateur qui montre intérêt de notre système pour deux tâches\n",
      "un environnement efficace pour la classification images à grande échelle\n",
      "la plupart des processus de classification images comportent trois principales étapes  extraction de descripteurs de bas niveaux la création un vocabulaire visuel par quantification et apprentissage à aide un algorithme de classification (egsvm) de nombreux problèmes se posent pour le passage à échelle comme avec ensemble de données imagenet contenant 14 millions images et 21,841 classes la complexité concerne le temps exécution de chaque tâche et les besoins en mémoire et disque (eg le stockage des sifts nécessite 11to) nous présentons une version parallèle de libsvm pour traiter de grands ensembles de données dans un temps raisonnable de plus il y a beaucoup de perte information lors de la phase de quantification et les mots visuels obtenus ne sont pas assez discriminants pour de grands ensembles images. nous proposons utiliser plusieurs descripteurs simultanément pour améliorer la précision de la classification sur de grands ensembles images. nous présentons nos premiers résultats sur les 10 plus grandes classes 24,817 images d'imagenet\n",
      "une approche multidimensionnelle basée sur les comportements individuels pour la prédiction de la diffusion de information sur twitter\n",
      "aujourd'hui les réseaux sociaux en ligne sont devenus des outils très puissants de propagation de information ils favorisent la diffusion rapide à grande échelle de contenu et les conséquences une information inexacte voire fausse peuvent alors prendre une ampleur considérable par conséquent il devient indispensable de proposer des moyens analyser le phénomène de diffusion de information dans ces réseaux de nombreuses études récentes ont traité de la modélisation du processus de diffusion de information, essentiellement un point de vue topologique et dans une perspective théorique mais les facteurs impliqués sont encore méconnus nous proposons ici une solution pratique dont objectif est de prédire la dynamique temporelle de la diffusion au sein de twitter basée sur des techniques apprentissage automatique notre approche repose sur inférence de probabilités de diffusion tirées une analyse multi-dimensionnelle des comportements individuels les expérimentations menées montrent intérêt de la modélisation proposée\n",
      "une distance hiérarchique basée sur la sémantique pour la comparaison histogrammes nominaux\n",
      "la plupart des distances entre histogrammes sont définies pour comparer des histogrammes ordonnés dont les entités représentées sont totalement ordonnées ou des histogrammes nominaux dont les entités représentées ne peuvent pas être comparées) cependant il n'existe aucune distance qui permette de comparer des histogrammes nominaux dans lesquels il est possible de quantifier des valeurs de proximité sémantique entre les entités considérées cet article propose une nouvelle distance permettant de pallier ce problème dans un premier temps une hiérarchie histogrammes obtenue par le biais une fusion progressive des entités considérées prenant en compte leurs proximités sémantiques) est construite pour chaque étage de cette hiérarchie une distance standard de comparaison histogrammes nominaux est calculée finalement pour obtenir la distance proposée ces différentes distances sont fusionnées en prenant en compte la cohérence sémantique associée aux niveaux de chaque étage de la hiérarchie cette distance a été validée dans le cadre de la classification de données géographiques les résultats obtenus sont encourageants et montrent ainsi intérêt et utilité de cette dernière pour des processus de fouille de données\n",
      "user evaluation why\n",
      "research in information visualisation has changed significantly in the past two decades once it was sufficient to simply design and implement an impressive visualisation system today editors and reviewers expect papers to present not only a novel system but empirical evidence of its worth why has this change come about and what impact has it had on those working in this area this talk will discuss how a field dominated by algorithms and tools became infected by human participants and why this is a positive development in a maturing research discipline\n",
      "utilisation invariants pour une médiation inter-domaines de modèles utilisateurs  ressources invariantes et invariants sémantiques\n",
      "les services de personnalisation du web 2.0 reposent sur exploitation de modèles utilisateurs schématiquement plus la quantité informations sur les utilisateurs est grande meilleures sont la modélisation et la qualité du service en pratique nombre de services rencontrent un problème de manque informations sur les utilisateurs dans cet article nous y répondons par médiation inter-domaines de modèles utilisateurs c'est-à-dire la complétion de modèles en explorant des données un autre domaine la médiation que nous proposons repose sur un transfert informations inter-domaines ce transfert consiste en utilisation de couples invariants ou très corrélés pouvant être des couples de ressources ou de descripteurs sémantiques identifiés après enrichissement sémantique des modèles nous montrons que le transfert sous forme de couple de ressources permet une complétion de qualité et que exploitation de descripteurs sémantiques augmente la couverture à qualité égale enrichir sémantiquement est donc bénéfique pour le transfert inter-domaines\n",
      "validation et optimisation une décomposition hiérarchique de graphes\n",
      "de nombreux algorithmes de fragmentation de graphes fonctionnent par agrégations ou divisions successives de sous-graphes menant à une décomposition hiérarchique du réseau étudié une question importante dans ce domaine est de savoir si cette hiérarchie reflète la structure du réseau ou si elle n'est qu'un artifice lié au déroulement de la procédure nous proposons un moyen de valider et au besoin optimiser la décomposition multi-échelle produite parce type de méthode on applique notre approche sur algorithme proposé par blondel et al 2008) basé sur la maximisation de la modularité dans ce cadre une généralisation de cette mesure de qualité au cas multi-niveaux est introduite nous testons notre méthode sur des graphes aléatoires ainsi que sur des exemples réels issus de divers domaines\n",
      "vers la construction un observatoire des pratiques agricoles  gestion et propagation de imprécision des données agronomiques\n",
      "un des objectifs observox est de traiter et gérer imprécision des données agronomiques tant spatialement parcelles agricoles et quantitativmeent quantités de produits disséminées et de toujours associer une évaluation de la qualité aux données aussi nous avons choisi le cadre théorique des ensembles flous a partir un modèle conceptuel gérant l'imperfection nous contruisons une base de données gérant des entités spatiotemporelles imprécises appelées « entités agronomiques floues » cependant ce choix de représentation rend possible le chevauchement des composantes spatiales entre entités dans ce cas nous propageons imprécision du spatial vers le quantitatif à aide un opérateur de caractère additif qui prend en compte à la fois information spatiale et quantitative et qui fournit une information quantitative locale et floue le système ainsi construit nous permet obtenir une représentation floue des quantités de produits phytosanitaires disséminés à chaque endroit du territoire étudié\n",
      "vers une approche efficace extraction de motifs spatio-séquentiels\n",
      "ces dernières années augmentation de la quantité informations spatio-temporelles stockées dans les bases de données a fait naître de nouveaux besoins notamment en matière de gestion des risques naturels sanitaires ou anthropiques (p ex compréhension de la dynamique une épidémie de dengue) dans cet article nous définissons un cadre théorique pour extraction de motifs spatio-séquentiels séquences de motifs spatiaux représentant évolution dans le temps une localisation et de son voisinage nous proposons un algorithme extraction efficace qui effectue un parcours en profondeur en s'appuyant sur des projections successives de la base de données nous introduisons également une mesure intérêt adaptée aux aspects spatio-temporels de ces motifs les expérimentations réalisées sur des jeux de données réels soulignent la pertinence de approche proposée par rapport aux méthodes de la littérature\n",
      "vers une méthode automatique de construction de hiérarchies contextuelles\n",
      "dans de nombreux domaines (e.g. fouille de données entrepôts de données) existence de hiérarchies sur certains attributs peut être extrêmement utile dans le processus analytique toutefois cette connaissance n'est pas toujours disponible ou adaptée il est alors nécessaire de disposer un processus de découverte automatique pour palier ce problème dans cet article nous combinons et adaptons des techniques issues de la théorie de information et du clustering pour proposer une technique orientée données de construction automatique de taxonomies les deux principaux avantages une telle approche sont son caractère totalement non-supervisé et absence de paramètre utilisateur à spécifier afin de valider notre approche nous avons appliquée sur des données réelles et avons conduit plusieurs types d'expérimentation d'abord les hiérarchies obtenues ont été expertisées pour en examiner le pouvoir informatif ensuite nous avons évalué apport de ces taxonomies comme support à des tâches de fouille de données nécessitant une définition hiérarchique des valeurs attributs  extraction de séquences fréquentes multidimensionnelles et multi-niveaux ainsi que la construction de résumés de tables relationnelles les résultats obtenus permettent de conclure quant à intérêt de notre approche\n",
      "webmarks  le marquage intérêt sur le web de données\n",
      "depuis son apparition au sein du w3c la définition de la ressource web n'a cessé évoluer au delà du simple document lieu service concept d'ontologie représentation un objet réel ou non la ressource web est complexe et il nous a semblé que les outils à disposition des internautes pour sa manipulation comme les bookmarks par exemple n'exploitaient pas pleinement ces nouvelles dimensions dans cet article nous présenterons le modèle webmarks qui permet de préciser objet du marquage la ressource mais également intérêt de auteur de la marque implémentation de ce modèle au sein du projet isicil sera également présentée et nous discuterons de son apport en comparaison des technologies existantes\n",
      "@krex  une méthode de construction des connaissances pour la maîtrise des activités à risques - application au domaine de la sécurité nucléaire\n",
      "dans les industries à risque comme le nucléaire les connaissances liées au savoir et à expérience participent à la maîtrise des activités elles sont explicites formalisables dans des documents ou tacites expression du savoir faire moins souvent prise en compte areva développe la méthode @krex pour valoriser le retour expérience existant créer une dynamique extraction et de capitalisation des connaissances faciliter leur partage et leur enrichissement cette communication décrit le protocole expérimental de construction des connaissances explicites et tacites du métier sécurité nucléaire\n",
      "a la recherche des tweets porteurs informations journalistiques\n",
      "nan\n",
      "acquisition de structures lexico-sémantiques à partir de textes  un nouveau cadre de travail fondé sur une structuration prétopologique\n",
      "les structures lexico-sémantiques jouent un rôle essentiel dans les processus de fouille de textes en codant les relations sémantiques entre concepts du discours elles apportent une connaissance stratégiques pour enrichir les capacités de raisonnement le développement de telles structures étant fortement limité du fait des efforts nécessaires à leur construction nous proposons un nouveau formalisme acquisition automatique ontologies terminologiques à partir de textes nous utilisons pour cela une formalisation prétopologique de espace des termes sur laquelle s'appuie un modèle générique de structuration nous présentons une étude empirique préliminaire rendant compte du potentiel de ce modèle en terme extraction de connaissances\n",
      "adaptation de algorithme cart pour la tarification des risques en assurance non-vie\n",
      "les développements récents en tarification de assurance non-vie se concentrent majoritairement sur la maîtrise et amélioration des modèles linéaires généralisés performants ces modèles imposent cependant à la fois des contraintes sur la structure du risque modélisé et sur les interactions entre variables explicatives du risque ces restrictions peuvent conduire dans certaines sous-populations d'assurés à une estimation biaisée de la prime assurance les arbres de régression permettent de s'affranchir de ces contraintes et de plus augmentent la lisibilité des résultats de la tarification nous présentons une modification de algorithme cart pour prendre en compte les spécificités des données assurance non-vie nous comparons alors notre proposition aux modèles linéaires généralisés sur un portefeuille réel de véhicules notre proposition réduit les mesures erreur entre le risque mesuré et le risque modélisé et permet ainsi une meilleure tarification\n",
      "agrégation robuste de données massives à la volée  application aux compteurs électriques communicants\n",
      "dans les années à venir plusieurs millions de compteurs électriques communicants seront déployés sur ensemble du territoire français afin assurer la fiabilité un réseau de cette envergure nous proposons une topologie de communication multi-chemins qui repose sur la duplication des données transmises toute exploitation des données collectées doit alors tenir compte de la présence éléments dupliqués dans cet article nous proposons une nouvelle méthode permettant de calculer en ligne des consommations électriques agrégées agrégation spatiale) idée est adapter algorithme probabiliste summation sketch de considine et al au contexte des compteurs communicants cette approche a avantage être insensible à la duplication et permet de profiter de la structure massivement distribuée du réseau de communication des futurs compteurs électriques expérimentation de cette méthode sur des données réelles montre qu'elle donne une bonne précision sur estimation des consommations agrégées cette approche est aussi complétée par une méthode basée sur la théorie des sondages  on obtient une meilleure réactivité de estimateur avec rapidement et donc sur des données significativement partielles une erreur inférieure à 2.5%\n",
      "aide à analyse visuelle de réseaux sociaux pour la détection de comportements suspects\n",
      "cet article traite de analyse visuelle de réseaux sociaux pour la détection de comportements suspects à partir de données de communications fournies à des enquêteurs suivant deux procédures  interception légale et la rétention de données nous proposons les contributions suivantes  i) un modèle de données et un ensemble opérateurs pour interroger ces données dans le but extraire des comportements suspects et ii) une représentation visuelle conviviale pour une navigation simplifiée dans les données de communication accompagnée avec une implémentation\n",
      "algorithmes de recherche exhaustif et guidé pour la recommandation un expert dans un réseau professionnel\n",
      "nan\n",
      "analyse comparative de méthodologies et outils de construction automatique ontologies à partir de ressources textuelles\n",
      "plusieurs méthodologies et outils de construction automatique des ontologies à partir de ressources textuelles ont été proposés ces dernières années dans cet article nous analysons quatre approches en les comparant à une approche de référence – methontology dans leur sélection nous avons privilégié celles qui couvrent ensemble des étapes du processus de construction d'ontologies puis nous analysons et comparons la portée les limites et les performances des implémentations logicielles associées aux approches analysées ces outils ont été testés sur un corpus de ressources textuelles et nous avons comparé leurs résultats à ceux obtenus manuellement\n",
      "analyse du comportement limite indices probabilistes pour une sélection discriminante\n",
      "nous étudions ici le comportement de deux types indices probabilistes discriminants en présence de données dont le volume va en croissant a cet égard un modèle spécifique de croissance de la taille des données et de liaison entre variables est mis en œuvre et celui-ci va permettre de déterminer le comportement limite des différents indices quel que soit le niveau de liaison entre la prémisse et la conclusion de la règle donnée la clarté des résultats obtenus nous conduit à en chercher explication formelle expérimentation a été effectuée avec la base de données uci wages\n",
      "analyse factorielle des correspondances hiérarchique pour la fouille images\n",
      "nous proposons un outil graphique interactif qui permet de visualiser et extraire des connaissances à partir des résultats de analyse factorielle des correspondances afc) sur les images afc est une technique descriptive développée pour analyser des tableaux de contingence afc est originellement utilisée dans analyse des données textuelles adt) où le corpus est représenté par un tableau de contingence croisant des documents et des mots dans la fouille images nous définissons abord les « mots visuels » dans les images analogues aux mots textuels) ces mots visuels sont construits à partir des descripteurs locaux sift scale invariant feature transform dans l'image ensuite nous appliquons afc sur le tableau de contingence obtenu notre outil appelé hcaviz analyse ce tableau de contingence de façon récursive et aide utilisateur à interpréter et interagir avec les résultats de afc. abord, les résultats de la première afc sur les images sont visualisés utilisateur sélectionne ensuite un groupe images et fait une deuxième afc sur le nouveau tableau de contingence ce processus peut continuer jusqu'à ce qu'un thème « pur » se dévoile ceci permet de découvrir une arborescence des thèmes dans une collection images une application sur la base caltech-4 illustre intérêt de hcaviz dans la fouille images\n",
      "analyse spatiotemporelle des vecteurs de mouvement  application au comptage des personnes\n",
      "cet article présente une nouvelle approche qui permet de compter le nombre individus franchissant une ligne de comptage approche proposée accumule dans le temps les vecteurs de mouvement pour chaque point de la ligne de comptage formant une carte spatiotemporelle une procédure de détection en ligne des blobs est ensuite utilisée afin de déterminer les régions de la carte spatiotemporelle qui correspondent à des personnes franchissant cette ligne le nombre individus associé à chaque blob est estimé grâce à un modèle de régression linéaire appliqué aux caractéristiques du blob approche proposée est validée sur la base de plusieurs ensembles de données enregistrées à aide une caméra verticale ou une caméra oblique\n",
      "annotation entités nommées par extraction de règles de transduction\n",
      "la reconnaissance entités nommées est une problématique majoritairement traitée par des modèles spécifiés à aide de règles ou par apprentissage numérique les premiers ont le désavantage être coûteux à développer pour obtenir une couverture satisfaisante les seconds sont souvent difficiles à interpréter par des experts (linguistes) dans cet article nous présentons une approche dont objectif est extraire des règles symboliques discriminantes qu'un humain puisse consulter a partir un corpus de référence nous extrayons des règles de transduction dont seules les plus informatives sont retenues elles sont ensuite appliquées pour effectuer une annotation  à cet effet un algorithme recherche parmi les annotations possibles celles de meilleure qualité en termes de couverture et de probabilité nous présentons les résultats expérimentaux et discutons de intérêt et des perspectives de notre approche\n",
      "apport de la catégorisation iconique pour la gestion coopérative des connaissances1\n",
      "nan\n",
      "apport des données thématiques dans les systèmes de recommandation  hybridation et démarrage à froid\n",
      "des travaux récents pilaszy et al. 2009 suggèrent que les métadonnées sont quasiment inutiles pour les systèmes de recommandation y compris en situation de cold-start  les données de logs de notation sont beaucoup plus informatives nous étudions sur une base de référence de logs usages pour la recommandation automatique de dvd (netflix) les performances de systèmes de recommandation basés sur des sources de données collaboratives thématiques et hybrides en situation de démarrage à froid (cold-start) nous exhibons des cas expérimentaux où les métadonnées apportent plus que les données de logs usage collaboratives) pour la performance prédictive pour gérer le cold-start un système de recommandation nous montrons que des approches \"en cascade\" thématiques puis hybrides puis collaboratives seraient plus appropriées\n",
      "apprendre les contraintes topologiques dans les cartes auto-organisatrices\n",
      "la carte auto-organisatrice som  self-organizing map est une méthode populaire pour analyse de la structure un ensemble de données cependant certaines contraintes topologiques de la som sont fixées avant apprentissage et peuvent ne pas être pertinentes pour la représentation de la structure des données dans cet article nous nous proposons améliorer les performances des som avec un nouvel algorithme qui apprend les contraintes topologiques de la carte à partir des données des expériences sur des bases de données artificielles et réelles montrent que algorithme proposé produit de meilleurs résultats que som classique ce n'est pas le cas avec une relaxation triviale des contraintes topologiques qui résulte en une forte augmentation de erreur topologique de la carte\n",
      "apprentissage génératif de la structure de réseaux logiques de markov à partir un graphe des prédicats\n",
      "les réseaux logiques de markov mlns) combinent apport statistique des réseaux de markov à la logique du premier ordre dans cette approche chaque clause logique se voit affectée un poids instanciation des clauses permettant alors de produire un réseau demarkov apprentissage un mln consiste à apprendre une part sa structure la liste de clauses logiques et autre part les poids de celles-ci nous proposons ici une méthode apprentissage génératif de réseau logique de markov cette méthode repose sur utilisation un graphe des prédicats produit à partir un ensemble de prédicats et une base apprentissage. une méthode heuristique de variabilisation est mise en oeuvre afin de produire le jeu de clauses candidates les résultats présentés montrent intérêt de notre approche au regard de état de l'art\n",
      "cartes cognitives  une exploitation à base d'échelle vue et profil\n",
      "une carte cognitive est un réseau influences entre différents concepts le modèle des cartes cognitives permet à un utilisateur de calculer influence entre deux concepts les cartes cognitives contenant un grand nombre de concepts et influences sont difficiles à comprendre cet article introduit la notion de carte cognitive ontologique qui associe une ontologie à une carte cognitive classique pour en organiser les concepts afin de faciliter la compréhension une carte utilisateur peut obtenir une vue de cette carte la simplifiant selon une échelle qu'il aura choisie un profil peut être créé pour construire des vues correspondant aux objectifs un type d'utilisateur si une carte est manipulée par différents utilisateurs leurs profils combinés permettent de construire une vue partagée\n",
      "catégorisation des mesures intérêt pour extraction des connaissances\n",
      "la recherche de règles association intéressantes est un domaine de recherche important et actif en fouille de données les algorithmes de la famille apriori reposent sur deux mesures pour extraire les règles le support et la confiance bien que ces deux mesures possèdent des vertus algorithmiques accélératrices elles génèrent un nombre prohibitif de règles dont la plupart sont redondantes et sans intérêt il est donc nécessaire de disposer autres mesures filtrant les règles inintéressantes des travaux ont été réalisés pour dégager les \"bonnes\" propriétés des mesures extraction des règles et ces propriétés ont été évaluées sur 61 mesures objectif de cet article est de dégager des catégories de mesures afin de répondre à une préoccupation des utilisateurs  le choix une ou plusieurs mesures lors un processus extraction des connaissances dans le but éliminer les règles valides non pertinentes extraites par le couple (support confiance) évaluation des propriétés sur les 61 mesures a permis de dégager 9 classes de mesures classes obtenues grâce à deux techniques  une méthode de la classification ascendante hiérarchique et une version de la méthode de classification non-hiérarchique des k-moyennes\n",
      "classificateurs aléatoires topologiques à base de graphes de voisinage\n",
      "en apprentissage supervisé les méthodes ensemble me) ont montré leurs qualités une des méthodes de référence dans ce domaine est les forêts aléatoires (fa) cette dernière repose sur des partitionnements de espace de représentation selon des frontières parallèles aux axes ou obliques les conséquences de cette façon de partitionner espace de représentation peuvent affecter la qualité de chaque prédicteur il nous a semblé que cette approche pouvait être améliorée si on se libérait de cette contrainte de manière à mieux coller à la structure topologique de ensemble d'apprentissage dans cet article nous proposons une nouvelle me basée sur des graphes de voisinage dont les performances sur nos premières expérimentations sont aussi bonnes que celles des fa\n",
      "classification des aéronefs par estimation de la pose\n",
      "dans le présent travail nous proposons un outil aide à la reconnaissance de cibles radar basé sur la signature de forme et de la pose de la cible la tâche principale dans le cadre de cet article consiste à établir la fonction de recherche images isar par exemple en exploitant information de pose estimée depuis les images isar objectif est introduire information de pose dans indexation des images notamment dans la phase de sélection des images candidates nous proposons une nouvelle méthode estimation de la pose basée sur axe le plus symétrique de la cible la méthode proposée est ensuite comparée avec autres techniques connues telles que la transformée de hough et la transformée en ondelette enfin la tâche de classification est réalisée en utilisant les k-plus proches voisins incluant information de la pose\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "closed-set-based discovery of representative association rules revisited\n",
      "the output of an association rule miner is often huge in practice this is why several concise lossless representations have been proposed such as the “essential” or “representative” rules we revisit the algorithm given by kryszkiewicz (int symp intelligent data analysis 2001 springer-verlag lncs 2189 350–359 for mining representative rules we show that its output is sometimes incomplete due to an oversight in its mathematical validation and we propose an alternative complete generator that works within only slightly larger running times\n",
      "comparaison entre deux indices pour évaluation probabiliste discriminante des règles association\n",
      "élaboration une échelle de probabilité discriminante pour la comparaison mutuelle entre plusieurs attributs observés sur un échantillon objets de \"grosse\" taille nécessite une normalisation préalable objet de cet article est analyse comparée entre deux approches la première dérive de l \"analyse de la vraisemblance des liens relationnels normalisée\" la seconde est fondée sur la notion de \"valeur test\" sur un échantillon virtuel de taille 100 synthétisant léchantillon initial\n",
      "complex information processing\n",
      "it is commonplace nowadays to claim that information is everywhere and that as a result finding the right information mathematically  according to a set of criteria optimizing a specific goal is very difficult defence applications have to cope with similar problems  communication networks surveillance and information systems transmit and generate significant amounts of complex information which cannot be processed with low level algorithms the challenge is to build high-level processing units which demand a lot of computing power so as process video streams and communication packets with little possibility of a false alarm as automatically as possible methods for processing aligning merging low-level and high-level information from syntactic to semantic information extracted from still images videos speech text and the internet are being considered the framework includes theoretical approaches algorithms as well as evaluation methods topics of interest are data fusion learning techniques data mining hci even artificial intelligence defence applications are numerous from scene understanding to weak signal detection\n",
      "conception et implémentation une nouvelle technique cellulaire de discrétisation  intégration dans tanagra\n",
      "nan\n",
      "construction une ontologie aide au renforcement de la sécurité des systèmes de transport automatisés\n",
      "nan\n",
      "construction ontologique à partir de séquences expression de champignons\n",
      "nan\n",
      "data stream summarization by on-line histograms clustering\n",
      "nan\n",
      "découverte de motifs évolution significatifs dans les séries temporelles images satellites\n",
      "les séries temporelles images satellites ou satellite image time series – sits sont importantes sources informations sur évolution du territoire étudier ces images permet de comprendre les changements sur des zones précises mais aussi de découvrir des schémas évolution à grande échelle toutefois découvrir ces phénomènes impose de répondre à plusieurs défis qui sont liés aux caractéristiques des sits et à leurs contraintes premièrement chaque pixel une image satellite est décrit par plusieurs valeurs les niveaux radiométriques sur différentes longueurs d'ondes) deuxièmement ces motifs évolution portent sur des périodes très longues et ne sont pas forcément synchrones selon les régions troisièmement les régions qui ne sont pas concernées par des évolutions significatives sont majoritaires et leur domination rend difficile extraction des motifs évolution. dans cet article nous proposons une méthode qui répond à ces difficultés et nous la validons sur une série images satellites acquises sur une période de 20 ans\n",
      "des graphes de documents aux réseaux sociaux\n",
      "nan\n",
      "détection de changements de distribution dans un flux de données  une approche supervisée\n",
      "analyse de flux de données traite des données massives grâce à des algorithmes en ligne qui évitent le stockage exhaustif des données la détection de changements dans la distribution un flux est une question importante dont les applications potentielles sont nombreuses dans cet article la détection de changement est transposée en un problème apprentissage supervisé nous avons choisi utiliser la méthode de discrétisation supervisée modl car celle-ci présente des propriétés intéressantes notre approche est comparée favorablement à une méthode de état-de-art sur des flux de données artificiels\n",
      "détection de redondances dans les tableaux guidée par une ontologie\n",
      "nous nous intéressons dans cet article à la réconciliation annotations floues associées à des tableaux de données par une méthode annotation sémantique qui est guidée par une ontologie de domaine etant donnés deux tableaux la méthode consiste à détecter leurs instances de relation redondantes elle s'appuie sur les connaissances déclarées dans l'ontologie ainsi que sur des scores de similarité entre les annotations floues représentées par des sous-ensembles flous numériques ou par des sous-ensembles flous symboliques\n",
      "détection des profils à long terme et à court terme dans les réseaux sociaux\n",
      "la conception des profils et contextes utilisateurs se situe au coeur de étude et de la mise en oeuvre des mécanismes de personnalisation ou adaptation de contenus recherche d'information systèmes de recommandation etc) plusieurs modèles et dimensions de profils et contextes sont décrits dans la littérature dans la vie réelle tout comme dans les systèmes d'information le comportement de utilisateur est très souvent influencé par son environnement social cependant la dimension sociale des profils et contextes utilisateurs reste très peu étudiée et évaluée dans cet article nous présentons une méthode de visualisation des profils utilisateurs permettant évaluer la pertinence du réseau social de utilisateur dans évolution de son profil expérimentation de la méthode à partir de facebook permet identifier une part les centres intérêts à court-terme et à long-terme des profils utilisateurs et autre part influence réelle à court-terme et à long-terme du réseau social de chaque utilisateur ces résultats démontrent intérêt de modéliser et intégrer une dimension sociale dans les profils et contextes utilisateurs afin de tenter améliorer les mécanismes de personnalisation ou adaptation de contenus\n",
      "early classification on temporal sequences\n",
      "early classification of temporal sequences has applications in for example health informatics intrusion detection anomaly detection and scientific and engineering sequence data monitoring in early classification instead of optimizing accuracy our goal is to produce classification as early as possible provided that the accuracy meets some expectation in this talk i will advocate early classification as an exciting and challenging research problem which has not been systematically studied in the literature i will discuss several interesting formulations of the problem which provide complimentary features possibly desirable in different application scenarios i will also review some of our recent progress on this aspect\n",
      "entropic-genetic clustering\n",
      "this paper addresses the clustering problem given the similarity matrix of a dataset we define two distinct criteria with the aim of simultaneously minimizing the cut size and obtaining balanced clusters the first criterion minimizes the similarity between objects belonging to different clusters and is an objective generally met in clustering the second criterion is formulated with the aid of generalized entropy the trade-off between these two objectives is explored using a multi-objective genetic algorithm with enhanced operators\n",
      "equilibrer analyse des motifs fréquents\n",
      "cet article propose une méthode originale évaluation de la qualité des motifs en anticipant la manière qui sera utilisée pour les analyser nous commençons par introduire le modèle de analyse aléatoire un ensemble de motifs selon une mesure d'intérêt avec ce modèle nous constatons que étude des motifs fréquents avec le support conduit à une analyse déséquilibrée du jeu de données afin que chaque transaction reçoive la même attention nous définissons le support équilibré qui corrige le support classique en pondérant les transactions nous proposons alors un algorithme qui calcule ces poids et nous validons expérimentalement son efficacité\n",
      "equivalence topologique entre mesures de proximité\n",
      "le choix une mesure de proximité entre objets a un impact direct sur les résultats de toute opération de classification de comparaison évaluation ou de structuration un ensemble d'objets pour un problème donné utilisateur est amené à choisir une parmi les nombreuses mesures de proximité existantes or selon la notion équivalence choisie comme celle basée sur les préordonnances certaines sont plus ou moins équivalentes dans cet article nous proposons une nouvelle approche pour comparer les mesures de proximité celle-ci est basée sur équivalence topologique a cet effet nous introduisons un nouveau concept baptisé équivalence topologique ce dernier fait appel à la structure de voisinage local nous proposons alors de définir équivalence topologique entre deux mesures de proximité à travers la structure topologique induite par chaque mesure nous établissons ensuite des liens formels avec équivalence en préordonnance les deux approches sont comparées sur le plan théorique et sur le plan empirique nous illustrons le principe de cette comparaison sur un exemple simple pour une quinzaine de mesures de proximités de la littérature\n",
      "estimation de la densité arcs dans les graphes de grande taille une alternative à la détection de clusters\n",
      "la recherche de structures dans les graphes est un sujet étudié depuis longtemps qui a bénéficié un regain intérêt avec la mise à disposition de graphes de grande taille sur le web tels les réseaux sociaux de nombreuses méthodes de recherche de clusters “naturels” dans les graphes ont été proposées fondées notamment sur la modularité de newman on introduit dans cet article une nouvelle façon de résumer la structure des graphes de grande taille en utilisant des estimateurs de densité des arcs exploitant des modèles en grille basés sur un co-partitionnent des noeuds source et cible des arcs les structures identifiées par cette méthode vont au delà de la “classique” détection de clusters dans les graphes et permettent estimer asymptotiquement la densité des arcs les expérimentations confirment le potentiel de l'approche qui permet identifier des structures fortement informatives dans les graphes sans faire hypothèse une décomposition en clusters denses\n",
      "être ou ne pas être usager internet telle est la question \n",
      "nan\n",
      "evaluation des outils extraction terminologique quezao et acabit\n",
      "article décrit évaluation de deux outils extraction terminologique acabit et quezao si acabit est plus connu car librement disponible quezao est issu des travaux orange labs sur la recherche d'informations après une comparaison sur les approches théoriques des deux systèmes une évaluation concrète va porter sur un corpus actualité 2424actu) pour aspect qualitatif et sur un corpus de presse pour aspect quantitatif\n",
      "extraction de motifs séquentiels contextuels\n",
      "les motifs séquentiels traditionnels ne tiennent généralement pas compte des informations contextuelles fréquemment associées aux données séquentielles dans le cas des séquences achats de clients dans un magasin extraction classique de motifs se focalise sur les achats des clients sans considérer leur catégorie socio-professionnelle leur sexe leur âge or en considérant le fait qu'un motif séquentiel est spécifique à un contexte donné un expert pourra adapter sa stratégie au type du client et prendre les décisions adéquates dans cet article nous proposons extraire des motifs de la forme «achat des produits a et b suivi de achat du produit c est spécifique aux jeunes clients» en mettant en valeur les propriétés formelles de tels contextes nous développons un algorithme efficace extraction de motifs séquentiels contextuels les expérimentations effectuées sur un jeu de données réelles montrent les apports et efficacité de approche proposée\n",
      "extraction de motifs temporels à partir de séquences événements avec intervalles temporels\n",
      "la fouille de base de données séquentielles a pour objet extraction de motifs séquentiels représentatifs la plupart des méthodes concernent des motifs composés événements liés par des relations temporelles basées sur la précédence des instants pourtant dans de nombreuses situations réelles une information quantitative sur la durée des événements ou le délai inter-événements est nécessaire pour discriminer les phénomènes nous proposons deux algorithmes qtiapriori et qtiprefixspan pour extraire des motifs temporels composés événements associés à des intervalles décrivant leur position dans le temps et leur durée chacun eux ajoute aux algorithmes gsp et prefixspan une étape de catégorisation intervalles multi-dimensionnels pour extraire les intervalles temporelles représentatifs les expérimentations sur des données simulées montrent la capacité des algorithmes à extraire des motifs précis en présence de bruit et montrent amélioration des performances en temps de calcul\n",
      "extraction et analyse de réseaux sociaux issus de bases de données relationnelles\n",
      "dans un contexte d'entreprise beaucoup informations importantes restent stockées dans des bases de données relationnelles constituant une source riche pour construire des réseaux sociaux le réseau ainsi extrait a souvent une taille importante ce qui rend son analyse et sa visualisation difficiles dans ce travail nous proposons une étape extraction suivie une étape agrégation des réseaux sociaux à partir des bases de données relationnelles étape extraction ou de construction transforme une base de données relationnelle en base de données graphe puis le réseau social est extrait étape agrégation, qui est basée sur algorithme k-snap produit un graphe résumé\n",
      "extraction sous contraintes ensembles de cliques homogènes\n",
      "nous proposons une méthode de fouille de données sur des graphes ayant un ensemble étiquettes associé à chaque sommet une application est par exemple analyser un réseau social de chercheurs co-auteurs lorsque des étiquettes précisent les conférences dans lesquelles ils publient.nous définissons extraction sous contraintes ensembles de cliques tel que chaque sommet des cliques impliquées partage suffisamment étiquettes. nous proposons une méthode pour calculer tous les ensembles maximaux de cliques dits homogènes qui satisfont une conjonction de contraintes fixée par analyste et concernant le nombre de cliques séparées la taille des cliques ainsi que le nombre étiquettes partagées les expérimentations montrent que approche fonctionne sur de grands graphes construits à partir de données réelles et permet la mise en évidence de structures intéressantes\n",
      "heuristique pour extraction de motifs ensemblistes bruités\n",
      "la recherche de motifs ensemblistes dans des matrices de données booléennes est une problématique importante dans un processus extraction de connaissances elle consiste à rechercher tous les rectangles de 1 dans une matrice de données à valeurs dans {0,1} dans lesquelles ordre des lignes et colonnes n'est pas important plusieurs algorithmes ont été développés pour répondre à ce problème mais s'adaptent difficilement à des données réelles susceptibles de contenir du bruit un des effets du bruit est de pulvériser un motif pertinent en un ensemble de sous-motifs recouvrants et peu pertinents entraînant une explosion du nombre de motifs résultats dans le cadre de ce travail nous proposons une nouvelle approche heuristique basée sur les algorithmes de graphes pour la recherche de motifs ensemblistes dans des contextes binaires bruités pour évaluer notre approche différents tests ont été réalisés sur des données synthétiques et des données réelles issues applications bioinformatiques\n",
      "import automatique et interactif de données dans les systèmes de visualisations\n",
      "la première étape du processus de visualisation information consiste à transformer les données un format brut vers une structure de données utilisable par les différents composants de visualisation dans les applications réelles cette première étape représente une barrière empêchant accès des utilisateurs novices à une riche variété de techniques de visualisation par exemple il peut être techniquement impossible pour un utilisateur lambda de transformer des données arborescentes en un modèle de graphe pouvant utiliser une représentation à base de treemap une autre barrière est aussi la multitude de transformations possible des données brutes il faut pouvoir explorer cet ensemble de combinaisons basé sur nos retours expériences avec des utilisateurs finaux dans cet article nous considérons que le format brut est sous forme tabulaire ce format est le plus couramment utilisé et est facilement accessible par nos utilisateurs nous proposons une méthode novatrice permettant de générer automatiquement des graphes valués à partir de n'importe quelle table en analysant le contenu de chaque dimension nous identifions les interconnexions entre celles-ci puis nous caractérisons les entités les attributs et les relations possibles au sein des tables finalement nous intégrons utilisateur dans le processus de transformation en lui proposant un ensemble de transformations valides\n",
      "intégration de données haptiques brutes dans des systèmes experts de diagnostic des connaissances\n",
      "cet article a pour cadre un environnement informatique pour apprentissage humain eiah) dédié à la chirurgie orthopédique et plus précisément sur le diagnostic des connaissances des apprenants pour ce faire un réseau bayésien infère à partir exercices que les étudiants réalisent sur un simulateur avec bras articulé ce réseau résulte une approche centrée expert du domaine comme très souvent dans les eiah pourtant dans un domaine comme la chirurgie où les connaissances sont tacites le geste de apprenant semble intéressant à considérer le but de nos travaux est donc adopter une démarche plus centrée sur les données en incorporant au réseau bayésien les données haptiques continues issues du simulateur divers problèmes se posent néanmoins une part sur le besoin étudier la nature des données pour conserver la généricité du système et autre part pour trouver des méthodes de validation pertinentes concernant leur traitement\n",
      "interprétation graphique de la courbe roc\n",
      "nan\n",
      "interprétation spectrale de la classification relationnelle\n",
      "ce papier présente une vue spectrale sur approche de analyse relationnelle pour la classification des données catégorielles il établit abord le lien théorique entre approche de analyse relationnelle et le problème de classification spectrale en particulier le problème de classification relationnelle est présenté comme un problème de maximisation de trace ce problème est donc transformé par la relaxation spectrale en un problème optimisation sous contraintes qui peut être résolu par des multiplicateurs de lagrange la solution est donnée par un problème de valeurs propres\n",
      "introduction de ingénierie ontologique dans la méthodologie de développement des progiciels de gestion des collectivités territoriales\n",
      "nan\n",
      "isicil  intégration sémantique informations à travers des communautés intelligence en ligne\n",
      "nan\n",
      "les moteurs de wikis sémantiques  un état de art\n",
      "cet article est un état de art sur les moteurs de wiki sémantique en particulier sur leur utilisation des technologies du web sémantique les principales notions liées aux wikis sémantiques sont abord présentées ensuite plusieurs projets actifs de moteurs de wiki sont comparés selon différents points de vue finalement des recommandations sont données pour le choix un moteur de wiki en conclusion les auteurs s'interrogent sur les perspectives des wikis sémantiques telles que la faible interopérabilité de certains moteurs\n",
      "m3a  une plateforme ingénierie de maintenance assistée par apprentissage automatique\n",
      "nan\n",
      "mesure de concordance pour les bases de données évidentielles\n",
      "dans cet article nous proposons une mesure de concordance une source avec les autres sources cette mesure pourra servir à réduire importance de ses fonctions de masse avant de les combiner afin de trouver un compromis et donc réduire le conflit cette mesure sera illustrée par des données réelles\n",
      "mesures hétérogénéité sémantique des systèmes p2p non-structurés\n",
      "autonomie des participants dans les systèmes p2p pour le partage de données peut conduire à une situation hétérogénéité sémantique dans le cas où les participants utilisent leurs propres ontologies pour représenter leurs données dans cet article nous commençons par définir des mesures de disparité entre participants en considérant leurs contextes sémantiques en considérant la topologie du système et les disparités entre participants nous proposons des mesures hétérogénéité sémantique un système p2p non-structuré\n",
      "mixer les moyens pour extraire les gloses\n",
      "nous proposons extraire des connaissances lexicales en exploitant les « gloses » de mot ces descriptions spontanées de sens repérables par des marqueurs lexicaux et des configurations morpho-syntaxiques spécifiques ainsi dans extrait suivant le mot testing est suivi une glose en c'est-à dire  « 10 % de ces embauches vont porter sur un métier qui monte  le «testing» c'est-à-dire la maîtrise des méthodologies rigoureuses de test des logiciels» cette approche ouvre des perspectives pour acquisition lexicale et terminologique fondamentale pour de nombreuses tâches dans cet article nous comparons deux façons extraire les unités en relation de glose  patrons et statistiques associations unités sur le web en les évaluant sur des données réelles\n",
      "mobility data mining and privacy mining human movement patterns from trajectory data\n",
      "the technologies of mobile communications and ubiquitous computing pervade our society and wireless networks sense the movement of people and vehicles generating large volumes of mobility data such as mobile phone call records and gps tracks this is a scenario of great opportunities and risks  on one side mining this data can produce useful knowledge supporting sustainable mobility and intelligent transportation systems  on the other side individual privacy is at risk as the mobility data contain sensitive personal information a new multidisciplinary research area is emerging at this crossroads of mobility data mining and privacy the talk assesses this research frontier from a data mining perspective and illustrates the results of a european-wide research project called geopkdd geographic privacy-aware knowledge discovery and delivery geopkdd has created an integrated platform named matlas for complex analysis of mobility data which combines spatio-temporal querying capabilities with data mining visual analytics and semantic technologies thus providing a full support for the mobility knowledge discovery process in this talk we focus on the key data mining models  trajectory patterns and trajectory clustering and illustrate the analytical power of our system in unvealing the complexity of urban mobility in a large metropolitan area by means of a large scale experiment based on a massive real life gps dataset obtained from 17,000 vehicles with on-board gps receivers tracked during one week of ordinary mobile activity in the urban area of the city of milan italy\n",
      "modèle pour une analyse du phénomène de linéarité de catégories sémantiques dans les énoncés en français\n",
      "nan\n",
      "modélisation une ressource termino-ontologique de domaine pour annotation sémantique de tableaux\n",
      "nous proposons dans cet article une modélisation une ressource termino-ontologique rto) de domaine guidée par la tâche annotation sémantique de tableaux annotation un tableau consiste à annoter ses cellules pour pouvoir ensuite identifier les concepts représentés par ses colonnes et enfin identifier la ou les relations n-aires qu'il représente la rto proposée permet une part de modéliser dans sa composante lexicale les termes utilisés pour annotation des cellules en intégrant la gestion des synonymes et du multilingue et autre part de modéliser dans sa composante conceptuelle les concepts symboliques les concepts numériques et les relations n-aires qui sont propres au domaine étudié\n",
      "modélisation de la dynamique de phénomènes spatio-temporels par des séquences de motifs\n",
      "dans ce papier nous proposons un nouveau cadre théorique permettant de modéliser la dynamique de phénomènes spatio-temporels nous définissons le concept de séquences spatio-temporelles de motifs afin de capturer les interactions entre des ensembles de propriétés et un phénomène à observer un algorithme incrémental est proposé pour extraire des séquences spatiotemporelles de motifs sous contraintes et une nouvelle structure de données est mise en place afin améliorer ses performances un prototype a été développé et testé sur des données réelles\n",
      "modélisation de la propagation de information sur le web  de extraction des données à la simulation\n",
      "nous proposons un modèle de la propagation de information dans un réseau en détaillant toutes les étapes de sa réalisation et de son utilisation dans un cadre de simulation a partir de données réelles extraites du web nous identifions parmi les sources des catégories de comportements de publication distincts nous proposons ensuite une extension un modèle de diffusion de information existant afin augmenter son pouvoir d'expression en particulier pour reproduire ces comportements de publication puis nous le validons sur un exemple de simulation\n",
      "moteur de questions réponses à partir de données du web sémantique\n",
      "nan\n",
      "moteur de questions-réponses une base de connaissances\n",
      "cet article présente comment la gestion et exploitation de connaissances issues du site web wikipedia ont permis de développer une telle fonction qui a été intégrée depuis février 2010 dans un moteur de recherche internet français pour le grand public aujourd'hui cette fonction est capable de répondre à des questions formulées en langage naturelle sur environs 170 000 lieux ou personnes la formalisation des données extraites de wikipedia en connaissances au format owl ou rdfs a permis de déduire de nouvelles informations manquantes de typer les entités nommées trouvées et de traiter de nouvelles formes de questions qui étaient non traitées\n",
      "motifs séquentiels delta-libres\n",
      "bien que largement étudiée extraction de motifs séquentiels reste une tâche très difficile et pose aussi le défi du grand nombre de motifs produits dans cet article nous proposons une nouvelle approche extrayant les motifs séquentiels les plus généraux à fréquence similaire nous montrons en quoi extension de cette notion déjà connue pour les motifs ensemblistes est un problème particulièrement difficile pour les séquences les motifs delta-libres ainsi produits sont en nombre réduit et facilitent les usages un processus de fouille et nous montrons leur apport comme descripteurs dans un contexte de classification de séquences\n",
      "mumie une approche automatique pour interopérabilité des métadonnées\n",
      "avec explosion du multimedia utilisation des métadonnées est devenue cruciale pour assurer une bonne gestion des contenus cependant il est nécessaire d assurer un accès uniforme aux métadonnées plusieurs techniques ont ainsi été développées afin de réaliser cette interopérabilité la plupart entre elles sont spécifiques à un seul langage de description les systèmes de matching existants présentent certaines limites en particulier dans le traitement des informations structurelles nous présentons dans cet article un nouveau système intégration qui supporte des schémas provenant de langages descriptifs différents de plus la méthode de matching proposée a recours à plusieurs types information de façon à augmenter la précision de matching\n",
      "nomao  la recherche géolocalisée personnalisée\n",
      "nan\n",
      "nouvelle approche de fouille de graphes ac-réduits fréquents\n",
      "la fouille de graphes est devenue une piste de recherche intéressante et un défi réel en matière de fouille de données parmi les différentes familles de motifs de graphes les graphes fréquents permettent une caractérisation intéressante des groupes de graphes ainsi qu'une discrimination des différents graphes lors de la classification ou de la segmentation a cause de la np-complétude du test isomorphisme de sous-graphes et de immensité de espace de recherche les algorithmes de fouille de graphes sont exponentiels en temps exécution et/ou occupation mémoire dans cet article nous étudions un nouvel opérateur de projection polynomial nommé ac-projection basé sur une propriété clé du domaine de la programmation par contraintes à savoir arc consistance cet opérateur est censé remplacer utilisation de isomorphisme de sous-graphes en établissant un biais sur la projection cette étude est suivie une évaluation expérimentale du pouvoir discriminant des patterns ac-réduits découverts\n",
      "optimisation de extraction de alignement des ontologies avec la contrainte de différence\n",
      "dans ce papier nous proposons une approche basée sur la programmation par contraintes pour aborder efficacement le problème de alignement des ontologies et plus particulièrement extraction des correspondances à partir des mesures de similarités la complexité de ce problème est accentuée dans les applications à caractère dynamique où aspect performance est capital plus précisément nous exploitons la contrainte globale de différence développée dans le domaine de la programmation par contraintes pour extraire un alignement total et injectif nous montrons que cette approche est efficace et se prête à une mise en oeuvre à la fois interactive et automatique\n",
      "optimisation directe des poids de modèles dans un prédicteur bayésien naïf moyenné\n",
      "le classifieur bayésien naïf est un outil de classification efficace en pratique pour de nombreux problèmes réels en dépit de hypothèse restrictive indépendance des variables conditionnellement à la classe récemment de nouvelles méthodes permettant améliorer la performance de ce classifieur ont vu le jour sur la base à la fois de sélection de variables et de moyennage de modèles dans cet article nous proposons une extension de la sélection de variables pour le classifieur bayésien naïf en considérant un modèle de pondération des variables utilisées et des algorithmes optimisation directe de ces poids les expérimentations confirment la pertinence de notre approche en permettant une diminution significative du nombre de variables utilisées sans perte de performance prédictive\n",
      "parameter-free association rule mining with yacaree\n",
      "nan\n",
      "point of view based clustering of socio-semantic networks\n",
      "nan\n",
      "pondération et classification simultanée de données binaires et continues\n",
      "dans cet article nous proposons une nouvelle approche de classification topologique et de pondération des variables mixtes qualitatives et quantitatives codées en binaire durant un processus apprentissage non supervisé cette approche est basée sur le modèle des cartes auto-organisatrices apprentissage est combiné à un mécanisme de pondération des différentes variables sous forme de poids influence sur la pertinence des variables apprentissage des pondérations et des prototypes est réalisé une manière simultanée en favorisant une classification optimisée des données approche proposée a été validée sur des données qualitatives codées en binaire et plusieurs bases de données mixtes\n",
      "ppmi  étude formelle une variante à valeurs positives de la pmi\n",
      "nan\n",
      "prévision de trajectoires de cyclones à aide de forêts aléatoires avec arbres de régression\n",
      "nous présentons une étude pour la prédiction des trajectoires de cyclones dans océan atlantique nord à partir de données issues images satellites on y extrait des mesures de vitesses de vent de vorticité humidité base jra-25 et des mesures de latitude de longitude et de vitesse de vent instantanée des cyclones toutes les 6 heures base ibtracs) les modèles de référence à ce jour ne tiennent pas compte des corrélations entre les données et les prévisions ce qui limite leur intérêt pour certains utilisateurs nous proposons ainsi de prédire le déplacement en latitude et le déplacement en longitude au même instant à un horizon de 120 h toutes les 6 h à aide de forêts aléatoires avec arbres de régression sur le long terme à partir de 18 h la méthode proposée donne de meilleurs résultats que les méthodes existantes\n",
      "prise en compte du réseau de sources pour la fusion informations\n",
      "nan\n",
      "propositionaliser des attributs numériques sans les discrétiser ni les agréger\n",
      "la fouille de données relationnelles considère des données contenues dans au moins deux tables reliées par une association un-à-plusieurs par exemple des clients et leurs achats ou des molécules et leurs atomes une façon de fouiller ces données consiste à transformer les données en une seule table attribut-valeur cette transformation est appelée propositionalisation les approches existantes gèrent principalement les attributs catégoriels une première solution est donc de discrétiser les attributs numériques pour les transformer en attributs catégoriels les approches alternatives qui gèrent les attributs numériques consistent à les agréger nous proposons une approche duale de la discrétisation qui inverse ordre de traitement du nombre objets et du seuil et dont la discrétisation généralise les quartiles nous pouvons ainsi construire des attributs que les approches existantes de propositionalisation ne peuvent pas construire et qui ne peuvent pas non plus être obtenus par les systèmes complets de fouille de données\n",
      "reasoning about the learning process\n",
      "data mining is faced with new challenges in emerging applications like financial data traffic tcp/ip sensor networks etc data continuously flow eventually at high speed the processes generating data evolve over time and the concepts we are learning change in this talk we present a one-pass classification algorithm able to detect and react to changes we present a framework that identify contexts using drift detection characterize contexts using meta-learning and select the most appropriate base model for the incoming data using unlabeled examples evolving data requires that learning algorithms must be able to monitor the learning process and the ability of predictive self-diagnosis a significant and useful characteristic is diagnostics - not only after failure has occurred but also predictive before failure) these aspects require monitoring the evolution of the learning process taking into account the available resources and the ability of reasoning and learning about it\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reconnaissance actions par modélisation du mouvement\n",
      "cet article propose une approche utilisant les modèles de direction et de magnitude de mouvement pour détecter les actions qui sont effectuées par des êtres humains dans des séquences vidéo des mélanges gaussiens et de lois de von mises sont estimés à partir des orientations et des magnitudes des vecteurs du flux optique calculés pour chaque bloc de la scène les paramètres de ces modèles sont estimés grâce à un algorithme apprentissage en ligne les actions sont reconnues grâce à une mesure qui se base sur la distance de bhattacharyya et qui permet de comparer le modèle une séquence donnée avec les modèles créés à partir de séquences apprentissage. approche proposée est évaluée sur deux ensembles de vidéos contenant des actions variées exécutées aussi bien dans des environnements intérieur qu'extérieur\n",
      "résumés et interrogations de logs de requêtes olap\n",
      "une façon assister analyse entrepôt de données repose sur exploitation et la fouille de fichiers logs de requêtes olap mais à notre connaissance il n'existe pas de méthode permettant obtenir une représentation un tel log qui soit à la fois concise et exploitable dans ce papier nous proposons une méthode pour résumer et interroger des logs de requêtes olap idée de base est qu'une requête résume une autre requête et qu'un log qui est une séquence de requêtes résume un autre log notre cadre formel est composé une algèbre simple destinée à résumer des requêtes olap et une mesure évaluant la qualité du résumé obtenu nous proposons également plusieurs stratégies pour calculer automatiquement des résumés de logs de bonne qualité et nous montrons comment des propriétés simples sur les résumés peuvent être utilisées pour interroger un log efficacement des tests sur des logs de requêtes mdx ont montré intérêt de notre approche\n",
      "sélection des variables informatives pour apprentissage supervisé multi-tables\n",
      "dans la fouille de données multi-tables les données sont représentées sous un format relationnel dans lequel les individus de la table cible sont potentiellement associés à plusieurs enregistrements dans des tables secondaires en relation un-à-plusieurs la plupart des approches existantes opèrent en transformant la représentation multi-tables notamment par mise à plat par conséquent on perd la représentation initiale naturellement compacte mais également on risque introduire des biais statistiques notre approche a pour objectif évaluer informativité des variables explicatives originelles par rapport à la variable cible dans le contexte des relations un-à-plusieurs elle consiste à résumer information contenue dans chaque variable par un tuple attributs représentant les effectifs des modalités de celle-ci des modèles en grilles multivariées sont alors employés pour qualifier information apportée conjointement par les nouveaux attributs ce qui revient à une estimation de densité conditionnelle de la variable cible connaissant la variable explicative en relation un-à-plusieurs les premières expérimentations sur des bases de données artificielles et réelles montrent qu'on arrive à identifier les variables explicatives potentiellement pertinentes sur tout le domaine relationnel\n",
      "service de recherche web3.0 de contenus audiovisuels\n",
      "nan\n",
      "structuration automatique des flux télévisuels par apprentissage non supervisé des répétitions\n",
      "nan\n",
      "système de recherche de musique adaptable à la perception de chaque utilisateur\n",
      "dans le cadre de nos travaux sur le portage linguistique des systèmes de gestion de contenu traitant des énoncés spontanés en langue naturelle nous présentons ici une évaluation du portage imrs système de recherche de morceau de musique en langue naturelle kumamoto 2007) du japonais vers le français cette évaluation peut se faire au niveau des représentations internes en les comparant ou au niveau de la tâche ici nous nous intéressons à une évaluation liée à la tâche en proposant un service web qui permet de mesurer la performance globale de la nouvelle version obtenue nous avons par la suite cherché à améliorer et ajouter de nouvelles fonctionnalités en proposant un service de recherche de musique adaptable à la perception de chaque utilisateur en effet un même morceau de musique peut être jugé calme pour un premier auditeur très calme pour un deuxième et assez calme pour un troisième etc on se demande impression finale que porte ce dernier morceau de musique c'est naturel que les utilisateurs évaluent différemment un même morceau de musique car ils ont des perceptions différentes devant cette situation nous proposons un service de recherche de musique basé des méthodes simples et automatisées et qui sont adaptables à la perception de chaque utilisateur\n",
      "système pour la catégorisation automatique des offres emploi en une typologie de fonctions\n",
      "depuis les deux dernières décennies augmentation du nombre de sites emploi sur internet a accentué la nécessité de proposer des outils aide à la décision adaptés aux besoins des recruteurs cet article présente un système pour la catégorisation des textes offres emploi destinées à être diffusées sur internet après un pré-traitement adapté des offres les termes descripteurs sont choisis en fonction de leur pouvoir discriminant vis-à-vis des différentes classes ce qui permet de réduire leur nombre de manière significative les offres sont ensuite représentées par leurs coordonnées dans espace factoriel obtenu par analyse des correspondances et la classification réalisée dans un cadre supervisé à aide de svm\n",
      "towards a distributedweb search engine\n",
      "in the ocean of web data web search engines are the primary way to access content as the data is on the order of petabytes current search engines are very large centralized systems based on replicated clusters web data however is always evolving the number of web sites continues to grow rapidly 230 millions at the end of 2009 and there are currently more than 20 billion indexed pages on the other hand internet users are above one billion and hundreds of million of queries are issued each day in the near future centralized systems are likely to become less effective against such a data-query load thus suggesting the need of fully distributed search engines such engines need to maintain high quality answers fast response time high query throughput high availability and scalability  in spite of network latency and scattered data in this talk we present the main challenges behind the design of a distributed web retrieval system and our research in all the components of a search engine  crawling indexing and query processing\n",
      "treillis des concepts skylines  analyse multidimensionnelle des skylines fondée sur les ensembles en accord\n",
      "le concept de skyline a été introduit pour mettre en évidence les objets « les meilleurs » selon différents critères une généralisation multidimensionnelle du skyline a été proposée à travers le skycube qui réunit tous les skylines possibles selon toutes les combinaisons de critères et permet analyser les liens entre objets skylines comme le data cube le skycube s'avère extrêmement volumineux si bien que des approches de réduction sont incontournables dans cet article nous définissons une approche de matérialisation partielle du skycube idée sous-jacente est éliminer de la représentation les skycuboïdes facilement re-calculables pour atteindre cet objectif de réduction nous caractérisons un cadre formel  le treillis des concepts accords cette structure combine la notion ensemble en accord et le treillis des concepts à partir de cette structure nous dérivons le treillis des concepts skylines qui en est une instance contrainte le point fort de notre approche est être orientée attribut ce qui permet de borner le nombre de noeuds du treillis et obtenir une navigation efficace à travers les skycuboïdes\n",
      "un critère bayésien pour évaluer la robustesse des règles de classification\n",
      "utilisation de règles de classification dans les modèles prédictifs a été très étudiée ces dernières années la forme simple et interprétable des règles en font des motifs très populaires les classifieurs combinant des règles de classification intéressantes selon une mesure intérêt offrent de bonnes performances de prédictions cependant les performances de ces classifieurs dépendent de la mesure intérêt (e.g. confiance taux d'accroissement   et du seuillage (non-trivial de cette mesure pour déterminer les règles pertinentes de plus il est facile de montrer que les règles extraites ne sont pas individuellement robustes dans cet article nous proposons un nouveau critère pour évaluer la robustesse des règles de classification dans les données booléennes notre critère est issu une approche bayésienne  nous proposons une expression analytique de la probabilité une règle connaissant les données ainsi les règles les plus probables sont robustes le critère bayésien nous permet alors identifier sans paramètre les règles robustes parmi un ensemble de règles données\n",
      "un cycle de vie complet pour enrichissement sémantique des folksonomies\n",
      "les tags fournis par les utilisateurs des plateformes de tagging social ne sont pas explicitement liés sémantiquement et ceci limite considérablement les possibilités exploitation de ces données nous présentons dans cet article notre approche pour enrichissement sémantiques des folksonomies qui intègre une combinaison de traitements automatiques ainsi que la capture des contributions de structuration des utilisateurs via une interface ergonomique de plus notre modèle supporte les points de vue qui divergent tout en permettant de les combiner en respectant leur cohérence locale cette approche s'adresse aux communautés de connaissances collaborant en ligne et en intégrant leurs usages nous sommes en mesure de proposer un cycle de vie complet pour le processus de structuration sémantique des folksonomies la navigation dans les données de tagging est ainsi améliorée et les folksonomies peuvent alors être directement intégrées dans la construction de thesauri\n",
      "un outil de géolocalisation et de résumé automatique pour faciliter accès à information dans des corpus actualité\n",
      "nan\n",
      "un outil de navigation dans un espace sémantique\n",
      "nan\n",
      "un système cellulaire neuro-symbolique pour extraction et la gestion des connaissances\n",
      "le cnss – cellular neuro-symbolic system – est un système hybride ralliant conjointement le neuro-symbolique et le cellulaire cnss permet à partir une base de cas pratique de faire coopérer un réseau de neurones un graphe induction et un automate cellulaire pour la construction un modèle de prédiction en détectant et en éliminant les individus non applicables et les variables non pertinentes le réseau de neurones optimise la base apprentissage le résultat ainsi obtenu est affiné par un processus apprentissage symbolique à base de graphe induction. ce raffinement se fait par une modélisation booléenne qui va assister apprentissage symbolique à optimiser le graphe induction et va assurer par la suite la représentation et la génération des règles de classification sous forme conjonctives avant entamer la phase de déduction par un moteur inférence cellulaire cnss a été testé sur plusieurs applications en utilisant des problèmes académiques et réels les résultats montrent que le système cnss a des performances supérieures et de nombreux avantages\n",
      "une approche à base de règles floues pour les requêtes à préférences contextuelles\n",
      "nan\n",
      "une mesure de distance dans espace des alignements entre parties potentiellement homologues de deux ontologies légères\n",
      "nous proposons dans cet article une méthode qui calcule la distance entre ontologies dans un but aide à la décision sur la pertinence ou non de leur fusion cette méthode calcule la distance entre parties homologues de deux ontologies par rapport à leurs niveaux de détail et leurs structures taxonomiques et ce en exploitant les correspondances produites par un alignement préalablement effectué entre ces ontologies et en adaptant la méthode de la distance édition entre arbres ordonnés nous limitons notre étude ici aux ontologies légères c'est à dire des taxonomies représentées en langages owl le langage ontologies pour le web notre méthode a été implémentée et testée sur des ontologies réelles et les résultats obtenus semblent prometteurs\n",
      "une méthodologie de recommandations produits fondée sur actionnabilité et intérêt économique des clients\n",
      "dans un contexte économique difficile la fidélisation des clients figure au premier rang des préoccupations des entreprises en effet selon le gartner fidéliser des clients existants coûterait beaucoup moins cher que prospecter de nouveaux clients pour y parvenir les entreprises optimisent la marge et le cycle de vie des clients en développant une relation personnalisée aboutissant à de meilleures recommandations dans cet article nous proposons une méthodologie pour les systèmes de recommandations fondée sur analyse des chiffres affaires des clients sur des familles de produits plus précisément la méthodologie consiste à extraire des comportements de référence sous la forme de règles association et à en évaluer intérêt économique et l'actionnabilité les recommandations sont réalisées en ciblant les contre-exemples les plus actionnables sur les règles les plus rentables notre méthodologie est appliquée sur 12 000 clients et 100 000 produits de vmmatériaux afin orienter les commerciaux sur les possibilités accroissement de la valeur client\n",
      "une nouvelle approche pour extraction non supervisée de critères\n",
      "récemment de nouvelles techniques regroupées sous le vocable de détection automatique opinions opinion mining ont fait leur apparition et proposent une évaluation globale un document ainsi elles ne permettent pas de mettre en avant le fait que les personnes expriment une opinion très positive du scénario un film alors qu'elles trouvent que les acteurs sont médiocres dans cet article nous proposons de caractériser automatiquement les segments de textes relevant un critère donné sur un corpus de critiques\n",
      "une nouvelle approche visuelle pour la classification hiérarchique et topologique\n",
      "nous proposons dans cet article une nouvelle méthode de classification hiérarchique et topologique notre approche consiste à construire de manière auto-organisée une partition de données représentées par un ensemble \"forêt\" arbres répartis sur une grille 2d chaque cellule de la grille est modélisée par un arbre dont les noeuds représentent les données la partition globale obtenue est visualisée à aide une carte de treemap dans laquelle chaque treemap représente un arbre de données nous évaluerons les capacités et les performances de notre approche sur des données aux difficultés variables des résultats numériques et visuels seront présentés et discutés\n",
      "utilisation une ontologie du domaine pour la découverte du contenu de bases de données géographiques\n",
      "essor récent des technologies associées à la géomatique a permis la production rapide de nombreuses données géographiques or pour tirer profit de ces données il convient de pouvoir évaluer leur pertinence et leur complexité vis à vis de application à laquelle on les destine dans cet article nous présentons une application permettant à un utilisateur de découvrir le contenu de bases de données géographiques à savoir quels types entités géographiques sont représentés au sein de chaque base et comment pour accéder à ces informations utilisateur interroge le système via une ontologie globale du domaine qui décrit les types entités topographiques du monde réel des ontologies locales ou application sont utilisées pour formaliser les spécifications de chaque base de données décrite elles sont annotées à aide de concepts issus de ontologie globale ce système est implémenté sous la forme une interface web et inclut un affichage cartographique échantillons de données\n",
      "utilisation de la machine cellulaire pour la détection des courriels indésirables\n",
      "nan\n",
      "utiliser des résultats alignement pour enrichir une ontologie\n",
      "en établissant des relations entre des concepts issus de deux ontologies distinctes les outils alignement peuvent être utilisés pour enrichir une des deux ontologies avec les concepts de l'autre a partir une expérience menée dans le cadre du projet anr geonto 1 dans le domaine de la topographie cet article identifie des traitements complémentaires à alignement pour enrichissement et montre leur mise en oeuvre dans taxomap framework\n",
      "vers la fusion informations hétérogènes et partielles pour aide au codage diagnostique\n",
      "nan\n",
      "visualisation de intra et inter structure des groupes en classification non supervisée\n",
      "la croissance exponentielle des données engendre des volumétries de bases de données très importantes une solution couramment envisagée est utilisation une description condensée des propriétés et de la structure des données de ce fait il devient crucial de disposer outils de visualisation capables de représenter la structure des données non pas à partir des données elles mêmes mais à partir de ces descriptions condensées nous proposons une méthode de description des données à partir de prototypes enrichis puis segmentés à aide un algorithme adapté de classification non supervisée nous introduisons ensuite un procédé de visualisation capable de mettre en valeur la structure intra et inter-groupes des données\n",
      "abstop-k &#945; un algorithme extraction de paires abstraites hautement corrélées pour mieux recommander dans la ”longue traine\n",
      "de nombreux systèmes de recommandation se focalisent sur les articles(que nous appellerons ”items” les plus ”populaires” et ignorent souventla ”longue traîne” des produits qui le sont moins nous proposons algorithmeabstop-ka qui améliore les recommandations en se basant sur la combinaison(pondérée par a de paires hautement corrélées entre des abstractions items etentre des paires items concrets classiquement recherchées\n",
      "action rules and meta-actions\n",
      "nan\n",
      "affichage de publicités sur des portails web\n",
      "nous nous intéressons au problème de affichage de publicités sur le web de plus en plus annonceurs souhaitent maintenant payer uniquement lorsque quelqu'un clique sur leurs publicités dans ce modèle opérateur du portail a intérêt à identifier les publicités les plus cliquées selon ses catégories de visiteurs comme les probabilités de clic sont inconnues a priori il s'agit un dilemme exploration/exploitation ce problème a souvent été traité en ne tenant pas compte de contraintes provenant du monde réel  les campagnes de publicités ont une durée de vie et possèdent un nombre de clics à assurer et ne pas dépasser pour cela nous introduisons une approche hybride mab+lp) entre la programmation linéaire et les bandits nos algorithmes sont testés sur des modèles crées avec un important acteur du web commercial ces expériences montrent que ces approches atteignent une performance très proche de optimum et mettent en évidence des aspects clés du problème\n",
      "agrégation de systèmes de fermetures cas des hiérarchies topologies et géométries convexes\n",
      "nan\n",
      "aide à la décision pour la maintenance ferroviaire préventive\n",
      "la maintenance de trains est un problème particulièrement délicat lié à de nombreux enjeux à la fois financiers sécuritaires et énergétiques nous nous intéressons à la mise en place une maintenance préventive basée sur la détection et la correction de tout comportement anormal susceptible de provoquer un problème majeur dans un futur proche nous proposons ainsi un outil aide à la décision afin de i) dégager des connaissances utiles sur historique des trains et ii) détecter et étudier les anomalies comportementales dans le but de prendre des décisions optimales en termes de maintenance ferroviaire\n",
      "allier csps et motifs locaux pour la découverte de motifs sous contraintes n-aires\n",
      "dans cet article nous étudions la relation entre la découverte de motifs sous contraintes et les csps constraint satisfaction problems afin de définir des contraintes de plus haut niveau qui sont précieuses pour mener à bien des tâches de fouille de données pour cela nous proposons une approche de modélisation et extraction de motifs sous contraintes n-aires exploitant les motifs locaux utilisateur définit un ensemble de contraintes n-aires et un solveur de csp génère ensemble des solutions notre approche profite des progrès récents sur extraction de motifs locaux et permet de modéliser de manière concise et élégante tout ensemble de contraintes combinant plusieurs motifs locaux permettant ainsi la découverte de motifs répondant mieux aux buts finaux de utilisateur. les expériences menées montrent la faisabilité de notre approche\n",
      "analyse de documents pédagogiques en vue de leur annotation\n",
      "utilisation des documents pédagogiques disponibles sur le web devient de plus en plus large tant pour enseignant qui a besoin de préparer son support de cours que pour étudiant qui désire par exemple s'autoformer la description un document pédagogique en alimentant par des métadonnées s'avère une solution qui confère une valeur ajoutée au document afin expliciter des informations placées dans ce document dans cette optique nous proposons une méthode annotation de documents pédagogiqes selon différents points de vue qui est basée sur analyse sémantique des éléments discursifs du texte\n",
      "analyse de séquences événements avec traminer\n",
      "nan\n",
      "analyse en ligne objets complexes avec analyse factorielle\n",
      "les entrepôts de données et analyse en ligne olap on-line analysis processing présentent des solutions reconnues et efficaces pour le processus aide à la décision notamment analyse en ligne grâce aux opérateurs olap permet de naviguer et de visualiser des données représentées dans un cube mutlidimensionnel mais lorsque les données ou les objets à analyser sont complexes il est nécessaire de redéfinir et enrichir ces opérateurs olap dans cet article nous proposons de combiner analyse olap et la fouille de données data mining afin de créer un nouvel opérateur de visualisation objets complexes cet opérateur utilise analyse factorielle des correspondances\n",
      "analyse globale du flux optique pour la détection évènements dans une scène de foule\n",
      "les systèmes de vidéo-surveillance sont de plus en plus autonomes dans la détection des événements anormaux cet article présente une méthode de détection des flux majeurs et des évènements qui surviennent dans une scène de foule ces détections sont effectuées en utilisant un modèle directionnel construit à partir un mélange de lois de von mises appliqué à orientation des vecteurs de mouvement les flux majeurs sont alors calculés en récupérant les orientations les plus importantes des mélanges divers évènements se produisant dans une foule sont aussi détectés en utilisant en plus du modèle d'orientation un modèle probabiliste de magnitude des vecteurs de mouvement les résultats de expérimentation sur un échantillon de vidéos événements sont présentés\n",
      "analyse incrémentale des usages pour le routage des requêtes dans les systèmes pairs à pairs\n",
      "nan\n",
      "applying markov logic to document annotation and citation deduplication\n",
      "structured learning approaches are able to take into account the relational structure of data thus promising an enhancement over non-relational approaches in this paper we explore two document-related tasks in relational domains setting the annotation of semi-structured documents and the citation deduplication for both tasks we report results of comparing relational learning approach namely markov logic to non-relational one namely support vector machines (svm) we discover that increased complexity due to the relational setting is difficult to manage in large scale cases where non-relational models might perform better moreover our experiments show that in markov logic the contribution of its probabilistic component decreases in large scale domains and it tends to act like first-order logic (fol)\n",
      "apport de la technique de fouille de données spatiales dans la prédiction des risques engendrés par les changements climatiques\n",
      "nan\n",
      "apprentissage de patrons lexico-syntaxiques à partir de textes\n",
      "ce papier présente une approche apprentissage de patrons lexico-syntaxiques à partir de textes annotés les patrons lexico-syntaxiques sont utilisés pour identifier des relations lexicales dans les corpus textuels leur construction manuelle est une tâche fastidieuse et des solutions permettant apprentissage sont souhaitables nous proposons une approche apprentissage qui repose sur utilisation des chemins de dépendance pour représenter les patrons et implémentation un algorithme de classification approche a été appliquée dans le domaine biomédical pour identifier des patrons lexico-syntaxiques exprimant des relations fonctionnelles\n",
      "apprentissage de spécifications de csp\n",
      "nan\n",
      "apprentissage supervisé adaptatif de concepts formels à partir des données nominales\n",
      "nan\n",
      "approche biomimétique coopérative pour la visualisation de grands graphes multidimensionels\n",
      "face à la quantité sans cesse grandissante de données stockées les algorithmes de fouille et de visualisation de données doivent pouvoir être capable de traiter de grandes quantités de données une des solutions est effectuer un prétraitement des données permettant la réduction de la dimension des données sans perte significative d'informations idée est donc de réduire ensemble de descripteurs avant de faire appel à la méthode de visualisation sous forme un graphe\n",
      "approche complexe de analyse de documents anciens\n",
      "cet article présente une méthode complexe pour la caractérisation et indexation images graphiques de documents anciens a partir un bref état de l'art une méthode pour décrire ces images en tenant compte de leur complexité est proposée trois étapes principales de ce traitement sont détaillées dont une méthode novatrice d'analyse de segmentation et de description des traits les résultats sont issus de travaux en cours et sont encourageants\n",
      "approche sémantique pour la préservation de la vie privée dans les médias sociaux\n",
      "nan\n",
      "bien cube les données textuelles peuvent s'agréger \n",
      "la masse des données aujourd'hui disponibles engendre des besoins croissants de méthodes décisionnelles adaptées aux données traitées ainsi récemment de nouvelles approches fondées sur des cubes de textes sont apparues pour pouvoir analyser et extraire de la connaissance à partir de documents originalité de ces cubes est étendre les approches traditionnelles des entrepôts et des technologies olap à des contenus textuels dans cet article nous nous intéressons à deux nouvelles fonctions d'agrégation la première propose une nouvelle mesure de tf-idf adaptative permettant de tenir compte des hiérarchies associées aux dimensions la seconde est une agrégation dynamique permettant de faire émerger des groupements correspondant à une situation réelle les expériences menées sur des données issues du serveur hal une univsersité confirment intérêt de nos propositions\n",
      "caractériser la terminologie des usagers de santé dans le domaine du cancer du sein\n",
      "internet est devenu une source importante informations médicales pour les patients et leurs proches  recherche informations sur leurs maladies et les dernières recherches cliniques ainsi que pour y constituer des communautés “numériques” de dialogue et de partage cependant accès à internet ne signifie pas nécessairement accès à information le manque de familiarité avec le langage médical constitue un problème majeur pour les usagers de santé dans accès à information et son interprétation ce papier s'inscrit dans la problématique étude et de caractérisation de la terminologie des usagers de santé pour pouvoir proposer des services adaptés à leur langage et à leur niveau de connaissances le travail réalisé est une ontologie dans le domaine du cancer du sein orientée vers les usagers de santé cette ontologie est construite à partir un ensemble de corpus de textes représentant deux catégories  les médiateurs et les usagers de santé les éléments de cette ontologie ont été analysés en utilisant des méthodes quantitatives et qualitatives sur plusieurs niveaux  termes concepts et relations\n",
      "cartocel  un outil de cartographie des connaissances guidée par la machine cellulaire casi\n",
      "nous présentons dans ce papier outil cartocel cartographies cellulaires permettant une visualisation automatique et dynamique des domaines de connaissances le fonctionnement de cartocel est basé sur une approche originale de modélisation booléenne de la cartographie des domaines de connaissances métiers/stratégiques inspirée du principe de la machine cellulaire casi cellular automata for symbolic induction) le but après une modélisation booléenne de la cartographie des domaines de connaissances est double  une part affiner la cartographie par une fouille de données orchestrée par casi et autre part réduire la complexité de stockage ainsi que le temps de calcul\n",
      "chorml  résumés visuels de bases des données géographiques\n",
      "nan\n",
      "classer discriminer et visualiser des séquences événements\n",
      "cet article 1 présente un ensemble outils destiné à analyser des séquences événements en sciences sociales et à visualiser les résultats obtenus nous commençons par formaliser la notion de séquence événements avant de définir une mesure de dissimilarité entre ces séquences afin de construire des typologies et de tester les liens entre ces séquences et autres variables d'intérêts initialement définie par moen (2000) cette mesure se base sur la notion de distance édition entre séquences et permet identifier les différences ordonnancement et de temporalité des événements nous proposons une extension de celle-ci afin de pouvoir prendre en compte la simultanéité des événements ainsi qu'une méthode de normalisation qui garantit le respect de inégalité triangulaire dans un deuxième temps nous présentons un ensemble outils destinés à interpréter les résultats nous proposons ainsi deux méthodes de visualisation un ensemble de séquences et nous introduisons la notion de sous-séquence discriminante qui permet identifier les différences ordonnancement des événements les plus significatives entre groupes ensemble des outils présentés est disponible au sein de la librairie r traminer\n",
      "classification de documents  calcul une distance structurelle\n",
      "la classification des documents numériques garantit un accès rapide et ciblé à l'information si nous considérons qu'un document est représenté par sa ou ses structures définir des classes de documents revient à définir des classes de structures une classe structurelle représente donc des structures « proches » ainsi associer la structure un document à sa classe structurelle revient à calculer une distance dite « structurelle » elle tiendra compte à la fois de organisation des éléments position des noeuds chemin) du coût adaptation des représentants des classes ainsi que de la représentativité des sous-graphes sur un corpus de documents représentant des notices de livres issus de la bibliothèque de l'université nous discuterons de la construction de cette distance de intérêt de chacun des trois paramètres utilisés\n",
      "classification et selection de caracteristique basees sur les concepts semantiques pour la recherche information multimedia\n",
      "le besoin récent de nombreuses applications multimédia basées sur le contenu a engendré une demande croissante de technologies dans le domaine de la recherche information multimédia basée sur état de art des techniques existantes nous proposons dans cet article une approche de recherche information multimédia qui prend en compte les informations de scène et exploite un modèle de sélection de caractéristiques les principaux avantages de notre modèle de recherche par rapport aux modèles existants sont  i) une méthode de classification basée sur des catégories de concept sémantique ii) un modèle de recherche par rapport aux modèles existants sont  i) une méthode de classification basée sur des catégories de concept sémantique ii) un modèle de sélection de caractéristiques iii) un index multidimensionnel notre framework propose un bon compromis entre précision et rapidité de la recherche\n",
      "classification supervisée pour de grands nombres de classes à prédire  une approche par co-partitionnement des variables explicatives et à expliquer\n",
      "dans la phase de préparation des données du data mining les méthodes de discrétisation et de groupement de valeurs supervisé possèdent de nombreuses applications  interprétation estimation de densité conditionnelle sélection de type filtre des variables recodage des variables en amont des classifieurs ces méthodes supposent habituellement un faible nombre de valeur à expliquer (classes) typiquement moins une dizaine et trouvent leur limite quand leur nombre augmente dans cet article nous introduisons une extension des méthodes de discrétisation et groupement de valeurs consistant à partitionner une part la variable explicative autre part la variable à expliquer le meilleur co-partitionnement est recherché au moyen une approche bayesienne de la sélection de modèle nous présentons ensuite comment utiliser cette méthode de prétraitement en préparation pour le classifieur bayesien naïf des expérimentations intensives démontrent apport de la méthode dans le cas de centaines de classes\n",
      "cnd-cube  nouvelle représentation concise sans perte information un cube de données\n",
      "le calcul des cubes de données est excessivement coûteux aussi bien en temps exécution qu'en mémoire et son stockage sur disque peut s'avérer prohibitif plusieurs efforts ont été consacrés à ce problème à travers les cubes fermés où les cellules préservant la sémantique agrégation sont réduites à une cellule sans perte d'information dans cet article nous introduisons le concept du cube de données non-dérivable fermé nommé cnd-cube qui généralise la notion des modèles non-dérivables fermés fréquents bidimensionnels à un contexte multidimensionnel nous proposons un nouvel algorithme pour extraire le cnd-cube à partir des bases de données multidimensionnelles en se basant sur trois contraintes anti-monotones à savoir “être fréquent” “être non dérivable” et “être un générateur minimal” les expériences montrent que notre proposition fournit la représentation la plus concise un cube de données et elle est ainsi la plus efficace pour réduire espace de stockage\n",
      "codage et classification non supervisée un corpus maya  extraire des contextes pour situer inconnu par rapport au connu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "écriture logo syllabique des anciens mayas comprend plus de 500 signes et est en bonne partie déchiffrée avec des degrés de certitude divers nous avons appliqué au codex de dresde un des trois seuls manuscrits qui nous soient parvenus codé sous latex avec le système mayatex notre méthode de représentation graduée par apprentissage non supervisé hybride entre clustering et analyse factorielle oblique sous la métrique de hellinger afin obtenir une image nuancée des thèmes traités  les individus statistiques sont les 212 segments de folio du codex et leurs attributs sont les 1687 bigrammes de signes extraits pour comparaison nous avons introduit dans cette approche endogène un élément exogène la décomposition en éléments des signes composites pour préciser plus finement les contenus la rétro-visualisation dans le texte original des résultats et expressions dégagées éclaire la signification de certains glyphes peu compris en les situant dans des contextes clairement interprétables\n",
      "combiner approche logique et numérique pour la réconciliation de données et alignement ontologies\n",
      "nan\n",
      "combinerweb 2.0 et web sémantique pour réduire les disparités expertise au sein de blogs entreprise\n",
      "avec avènement applications sociales en entreprise (blogs wikis etc.) il est fréquent que des individus aux niveaux expertise relativement distants se réunissent au sein de communautés en ligne ces disparités expertise se traduisent entre autres par des comportements différents dans la manière de tagguer les contenus créés notamment en ce qui concerne les termes utilisés rendant ainsi complexe la découverte informations pourtant publiées dans cet article nous mettons en avant la possibilité offerte par les technologies du web sémantique combinées avec les paradigmes du web social de résoudre cette problématique nous proposons ainsi une chaine de traitement combinant ontologies wikis sémantiques et indexation de contenus permettant la production de graphes sémantiques interconnectés et facilitant de cette manière la découverte de contenus créés au sein de tels systèmes\n",
      "comparaison de critères de pureté pour intégration de connaissances en clustering semi-supervisé\n",
      "utilisation de connaissances pour améliorer les processus de fouille de données a mobilisé un important effort de recherche ces dernières années il est cependant souvent difficile de formaliser ce type de connaissances comme celles-ci sont souvent dépendantes du domaine dans cet article nous nous intéressons à intégration de connaissances sous la forme objets étiquetés dans les algorithmes de clustering plusieurs critères permettant évaluer la pureté des clusters sont présentés et leur comportement est comparé sur des jeux de données artificiels les avantages et les inconvénients de chaque critère sont analysés pour aider utilisateur à faire un choix\n",
      "comparaisons structurelles de grandes bases de données par apprentissage non-supervisé\n",
      "dans le domaine de la fouille de données mesurer les similitudes entre différents sous-ensembles est une question importante qui a été peu étudiée jusqu'à présent dans cet article nous proposons une nouvelle méthode basée sur apprentissage non-supervisé les différents sous-ensembles à comparer sont caractérisés au moyen un modèle à base de prototypes ensuite les différences entre les modèles sont détectées en utilisant une mesure de similarité\n",
      "composition de servicesweb basée sur les réseau sociaux\n",
      "nous proposons dans cet article une première approche qui consiste à exploiter les réseaux sociaux afin de faciliter la composition de services par les utilisateurs finaux nous introduisons un framework nommé social composer (soco) qui implémente cette approche soco fournit à utilisateur des recommandations dynamiques de services basées entre autre sur le réseau social de utilisateur qui est construit implicitement à partir des interactions entre les utilisateurs les services les différentes compositions opérées par les membres du réseau social ainsi que le réseau social global\n",
      "construction de noyaux pour apprentissage supervisé à partir arbres aléatoires\n",
      "nous montrons qu'un ensemble arbres de décision avec une composante aléatoire permet de construire un noyau efficace destiné à apprentissage supervisé nous étudions théoriquement les propriétés un tel noyau et montrons que sous des conditions très souvent rencontrées en pratique il existe une séparabilité linéaire entre exemples de classes distinctes dans espace induit par celui-ci parallèlement nous observons également que le classique vote à la majorité un ensemble arbres est un hyperplan sans garantie d'optimalité dans espace induit par le noyau enfin comme le montrent nos expérimentations utilisation conjointe un ensemble arbres et un séparateur à vaste marge svm) aboutit à des résultats extrêmement encourageants\n",
      "cubes fermés / quotients émergents\n",
      "le concept de cube émergent a été introduit afin de comparer deux data cubes dans cet article nous introduisons deux nouvelles représentations réduites du cube émergent sans perte des mesures  le cube fermé émergent et le cube quotient émergent la première représentation est basée sur le concept de fermeture cubique c'est la plus petite représentation possible du cube de données émergent à partir du cube fermé émergent et donc en stockant le minimum d'informations il est possible de répondre efficacement aux requêtes qui peuvent être exécutées sur le cube émergent lui-même la seconde représentation s'appuie sur la structure du cube quotient qui a été proposé pour résumer un cube de données le cube quotient est revisité afin de le doter une sémantique basée sur la fermeture cubique et donc adapté au contexte du cube émergent le cube quotient émergent résultant est moins réduit que le cube fermé émergent mais il préserve la propriété de \" spécialisation/généralisation \" du data cube qui permet la navigation au sein du cube émergent nous établissons également le lien entre les deux représentations introduites et celle basée sur les bordures classiques en fouille de données des expérimentations effectuées sur divers jeux de données visent à comparer la taille des différentes représentations\n",
      "dafoe une plateforme pour construire des ontologies à partir de textes et de thésaurus\n",
      "nan\n",
      "découverte itemsets fréquents fermés sur architectures multicoeurs\n",
      "dans ce papier nous proposons plcm un algorithme parallèle de découverte itemsets fréquents fermés basé sur algorithme lcm reconnu comme algorithme séquentiel le plus efficace pour cette tâche nous présentons aussi une interface de parallélisme à la fois simple et puissante basée sur la notion de tuple space qui permet avoir une bonne répartition dynamique du travail grâce à une étude expérimentale détaillée nous montrons que plcm est le seul algorithme qui soit suffisamment générique pour calculer efficacement des itemsets fréquents fermés à la fois sur des bases creuses et sur des bases denses améliorant ainsi état de l'art\n",
      "découverte des dépendances fonctionnelles conditionnelles fréquentes\n",
      "les dépendances fonctionnelles conditionnelles dfc) ont été introduites en 2007 pour le nettoyage des données elles peuvent être considérées comme une unification de dépendances fonctionnelles df) classiques et de règles association ra) puisqu'elles permettent de spécifier des dépendances mixant des attributs et des couples de la forme attribut/valeur dans cet article nous traitons le problème de la découverte des dfc ie déterminer une couverture de ensemble des dfc satisfaites par une relation r nous montrons comment une technique connue pour la découverte des df exactes et approximatives peut être étendue aux dfc cette technique a été implémentée et des expériences ont été menées pour montrer la faisabilité et le passage à échelle de notre proposition\n",
      "density estimation on data streams  an application to change detection\n",
      "in recent years the amount of data to process has increased in many application areas such as network monitoring web click and sensor data analysis data stream mining answers to the challenge of massive data processing this paradigm allows for treating pieces of data on the fly and overcoming data storage the detection of changes in a data stream distribution is an important issue this article proposes a new schema of change detection  i the summarization of the input data stream by a set of micro-clusters ii the estimate of the data stream distribution exploiting micro-clusters iii the estimate of the divergence between the current estimated distribution and a reference distribution iv diagnostic step through the contribution of each predictive variable to the overall divergence between both distributions our schema of change detection is applied and evaluated on artificial data streams\n",
      "detecting anomalies in data streams using statecharts\n",
      "the environment around us is progressively equipped with various sensors producing data continuously the applications using these data face many challenges such as data stream integration over an attribute such as time and knowledge extraction from raw data in this paper we propose one approach to face those two challenges first datastreams integration is performed using state charts which represents a resume of data produced by the corresponding data producer second we detect anomalous events over temporal relations among state charts.we describe our approach in a demonstration scenario that is using a visual tool called patternator\n",
      "détection des mouvements anormaux dans des vidéos\n",
      "nan\n",
      "développement de méthodes de classification basées sur analyse de concepts formels sous la plateforme weka\n",
      "nan\n",
      "differentes variantes gmm-smos pour identification du locuteur\n",
      "dans cet article nous présentons différentes variantes gmm-smos pour identification du locuteur en mode indépendant du texte pour mettre en oeuvre les différents systèmes nous avons opté une représentation multi-gaussienne de espace des caractéristiques basées sur algorithme expectation maximisation (em) ces nouvelles représentations constituent les vecteurs entrés pour entraîner les supports vecteurs machines svms) par algorithme de type optimisation par minimisation séquentielle (smo)\n",
      "etude comparative des langages de requêtes sémantiques pour extraction des liens complexes dans une base de connaissances\n",
      "nan\n",
      "etude de stabilité de méthodes de sélection de motifs à partir des séquences protéiques\n",
      "nan\n",
      "expansion de requêtes sql par une ontologie de domaine\n",
      "cet article traite un problème dans le domaine de la gestion des bases de données classiques il s'agit exploiter une ontologie de domaine pour aider utilisateur une base de données relationnelle dans sa recherche et de lui permettre une interrogation transparente de la base de données pour cela nous proposons une approche expansion automatique de requêtes sql lorsque celles-ci n'ont pas de réponses notre approche est décrite par un algorithme défini de manière générique afin être utilisé pour une base de données quelconque\n",
      "explication de décisions de réconciliation  approche fondée sur les réseaux de petri colorés\n",
      "objectif des systèmes intégration de données est de faciliter exploitation et interprétation informations hétérogènes provenant de différentes sources lorsque on doit intégrer de grands volumes de données le recours à un expert n'est pas envisageable mais exploitation de processus intégration automatiques peut introduire des approximations ou des erreurs nous nous focalisons sur les résultats fournis par les méthodes de réconciliation de données ces dernières comparent les données entre elles et détectent celles qui réfèrent à la même entité du monde réel pour renforcer la confiance des utilisateurs dans les résultats retournés par ces méthodes nous proposons dans cet article une approche explication graphique fondée sur les réseaux de petri colorés qui est particulièrement adaptée aux approches de réconciliation globales numériques et guidées par une ontologie\n",
      "exploration de dépendances fonctionnelles et de règles association avec olap\n",
      "nan\n",
      "extraction itemsets distinctifs dans les flux de données\n",
      "extraction itemsets distinctifs est un sujet de recherche récent qui connait plusieurs algorithmes pour les données statiques knobbe et ho 2006 heikinheimo et al. 2007) ces solutions ne sont toutefois pas conçues pour le cas des flux de données pour lesquels les temps de réponse doivent être aussi faibles que possible nous considérons le problème de extraction itemsets distinctifs dans les flux qui peut avoir de nombreuses applications dans la sélection de variables la classification ou encore la recherche d'information nous proposons heuristique idkf itemsets distinctifs dans les flux et des résultats expérimentations en comparaison une technique de la littérature\n",
      "extraction de la région intérêt une personne sur un obstacle\n",
      "nan\n",
      "extraction de motifs graduels clos\n",
      "la découverte automatique de règles et motifs graduels “plus âge une personne est élevé plus son salaire est élevé” trouve de très nombreuses applications sur des bases de données réelles (eg biologie flots de données de capteurs) si des algorithmes de plus en plus efficaces sont proposés dans des articles récents il n'en reste pas moins que ces méthodes génèrent un nombre de motifs tellement important que les experts peinent à les exploiter dans cet article nous proposons donc une représentation condensée des motifs graduels en introduisant les concepts théoriques associés aux opérateurs de fermeture sur de tels motifs\n",
      "extraction de règles association séquentielle à aide de modèles semi-paramétriques à risques proportionnels\n",
      "la recherche de liens entre objets fréquents a été popularisée par les méthodes extraction de règles association dans le cas de séquences événements les méthodes de fouille permettent extraire des sous-séquences qui peuvent ensuite être exprimées sous la forme de règles association séquentielle entre événements cette utilisation de la fouille de séquences pour la recherche de liens entre des événements pose deux problèmes premièrement le critère principal utilisé pour sélectionner les sous-séquences événements est la fréquence or les occurrences de certains événements peuvent être fortement liées entre elles même lorsqu'elles sont peu fréquentes deuxièmement les mesures actuelles utilisées pour caractériser les règles association ne tiennent pas compte du caractère temporel des données comme importance du timing des événements ou le problème des données censurées dans cet article nous proposons une méthode pour rechercher des liens significatifs entre des événements à aide de modèles de durée les règles association sont construites à partir des motifs séquentiels observés dans un ensemble de séquences influence sur le risque que événement « conclusion » se produise après le ou les événements « prémisse » est estimée à aide un modèle semi-paramétrique à risques proportionnels outre la présentation de la méthode article propose une comparaison avec autres mesures association\n",
      "extraction des séquences fermées fréquentes à partir de corpus parallèles  application à la traduction automatique\n",
      "dans cet article nous abordons la problématique extraction de séquences fréquentes à partir de corpus de textes parallèles en prenant en compte ordre apparition des mots dans une phrase notre finalité est exploiter ces séquences dans la traduction automatique (ta) nous introduisons ainsi la notion de règles associatives inter-langues rail) et nous définissons notre modèle de traduction à base de ces associations nous décrivons également les différentes expérimentations conduites sur le corpus europarl afin de construire à partir des rail une table de traduction bilingue qui est intégrée par la suite dans un processus complet de ta\n",
      "fouille visuelle de données en 3d et réalité virtuelle  état de art\n",
      "la fouille visuelle de données ou visual data mining vdm a pour objectif de faciliter interprétation des résultats issus une fouille de données grâce à usage de représentations graphiques au cours de la dernière décennie un grand nombre de techniques de visualisation information ont été mises au point permettant la visualisation de données multidimensionnelles dans des environnements virtuels lors des travaux antérieurs les chercheurs ont proposé des taxonomies pour classer les techniques de vdm chi (2000) herman et al(2000)) toutefois ces taxonomies ne prennent en compte que partiellement les techniques récentes relatives à utilisation de la 3d et de la réalité virtuelle le but de cet article est de faire un état de art récent et spécifique à ces techniques celles-ci sont détaillées classées et comparées selon différents critères  les applications encodage graphique les techniques d'interaction les avantages et les inconvénients de chaque approche ces techniques sont présentées dans des tableaux accompagnées illustrations graphiques\n",
      "gestion sémantique des droits accès au contenu ontologie amo\n",
      "dans cet article nous proposons une approche de la gestion des droits accès pour les systèmes de gestion de contenu qui reposent sur les modèles et techniques du web sémantique nous présentons ontologie amo qui consiste 1) en un ensemble de classes et propriétés permettant annoter les ressources dont il s'agit de contrôler accès et 2) en une base de règles inférence modélisant la stratégie de gestion des droits à mettre en oeuvre appliquées sur la base annotations des ressources ces règles permettent de gérer les ressources selon une stratégie donnée cette modélisation garantit ainsi adaptabilité de ontologie à différentes stratégies de gestion des droits accès. nous illustrons utilisation de ontologie amo sur les documents du projet anr isicil produits par le wiki sémantique sweetwiki nous montrons comment les documents sont annotés avec amo quelles règles sont mises en oeuvre et quelles requêtes permettent le contrôle de accès aux documents\n",
      "identifying the presence of communities in complex networks through topological decomposition and component densities\n",
      "the exponential growth of data in various fields such as social networks and internet has stimulated lots of activity in the field of network analysis and data mining identifying communities remains a fundamental technique to explore and organize these networks few metrics are widely used to discover the presence of communities in a network we argue that these metrics do not truly reflect the presence of communities by presenting counter examples this is because these metrics concentrate on local cohesiveness among nodes where the goal is to judge whether two nodes belong to the same community or vise versa thus loosing the overall perspective of the presence of communities in the entire network in this paper we propose a new metric to identify the presence of communities in real world networks this metric is based on the topological decomposition of networks taking into account two important ingredients of real world networks the degree distribution and the density of nodes we show the effectiveness of the proposed metric by testing it on various real world data sets\n",
      "incfds un nouvel algorithme inférence incrémentale des dépendances fonctionnelles\n",
      "inférence des dépendances fonctionnelles est une des problématiques les plus étudiées en bases de données elle a fait objet de plusieurs travaux qui ont proposé des algorithmes afin inférer efficacement les dépendances fonctionnelles pour les utiliser dans différents domaines  administration de bases de données ré-ingénierie optimisation des requêtes etc toutefois pour les application réelles les bases de données sont évolutives et les relations sont fréquemment augmentées ou diminuées de tuples par conséquent afin de s'adapter à ce cadre dynamique une solution consiste à appliquer un des algorithmes disponibles dans la littérature pour inférer les dépendances fonctionnelles après chaque mise à jour cette solution étant coûteuse nous proposons dans cet article inférer les dépendances fonctionnelles une manière incrémentale à cet effet nous introduisons un nouvel algorithme appelé incfds et nous évaluons ses performances par rapport à approche classique inférence des dépendances fonctionnelles à partir une relation dynamique\n",
      "indexation et recherche images à très grande échelle avec une afc incrémentale et parallèle sur gpu\n",
      "nous présentons un nouvel algorithme incrémental et parallèle analyse factorielle des correspondances afc) pour la recherche images à grande échelle en utilisant le processeur de la carte graphique (gpu) afc est adaptée à la recherche images par le contenu en utilisant des descripteurs locaux des images (sift) afc permet de réduire le nombre de dimensions et de découvrir des thèmes qui permettent de diminuer le nombre images à parcourir et donc le temps de réponse une requête pour traiter de très grandes bases images, nous présentons une version incrémentale et parallèle d'afc puis nous utilisons ses indicateurs pour construire des fichiers inversés pour retrouver les images contenant les mêmes thèmes que image requête cette étape est elle aussi parallélisée sur gpu pour obtenir des réponses rapides les résultats numériques sur la base de données images nistér-stewénius plongée dans 1 million images de flickr montrent que notre algorithme incrémental et parallèle est très significativement plus rapide que sa version standard\n",
      "indice de complexité pour le tri et la comparaison de séquences catégorielles\n",
      "cet article 1 propose un nouvel indice de la complexité de séquences catégorielles bien que conçu pour des séquences représentant des trajectoires biographiques telles que celles rencontrées dans les sciences sociales il s'applique à tous types de listes ordonnées d'états indice prend en compte deux aspects distincts soit la complexité induite par ordonnancement des états successifs qui est mesurée par le nombre de transitions changements d'état et la complexité liée à la distribution des états dont rend compte entropie\n",
      "inférence bayesienne du maximum entropie pour le diagnostic du cancer\n",
      "nan\n",
      "intégration de connaissances a priori dans le principe du maximum entropie\n",
      "cet article montre que si on dispose une connaissance a priori sur le problème en main intégration de cette dernière dans le processus apprentissageune machine intelligente pour des tâches de classification peut améliorer la performance de cette machine nous étudions effet de intégration de la connaissance a priori de convexité sur le processus apprentissage du principe du maximum entropie maxent) en utilisant des exemples virtuels nous testons les idées proposées sur un problème benchmark bien connu dans la littérature des machines apprentissage, le problème de formes ondes de breiman nous avons abouti à un taux erreur de généralisation de 15.57% qui est très proche du taux erreur théorique estimé par breiman (14%)\n",
      "intégration interactive de contraintes pour la réduction de dimensions et la visualisation\n",
      "il existe aujourd'hui de nombreuses méthodes de réduction de dimensions que ce soit dans un cadre supervisé ou non-supervisé un des intérêts de ces méthodes est de pouvoir visualiser les données avec pour objectif que les objets qui apparaissent \"visuellement\" proches soient similaires dans un sens qui correspond aux connaissances un expert du domaine ou qui soit conforme aux informations de supervision nous nous plaçons ici dans un contexte semi-supervisé où des connaissances sont ajoutées de façon interactive  ces informations seront apportées sous forme de contraintes exprimant les écarts entre la représentation observée et les connaissances un expert nous pourrons par exemple spécifier que deux objets proches dans espace observation sont en fait peu similaires ou inversement la méthode utilisée ici dérive de analyse en composantes principales (acp) à laquelle nous proposons intégrer deux types de contraintes nous présentons une méthode de résolution qui a été implémentée dans un logiciel offrant une représentation 3d des données et grâce auquel utilisateur peut ajouter des contraintes de manière interactive puis visualiser les modifications induites par ces contraintes deux types expérimentation sont présentés reposant respectivement sur un jeu de données synthétique et sur des jeux standards  ces tests montrent qu'une représentation de bonne qualité peut être obtenue avec un nombre limité de contraintes ajoutées\n",
      "interrogation des résumés de flux de données\n",
      "les systèmes de gestion de flux de données sgfd) ont été conçus afin de traiter une masse importante de données produites en ligne de façon continue etant donné que les ressources matérielles ne permettent pas de conserver toute cette volumétrie seule la partie récente du flux est mémorisée dans la mémoire du sgfd ainsi les requêtes évaluées par ces systèmes ne peuvent porter que sur les données les plus récentes du flux par conséquent les sgfd actuels ne peuvent pas traiter des requêtes qui portent sur des périodes très longues nous proposons dans cet article une approche permettant évaluer des requêtes qui portent sur une période plus longue que la mémoire du sgfd ces fenêtres font appels à des données récentes et des données historisées nous présentons le niveau logique de cette approche ainsi que son implantation sous le sgfd esper une technique échantillonnage associée à une technique de fenêtre point de repère est appliquée pour conserver une représentation compacte des données du flux\n",
      "k-words lab  un outil analyse des mots clés permettant explorer les dynamiques un domaine scientifique\n",
      "nan\n",
      "kgram une machine abstraite de graphes de connaissance\n",
      "cet article présente la machine abstraite de graphes de connaissance kgram qui unifie les notions homomorphisme de graphe et de calcul de requêtes telles que celles du langage sparql sur des données rdf kgram implémente un ensemble extensible expressions qui définissent une famille de langages abstraits interrogation de graphes graal nous décrivons la démantique dynamique de graal en sémantique naturelle et nous présentons la machine abstraite kgram conçue comme interprète de graal qui implémente les règles de sémantique naturelle du langage\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le conflit dans la théorie des fonctions de croyance\n",
      "le conflit apparaît naturellement lorsque plusieurs sources informations imparfaites sont en jeu la théorie des fonctions de croyance offre un formalisme adapté à la fusion informations dans lequel la considération du conflit est centrale ce travail propose de revenir sur les différentes définitions du conflit dans cette théorie tentant de les synthétiser et de montrer comment supprimer ce conflit ou bien comment en tenir compte lors de la combinaison des informations\n",
      "modèle de langue à base de concepts pour la recherche information\n",
      "la majorité des modèles de langue appliqués à la recherche information repose sur hypothèse indépendance des mots plus précisément ces modèles sont estimés à partir des mots simples apparaissant dans les documents sans considérer les éventuelles relations sémantiques et conceptuelles pour pallier ce problème deux grnades approches ont été explorées  la première intègre des dépendances ordre surfacique entre les mots et la seconde repose sur utilisation des ressources sémantiques pour capturer les dépendances entre les mots le modèle de langue que nous présentons dans cet article s'inscrit dans la seconde approche nous proposons intégrer les dépendances entre les mots en représentant les documents et les requêtes par les concepts\n",
      "modélisation et interrogation de données xml multidimensionnelles\n",
      "xml étant devenu omniprésent et ses techniques de stockage et interrogation de plus en plus efficaces le nombre de cas utilisations de ces technologies augmente tous les jours un sujet prometteur est intégration xml et des entrepôts de données dans laquelle une base de données xml native stocke les données multidimensionnelles et exécute des requêtes olap écrites à aide du langage interrogation xml xquery ce papier explore les questions qui peuvent survenir lors de implémentation un tel entrepôt de données xml\n",
      "mysins  make your semantic information system\n",
      "nan\n",
      "objective novelty of association rules measuring the confidence boost1\n",
      "on sait bien que la confiance des régles association n'est pas vraiement satisfaisant comme mesure d'interêt nous proposons au lieu de la substituer par autres mesures (soit en employant de façon conjointe a autres mesures) évaluer la nouveauté de chaque règle par comparaison de sa confiance par rapport à des règles plus fortes qu'on trouve au même ensemble de données c'est à dire on considère un seuil “relatif” de confiance au lieu du seuil absolut habituel cette idée se précise avec la magnitude du “confidence boost” mesurant increment rélative de confiance près des régles plus fortes nous prouvons que nôtre proposition peut remplacer la “confidence width” et le blocage de régles employés a des publications précedentes\n",
      "osom  un algorithme de construction de cartes topologiques recouvrantes\n",
      "les modèles de classification recouvrante ont montré leur capacité à générer une organisation plus fidèle aux données tout en conservant la simplification attendue par une structuration en classes strictes par ailleurs les modèles neuronaux non-supervisés sont plébiscités lorsqu'il s'agit de visualiser la structure de classes nous proposons dans cette étude étendre les cartes auto-organisatrices traditionnelles aux cartes auto-organisatrices recouvrantes nous montrons que cette nouvelle structure apporte des solutions à certaines problématiques spécifiques en classification recouvrante nombre de classes complexité cohérence des recouvrements) algorithme osom s'inspire de la version recouvrante des nuées synamiques et de approche de kohonen pour générer de telles cartes recouvrantes nous discutons du modèle proposé un point de vue théorique fonction énergie associée complexité ) enfin nous présentons un cadre évaluation générale que nous utilisons pour valider les résultats obtenus sur des données réelles\n",
      "pattern mining the past present and future\n",
      "pattern mining is one of the fundamental techniques in data mining as one increases the complexity of the pattern types from subsets to subsequences subtrees and subgraphs one discovers potentially more informative patterns in this talk i will offer a tour of the past and the present research landscape in this area and i'll conclude with some thoughts on directions for the future\n",
      "pcar  nouvelle approche de génération de règles association cycliques\n",
      "les règles association cycliques vise la découverte de nouvelles relations entre des produits qui varient une façon régulièrement cyclique dans le temps dans ce cadre nous introduisons un nouvel algorithme nommé pcar caractérisé par sa performance et son aspect incrémental étude empirique que nous avons menée montre la robustesse et efficacité de notre algorithme proposé vs ceux de la littérature\n",
      "pgp-mc  extraction parallèle efficace de motifs graduels\n",
      "initialement utilisés pour les systèmes de commande les règles et motifs graduels de la forme “plus une personne est âgée plus son salaire est élevé” trouvent de très nombreuses applications par exemple dans les domaines de la biologie des données en flots (eg issues de réseaux de capteurs) etc très récemment des algorithmes ont été proposés pour extraire automatiquement de tels motifs cependant même si certains entre eux ont permis des gains de performance importants les algorithmes restent coûteux et ne permettent pas de traiter efficacement les bases de données réelles souvent très volumineuses en nombre de lignes et/ou nombre d'attributs) nous proposons donc dans cet article une méthode originale de recherche de ces motifs utilisant le multi-threading pour exploiter au mieux les multiples coeurs présents dans la plupart des ordinateurs et serveurs actuels efficacité de cette approche est validée par une étude expérimentale\n",
      "prédiction de séries temporelles et applications à analyse de séquences vidéo\n",
      "nan\n",
      "pretopolib la librairie java de la prétopologie\n",
      "pretopolib est une librairie java implémentant les concepts de la prétopologie son intérêt réside dans la représentation de structures de données permettant la manipulation des données par des opérations ensemblistes celle-ci offre un cadre de développement algorithmes efficaces pour la fouille de données apprentissage topologique et la modélisation des systèmes complexes\n",
      "proposition opérateurs olap pour un modèle multidimensionnel à base objets complexes\n",
      "nan\n",
      "proposition une méthode de classification associative adaptative\n",
      "la classification associative est une méthode de prédiction à base de règles issue de la fouille de règles association cette méthode est particulièrement intéressante car elle recherche de façon exhaustive les règles association pertinentes qu'elle filtre pour ne garder que les règles association de classe celles admettant pour conséquent une modalité de classe) qui sont utilisées comme classifieur les connaissances produites sont ainsi directement interprétable des études antérieures montrent les inconvénients de cette approche qu'il s'agisse de la génération massive de règles non utilisées ou de la mauvaise prédiction de la classe minoritaire lorsque les classes sont déséquilibrées nous proposons une approche originale du type boosting de règles association de classes qui utilise comme classifieur faible une base de règles significatives construites par un algorithme de génération itemsets fréquents qui se limité à extraction des seules règles de classe significatives et qui prend en compte le déséquilibre des données des comparaisons avec autres méthodes de classification assiciative montrent que notre approche améliore la précision et le rappel\n",
      "protein graph repository\n",
      "protein graph repository pgr) est un outil bioinformatique sur le web permettant obtenir une nouvelle representation de protéines sous la forme de graphes acides aminés une représentation plus simple et plus facile à étudier par les moyens informatiques et statistiques dédiés aux graphes la génération des graphes est faite à partir un parseur appliqué sur des fichiers des protéines pdb extraits de la base protein data bank et en precisant les parametres et la methode a utiliser les graphes generes sont ensuite enregistres dans un entrepot doté de moyens de recherche de filtrage et de telechargement pgr peut etre provisoirement consulte à adresse http://www.enode-edition.com/pgr/ il est spécialement dédié aux recherches intéressées à étude de données protéiques sous la forme de graphes et permettra donc de fournir des échantillons pour des travaux expérimentaux\n",
      "recent advances in partitioning clustering algorithms for interval-valued data\n",
      "nan\n",
      "recherche sémantique sur le web basée sur ontologie modulaire et le raisonnement à base de cas\n",
      "dans ce papier nous présentons une approche de recherche sémantique basée sur les ontologies modulaires et le raisonnement à base de cas (rapc) un cas représente ensemble des requêtes similaires associées à leurs résultats pertinents les ontologies modulaires sont utilisées pour représenter et indexer les cas qui sont construits sur la base des requêtes antérieures et les résultats pertinents sélectionnés par les utilisateurs la similarité à base ontologies est utilisée pour retrouver les cas similaires à la requête utilisateur et pour fournir à celui-ci des propositions de reformulation de requêtes correspondants à son besoin la principale contribution de ce travail réside dans utilisation un mécanisme de rapc et une représentation ontologique à deux fins amélioration de la recherche sémantique et enrichissement ontologies à partir de cas expérimentation de approche proposée montre que la précision et le rappel des résultats se sont nettement améliorés\n",
      "reconnaissance de concepts basée sur apprentissage\n",
      "nan\n",
      "réduction bi-directionnelle images - vers une méthode extraction de caractéristiques multi-niveaux\n",
      "inspiré des performances du cerveau humain à identifier les éléments par la vue le problème de la réduction de la dimension dans le domaine de la perception visuelle consiste à extraire une quantité réduite des caractéristiques un ensemble images afin de les identifier ce papier présente une approche innovante bi-directionnelle extraction de caractéristiques images fondée sur utilisation partielle une méthode spatiotemporelle les expériences numériques appliquées sur 70000 images représentant des chiffres écrits à la main ainsi que sur 698 images illustrant un visage sous différentes postures démontrent efficacité de notre approche à fortement réduire la dimension tout en conservant les relations intelligibles entre les objets des données permettant même obtenir une meilleure classification à partir des versions réduites des images qu'à partir des versions originales\n",
      "reglo  une nouvelle stratégie pour résumer un flux de séries temporelles\n",
      "les flux de séries temporelles sont aujourd'hui produits dans de nombreux domaines comme la finance zhu et shasha (2002)) la surveillance de réseaux borgne et al (2007) airoldi et faloutsos (2004)) la gestion de historique des usages fréquents giannella et al (2003) teng et al (2003)) etc résumer de tels flux est devenu un domaine important qui permet de surveiller et enregistrer des informations fiables sur les séries observées à ce jour la majorité des algorithmes de ce domaine s'est concentrée sur des résumés séparés et indépendants giannella et al (2003) zhu et shasha (2002) chen et al (2002)) en accordant à chaque série le même espace en mémoire toutefois la gestion de cet espace mémoire est un sujet important pour les flux de données et une stratégie accordant la même quantité de mémoire à chaque série n'est pas forcément appropriée dans cet article nous considérons que les séries doivent être en compétition vis à vis de espace mémoire selon leur besoin de précision ainsi nous proposons  1) une stratégie de gestion de espace mémoire optimisée et 2) une nouvelle méthode de résumé des séries temporelles par approximation dans ce but nous observons à la fois erreur globale et les erreurs locales la répartition de la mémoire suit les étapes suivantes  1) recherche de la séquence la mieux représentée et 2) recherche de la partie à compresser en minimisant erreur. nos expérimentations sur des données réelles montrent efficacité et la pertinence de notre approche\n",
      "regrouper les données textuelles et nommer les groupes à aide de classes recouvrantes\n",
      "organiser les données textuelles et en tirer du sens est un défi majeur aujourd'hui ainsi lorsque on souhaite analyser un débat en ligne ou un forum de discussion on voudrait pouvoir rapidement voir quels sont les principaux thèmes abordés et la manière dont la discussion se structure autour d'eux pour cela et parce que un même texte peut être associé à plusieurs thèmes nous proposons une méthode originale pour regrouper les données textuelles en autorisant les chevauchements et pour nommer chaque groupe de manière lisible la contribution principale de cet article est une méthode globale qui permet de réaliser toute la chaîne partant des données textuelles brutes jusqu'à la caractérisation des groupes à un niveau sémantique qui dépasse le simple ensemble de mots\n",
      "requêtes skyline avec prise en compte des préférences utilisateurs pour des données volumineuses\n",
      "appréhender parcourir des données ou des connaissances reste une tâche difficile en particulier lorsque les utilisateurs sont confrontés à de gros volumes de données de nombreux travaux se sont intéressés à extraire des points \"skylines\" comme outil de restitution la prise en compte des préférences a retenu attention des travaux les plus récents mais les solutions existantes restent très consommatrices en terme de stockage informations additionnelles afin obtenir des délais raisonnables de réponse aux requêtes notre proposition ec2sky efficient computation of compromises) se focalise sur deux points  1) comment répondre efficacement à des requêtes de type skyline en présence de préférences utilisateurs malgré de gros volumes de données aussi bien en terme de dimensions que de préférences  2) comment restituer les connaissances les plus pertinentes en soulignant les compromis associés aux préférences spécifiées\n",
      "résumé généraliste de flux de données\n",
      "lorsque le volume des données est trop important pour qu'elles soient stockées dans une base de données ou lorsque leur fréquence de production est élevée les systèmes de gestion de flux de données sgfd) permettent de capturer des flux enregistrements structurés et de les interroger à la volée par des requêtes permanentes exécutées de façon continue) mais les sgfd ne conservent pas historique des flux qui est perdu à jamais cette communication propose une définition formelle de ce que devrait être un résumé généraliste de flux de données la notion de résumé généraliste est liée à la capacité de répondre à des requêtes variées et de réaliser des tâches variées de fouille de données en utilisant le résumé à la place du flux d'origine une revue de plusieurs approches de résumés est ensuite réalisée dans le cadre de cette définition\n",
      "rss merger\n",
      "nan\n",
      "salines  un automate au service de extraction de motifs séquentiels multidimensionnels\n",
      "les entrepôts de données occupent aujourd'hui une place centrale dans le processus décisionnel outre leur consultation une des finalités des entrepôts est de servir de socle aux techniques de fouilles de données malheureusement les approches existantes exploitent peu les particularités des entrepôts (multidimensionnalité hiérarchies et données historiques) parmi ces méthodes extraction de motifs séquentiels multidimensionnels a récemment été étudiée nous montrons dans cet article que ces dernières ne tirent pas pleinement profit des hiérarchies et ne découvrent par conséquent qu'une partie seulement des motifs qualitativement intéressants nous proposons alors une méthode extraction de motifs séquentiels multidimensionnels basée sur un automate et extrayant de nouveaux motifs les différentes expérimentations menées sur des jeux de données synthétiques attestent des bonnes performances de notre proposition\n",
      "sélection par entropie de descripteurs textuels pour la catégorisation de documents\n",
      "nan\n",
      "self-clustering for identification of customer purchase behaviours\n",
      "la segmentation une base client peut avoir différents objectifs et plusieurs segmentation peuvent être utiles pour décrire les clients ou pour s'adapter avec les stratégies commerciales une entreprise dans ce papier nous présentons un schéma expérimental visant à proposer un ensemble de segmentations alternatives ces segmentations sont produites sur des données réelles par la transformation des données initiales la génération et la sélection de différentes segmentations\n",
      "sequencesviewer  comment rendre accessible des motifs séquentiels de gènes trop nombreux\n",
      "les techniques extraction de connaissances appliquées aux gros volumes de données issus de analyse de puces adn permettent de découvrir des connaissances jusqu'alors inconnues or ces techniques produisent de très nombreux résultats difficilement exploitables par les experts nous proposons un outil dédié à accompagnement de ces experts dans appropriation et exploitation de ces résultats cet outil est basé sur trois techniques de visualisation (nuages systèmes solaire et treemap qui permettent aux biologistes appréhender de grandes quantités de motifs séquentiels séquences ordonnées de gènes)\n",
      "siam système indexation des articles médicaux\n",
      "nan\n",
      "simplification de données de vol pour un stockage optimal et une visualisation accélérée\n",
      "le projet records collaboration entre industriels et université a pour objectif de développer une infrastructure de service sécurisée pour assurer le suivi et analyse des conditions utilisation d'aéronefs chaque aéronef est muni de capteurs au cours de chaque mission vol) les données mesurées sont enregistrées localement ces dernières sont par la suite transférées dans une base de données centralisée à des fins d'analyse le problème rencontré est la grande quantité de données ainsi enregistrées ce qui en rend exploitation difficile dans cet article nous proposons des techniques de compression et de simplification de données avec un taux de perte contrôlé nos expérimentations montrent des gains drastiques en volumétrie avec de très faibles pertes d'informations ceci représente une première étape avant appliquer des techniques extraction de connaissances\n",
      "simtole\n",
      "la plateforme simtole est dédiee a evaluation algorithmes alignement ontologies heterogenes et reparties a travers un reseau pair a pair (p2p) cette plateforme permet de simuler un réseau p2p dans lequel chaque pair dispose de sa propre ontologie ainsi que des outils permettant alignement entre ontologie locale et une ontologie stockée sur un pair distant le developpement de cette plateforme s'inscrit dans le cadre de travaux de recherche étudiant impact de la topologie du réseau p2p dans le processus inférence de correspondances sémantiques durant cette démonstration la plateforme simtole est présentée puis testée pour illustrer des scénarii montrant comment affiner le processus alignement ontologies dans un réseau p2p\n",
      "sotree  auto-organisation topologique et hiérarchique des données\n",
      "nous proposons dans cet article introduire une nouvelle approche pour la classification non supervisée hiérarchique notre méthode nommée so-tree consiste à construire une manière autonome et simultanée une partition topologique et hiérarchique des données chaque ”cluster” de la partition est associé à une cellule une grille 2d et est modélisé par un arbre dont chaque noeud représente une donnée nous évaluerons les capacités et les performances de notre approche sur des données aux difficultés variables les résultats préliminaires obtenus sont encourageants et prometteurs pour continuer dans cette direction\n",
      "sous-échantillonnage topographique par apprentissage semi-supervisé\n",
      "plusieurs aspects pourraient influencer les systèmes apprentissage existants un de ces aspects est lié au déséquilibre des classes dans lequel le nombre observations appartenant à une classe dépasse fortement celui des observations dans les autres classes dans ce type de cas assez fréquent le système apprentissage a des difficultés au cours de la phase entraînement liées au déséquilibre inter-classe nous proposons une méthode de sous-échantillonnage adaptatif pour traiter ce type de bases déséquilibrées le processus procède par le sous-échantillonnage des données majoritaires guidé par les données minoritaires tout au long de la phase un apprentissage semi-supervisée nous utilisons comme modèle apprentissage les cartes auto-organisatrices approche proposée a été validée sur plusieurs bases de données en utilisant les arbres de décision comme classificateur avec une validation croisée les résultats expérimentaux ont montré des performances très prometteuses\n",
      "suivi automobiles par classification hiérarchique ascendante\n",
      "nan\n",
      "système extraction des connaissances à partir des données temporelles basé sur les réseaux bayésiens dynamiques\n",
      "un grand nombre informations qui ont une structure complexe proviennent de diverses sources ces informations contiennent des connaissances très utiles pour aide à la décision extraction des connaissances à partir des données (ecd) permet acquérir des informations pertinentes pour les systèmes interactifs aide à la décision (siad) mais dans plusieurs domaines les données évoluent une manière dynamique et finissent par dépendre de plusieurs dimensions les réseaux bayésiens dynamiques rbd) sont des modèles représentant des connaissances incertaines sur des phénomènes complexes de processus dynamiques notre objectif revient à fixer les meilleures modèles de connaissances extraites par les rbd et à les utiliser pour la prise de décision dynamique ainsi nous proposons dans cet article une démarche pour la mise en place un processus extraction des connaissances à partir des données multidimensionnelles et temporelles\n",
      "tulip a scalable graph visualization framework\n",
      "the graph visualization framework tulip now enjoys 10 years of user experience and has matured its architecture and development cycle originally designed to interactively navigate large graphs the framework integrate state-of-the-art software engineering concepts and good practices it offers a large panel of graphical representations traditional graph drawing as well as alternate representations) tulip is most useful in a data mining and knowledge discovery context allowing users to easily add their own data analysis and computing routines through its plug-in architecture\n",
      "un modèle extraction de masses de croyance à partir de probabilités a posteriori pour une amélioration des performances en classification supervisée\n",
      "objectif de cet article est de montrer que utilisation de la règle de décision du maximum de masse de croyance en lieu et place de celle du maximum de probabilité a posteriori peut permettre de réduire le taux erreur en classification supervisée nous proposons une technique efficace pour extraire à partir un vecteur de probabilités a posteriori un vecteur de masses de croyance sur lequel baser la décision par le maximum de masse de croyance application de notre méthode dans le domaine de la classification automatique en stades de sommeil montre une amélioration des performances pouvant atteindre 80% de réduction du taux erreur de classification\n",
      "un système aide à extraction de relations sémantiques pour la construction ontologies à partir de textes\n",
      "cet article présente une méthode extraction de relations sémantiques pour la construction ontologies à partir de corpus de textes notre objectif est de proposer une méthode générique qui soit indépendante du domaine et de la langue elle repose sur une analyse distributionnelle des unités sémantiques du corpus pour faire émerger des relations sémantiques candidates cette méthode ne fait aucune hypothèse sur les types de relations recherchées ni sur leur forme linguistique il s'agit de regrouper les associations de termes dans des classes qui représentent des relations sémantiques candidates hypothèse sous-jacente est que les occurrences de ces associations réunies sur la base des éléments de contexte qu'elles partagent ont des chances de relever une même relation sémantique et que les relations candidates ainsi proposées peuvent aider le travail de conceptualisation de ontologue\n",
      "une approche fondée sur la corrélation entre prédicats pour le traitement des réponses pléthoriques\n",
      "interrogation de bases de données dont les dimensions ne cessent de croître se heurte fréquemment au problème de la gestion des réponses pléthoriques une des approches envisageables pour réduire ensemble des résultats retournés et le rendre exploitable est de contraindre la requête initiale par ajout de nouvelles conditions approche présentée dans cet article s'appuie sur identification de liens de corrélation entre prédicats associés aux attributs de la relation concernée la requête initiale peut ainsi être intensifiée automatiquement ou par validation de utilisateur à travers ajout de prédicats proches sémantiquement de ceux spécifiés\n",
      "une approche probabiliste pour identification de structures de communautés\n",
      "dans cet article nous valorisons et défendons idée que les modèles génératifs sont une approche prometteuse pour identification de structure de communautés (isc) nous proposons un nouveau modèle probabiliste pour identification de structures de communautés qui utilise le lissage afin de pallier le petit nombre de liens entre les noeuds notre modèle étant très sensible aux paramètres de lissage nous proposons également une méthode basée sur la modularité pour leur estimation les résultats expérimentaux obtenus sur 3 jeux de données montrent que notre modèle spce est largement meilleur que le modèle phits\n",
      "une méthode aide au management de connaissances pour améliorer le processus de suivi et évaluation de la prise en charge précoce des enfants imc  application de ashms\n",
      "nan\n",
      "une nouvelle approche de découverte des correspondances complexes entre ontologies\n",
      "les correspondances complexes ont été étudiées à plusieurs reprises dans le domaine alignement de schémas de bases de données par contre dans le domaine alignement des ontologies elles ont été peu étudiées nous proposons dans ce papier une nouvelle approche de découverte de correspondances complexes entre deux ontologies approche proposée est extensionnelle terminologique et implicative dans cette approche nous utilisons le modèle des règles association afin de découvrir des correspondances de type x => y1 ^  ^ yn entre deux ontologies\n",
      "une nouvelle stratégie apprentissage bayésienne\n",
      "dans cet article une nouvelle stratégie apprentissage actif est proposée cette stratégie est fondée sur une méthode de discrétisation bayésienne semi-supervisée des expériences comparatives sont menées sur des données unidimensionnelles objectif étant estimer la position un échelon à partir de données bruitées\n",
      "une ontologie pour acquisition et exploitation des connaissances en conception inventive\n",
      "acquisition des connaissances en vue de résoudre des problèmes concernant évolution des artefacts comme elle se doit être pratiquée en conception inventive a des caractéristiques spécifiques elle nécessite la sélection de certaines des connaissances qui peuvent induire des évolutions elle amène à reformuler le problème initial afin de construire un modèle abstrait de artefact concerné la méthode de conception inventive induite parla théorie de la résolution des problèmes inventifs aussi connue sous acronyme triz n'a pas encore fait objet une véritable formalisation nous proposons ici une ontologie des notions principales des concepts liés à acquisition des connaissances dans ce cadre cette ontologie outre la clarification des notions en jeu est utilisée comme support un encironnement informatique aide à la mise en oeuvre une méthode pour acquérir les connaissances et formuler les problèmes\n",
      "une structure basée sur les hiérarchies pour synthétiser les itemsets fréquents extraits dans des fenêtres temporelles\n",
      "le paradigme des flots de données rend impossible la conservation de intégralité de historique un flot qu'il faut alors résumer extraction itemsets fréquents sur des fenêtres temporelles semble tout à fait adaptée mais amoncellement des résultats indépendants rend impossible exploitation de ces résultats nous proposons une structuré basée sur les hiérarchies des données afin unifiant ces résultats de plus puisque la plupart des données un flot présentent un caractère multidimensionnel nous intégrons la prise en compte itemsets multidimensionnels enfin nous pallions une faiblesse majeure des tilted time windows ttw) en prenant en compte la distribution des données\n",
      "utilisation de graphes sémantiques pour extraction et la traduction des idées essentielles un texte\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "vers une extraction et une visualisation des co-localisations adaptées aux experts\n",
      "une des tâches classiques en fouille de données spatiales est extraction de co-localisations intéressantes dans des données géo-référencées objectif est de trouver des sous-ensembles de caractéristiques booléennes apparaissant fréquemment dans des objets spatiaux voisins toutefois les relations découvertes peuvent ne pas être pertinentes pour les experts et leur interprétation sous forme textuelle peut être difficile nous proposons dans ce contexte une nouvelle approche pour intégrer la connaissance des experts dans la découverte des co-localisations ainsi qu'une nouvelle représentation visuelle de ces motifs un prototype a été développé et intégré dans un sig des expérimentations on été menées sur des données géologiques réelles et les résultats validés par un expert du domaine\n",
      "visual sentence-phrase-based document representation for effective and efficient content-based image retrieval\n",
      "having effective and efficient methods to get access to desired images is essential nowadays with the huge amount of digital images this paper presents an analogy between content-based image retrieval and text retrieval we make this analogy from pixels to letters patches to words sets of patches to phrases and groups of sets of patches to sentences to achieve a more accurate document matching more informative features including phrases and sentences are needed to improve these scenarios the proposed approach is based first on constructing different visual words using local patch extraction and description after that we study different association rules between frequent visual words in the context of local regions in the image to construct visual phrases which will be grouped to different sentences\n",
      "visualisation de mesures agrégées pour estimation de la qualité des articleswikipedia\n",
      "wikipedia devenue une des bases de connaissances les plus populaires pose le problème de la fiabilité de information qu'elle dissémine nous proposons wikipediaviz un ensemble de visualisations basé sur un mecanisme de collecte et agrégation de données édition wikipedia pour aider le lecteur à appréhender la maturité un article nous listons cinq métriques importantes déterminées lors de sessions de conception participative avec des experts wikipedia pour juger de la qualité que nous présentons au lecteur sous forme de visualisations compactes et expressives dépeignant le profil évolution un article nos études utilisateur ont montré que wikipediaviz réduisait significativement le temps requis pour évaluer la qualité en maintenant une bonne précision\n",
      "wcum pour analyse un site web\n",
      "dans ce papier nous proposons une approche wcum web content and usage based approach permettant de relier analyse du contenu un site web à analyse de usage afin de mieux comprendre les comportements de navigation sur le site apport de ce travail réside une part dans la proposition une approche reliant analyse du contenu à analyse de usage et autre part dans extension de application des méthodes de block clustering appliquées généralement en bioinformatique au contexte web mining afin de profiter de leur pouvoir classificatoire dans la découverte de biclasses homogènes à partir une partition des instances et une partition des attributs recherchées simultanément\n",
      "a contextualization service for a personalized access model\n",
      "personalization paradigm aims at providing users with the most relevant content and services according to many factors such as interest center or location at the querying time all this knowledge and requirements are organized into user profiles and contexts a user profile encompasses metadata describing the user whereas a context groups information about the environment of interaction between the user and the system an interesting problem is therefore to identify which part of the profile is significant in a given context this paper proposes a contextualization service which allows defining relationships between user preferences and contexts further we propose an approach for the automatic discovery of these mappings by analyzing user behavior extracted from log files\n",
      "accompagner au début du 21ème siècle les organisations dans la mise en place une gestion des connaissances  retour expérience\n",
      "cet article présente succinctement le retour expérience ardans dans implantation de systèmes de gestion de connaissances dans des organisations très variées au début de ce 21ème siècle\n",
      "acquisition de la théorie ontologique un système extraction information\n",
      "la conception de systèmes extraction information ei) destinés à extraire les réseaux interactions géniques décrits dans la littérature scientifique est un enjeu important de tels systèmes nécessitent des repésentations sophistiquées s'appuyant sur des ontologies afin de définir différentes relations biologiques ainsi que les dépendances récursives qu'elles présentent entre elles cependant acquisition de ces dépendances n'est pas possible avec les techniques apprentissage automatique actuellement employées en ei car ces dernières ne gèrent pas la récursivité afin de palier ces limitations nous présentons une application à ei de la programmation logique inductive en mode multiprédicats nos expérimentations effectuées sur un corpus bactérien conduisent à un rappel global de 67.7% pour une précision de 755%\n",
      "acquisition annotation et exploration interactive images stéréoscopiques en réalité virtuelle  application en dermatologie\n",
      "nous présentons dans cet article le système skin3d qui implémente tous les composants matériels et logiciels nécessaires pour extraire des informations dans des images 3d de peau il s'agit à la fois du matériel éclairage et acquisition à base appareils photographiques stéréoscopiques une méthode de calibration de caméras utilisant les algorithmes génétiques de matériel de réalité virtuelle pour restituer les images en stéréoscopie et interagir avec elles et enfin un ensemble de fonctionnalités interactives pour annoter les images partager ces annotations et construire un hypermédia 3d nous présentons une étude comparative concernant la calibration et une application réelle de skin3d sur des images de visages\n",
      "aggregative and neighboring approximations to query semi-structured documents\n",
      "structures heterogeneity in web resources is a constant concern in element retrieval (ie tag retrieval in semi-structured documents) in this paper we present the shiri 1 querying approach which allows to reach more or less structured document parts without an a priori knowledge on their structuring\n",
      "an approach for handling risk and uncertainty in multiarmed bandit problems\n",
      "an approach is presented to deal with risk in multiarmed bandit prob-lems specifically the well known exploration-exploitation dilemma is solvedfrom the point of view of maximizing an utility function which measures thedecision maker's attitude towards risk and uncertain outcomes a link withthe preference theory is thus established simulations results are provided forin order to support the main ideas and to compare the approach with existingmethods with emphasis on the short term small sample size behavior of theproposed method\n",
      "analyse de dissimilarités par arbre induction\n",
      "dans cet article1 nous considérons des objets pour lesquels nous dis-posons une matrice des dissimilarités et nous nous intéressons à leurs liensavec des attributs nous nous centrons sur analyse de séquences états pourlesquelles les dissimilarités sont données par la distance d'édition toutefois lesméthodes développées peuvent être étendues à tout type objets et de mesurede dissimilarités nous présentons dans un premier temps une généralisation deanalyse de variance anova) pour évaluer le lien entre des objets non mesu-rables (p ex des séquences avec une variable catégorielle la clef de approcheest exprimer la variabilité en termes des seules dissimilarités ce qui nous per-met identifier les facteurs qui réduisent le plus la variabilité nous présentonsun test statistique général qui peut en être déduit et introduisons une méthodeoriginale de visualisation des résultats pour les séquences états nous présen-tons ensuite une généralisation de cette analyse au cas de facteurs multiples et endiscutons les apports et les limites notamment en terme d'interprétation fina-lement nous introduisons une nouvelle méthode de type arbre induction quiutilise le test précédent comme critère d'éclatement la portée des méthodesprésentées est illustrée à aide une analyse des facteurs discriminant le plusles trajectoires occupationnelles \n",
      "analyse de données pour la construction de modèles de procédures neurochirurgicales\n",
      "dans cet article nous appliquons une méthode analyse sur desdescriptions de procédures de neurochirurgie dans le but en améliorer lacompréhension la base de données xml utilisée dans cette étude estconstituée de la description de 157 chirurgies de tumeurs trois cent vingtdeux variables ont été identifiées et décomposées en variables prédictives(connues avant l'opération et variables à prédire décrivant des gesteschirurgicaux) une analyse factorielle des correspondances afc) a étéréalisée sur les variables prédictives ainsi qu'un arbre de décision basé sur undendrogramme préalablement établi six classes principales de variablesprédictives ont ainsi été identifiées puis pour chacune de ces classes uneanalyse afc a été réalisée sur les variables à prédire ainsi qu'un arbre dedécision bien que le nombre de cas et le choix des variables constituent unelimite à cette étude nous avons réussi à prédire certaines caractéristiques liéesaux procédures en partant de données prédictives\n",
      "analyse et application de modèles de régression pour optimiser le retour sur investissement opérations commerciales\n",
      "nan\n",
      "analyse et application de modèles de régression pour optimiser le retour sur investissement opérations commerciales\n",
      "les activités de négoce de matériaux sont un marché extrêmementcompétitif pour les acteurs de ce marché les méthodes de fouille de donnéespeuvent s'avérer intéressantes en permettant de dégager des gains de rentabilitéimportants dans cet article nous présenterons le retour expérience du projetde fouille de données mené chez vm matériaux pour améliorer le retour surinvestissement opérations commerciales la synergie des informaticiens dumarketing et des experts métier a permis améliorer extraction des connais-sances à partir des données de manière à aboutir à la connaissance actionnable laplus pertinente possible et ainsi aider les experts métier à prendre des décisions\n",
      "analyse multigraduelle olap\n",
      "les systèmes décisionnels reposent sur des bases de données multidimensionnellesqui offrent un cadre adéquat aux analyses olap articleprésente un nouvel opérateur olap nommé « blend » rendant possible desanalyses multigraduelles il s'agit de transformer la structuration multidimensionnellelors des interrogations pour analyser les mesures selon des niveauxde granularité différents recombinées comme un même paramètre nous menonsune étude des combinaisons valides de opération dans le contexte deshiérarchies strictes enfin une première série expérimentations implanteopération dans le contexte r-olap en montrant le faible coût de opération.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "analyse sémantique spatio-temporelle pour les ontologies owl-dl\n",
      "analyse sémantique est un nouveau paradigmeinterrogation du web sémantique qui a pour objectif identifier lesassociations sémantiques reliant des individus décrits dans desontologies owl-dl pour déduire davantage associationssémantiques et augmenter la précision de analyse, informationspatio-temporelle attachée aux ressources doit être prise en compte aces fins - et pour combler absence actuelle de raisonneurs spatiotemporeldéfini pour les ontologies rdf(s et owl- nous proposonsle système de représentation et interrogation ontologies spatiotemporellesontoast compatible avec le langage owl-dl nousprésentons les principes de base de algorithme de découverted'associations sémantiques entre individus intégré dans ontoast.cet algorithme utilise deux contextes un spatial et autre temporelqui permettent affiner la recherche nous décrivons enfin approchemise en oeuvre pour la déduction de connexions spatiales entreindividus\n",
      "assessing the uncertainty in knn data fusion\n",
      "nan\n",
      "binary sequences and association graphs for fast detection of sequential patterns\n",
      "we develop an efficient algorithm for detecting frequent patterns thatoccur in sequence databases under certain constraints by combining the useof bit vector representations of sequence databases with association graphs weachieve superior time and low memory usage based on a considerable reductionof the number of candidate patterns\n",
      "caractérisation automatique des classes découvertes en classification non supervisée\n",
      "dans cet article nous proposons une nouvelle approche de classifi- cation et de pondération des variables durant un processus apprentissage non supervisé cette approche est basée sur le modèle des cartes auto-organisatrices apprentissage de ces cartes topologiques est combiné à un mécanisme esti- mation de pertinences des différentes variables sous forme de poids influence sur la qualité de la classification nous proposons deux types de pondérations adaptatives  une pondération des observations et une pondération des distances entre observations apprentissage simultané des pondérations et des prototypes utilisés pour la partition des observations permet obtenir une classification op- timisée des données un test statistique est ensuite utilisé sur ces pondérations pour élaguer les variables non pertinentes ce processus de sélection de variables permet enfin grâce à la localité des pondérations exhiber un sous ensemble de variables propre à chaque groupe cluster) offrant ainsi sa caractérisation approche proposée a été validé sur plusieurs bases de données et les résultats expérimentaux ont montré des performances très prometteuses\n",
      "ciblage des règles association intéressantes guidé par les connaissances du décideur\n",
      "usage du modèle des règles association en fouille de données estlimité par la quantité prohibitive de règles qu'il fournit et nécessite la mise enplace une phase de post-traitement efficace afin de cibler les règles les plusutiles cet article propose une nouvelle approche intégrant explicitement lesconnaissances du décideur afin de filtrer et cibler les règles intéressantes\n",
      "cisna un système hybride ld+règles pour gérer des connaissances\n",
      "nan\n",
      "classification des images de télédétection avec envi fx\n",
      "importants volumes images satellites et aériennes de tout type(panchromatiques multispectrales hyperspectrales sont généréesquotidiennement et leur classification par des méthodes semi-automatiquesdevient nécessaire le logiciel envi feature extractiontm envi fxtm sebase sur une approche « objet » -par opposition à une approche pixelsclassique- et sur des algorithmes innovants pour la segmentation et laclassification des images de télédétection avec un haut niveau de précision\n",
      "collaborative outlier mining for intrusion detection\n",
      "intrusion detection is an important topic dealing with security of in-formation systems most successful intrusion detection systems ids) rely onsignature detection and need to update their signature as fast as new attacks areemerging on the other hand anomaly detection may be utilized for this purpose,but it suffers from a high number of false alarms actually any behaviour whichis significantly different from the usual ones will be considered as dangerousby an anomaly based ids therefore isolating true intrusions in a set of alarmsis a very challenging task for anomaly based intrusion detection in this paper,we consider to add a new feature to such isolated behaviours before they can beconsidered as malicious this feature is based on their possible repetition fromone information system to another we propose a new outlier mining principleand validate it through a set of experiments\n",
      "comment valider automatiquement des relations syntaxiques induites\n",
      "nous présentons dans cet article des approches visant à valider desrelations syntaxiques induites de type verbe-objet ainsi nous proposons u-tiliser dans un premier temps une approche s'appuyant sur des vecteurs séman-tiques déterminés à aide un thésaurus la seconde approche emploie unevalidation web nous effectuons des requêtes sur un moteur de recherche asso-ciées à des mesures statistiques afin de déterminer la pertinence une relationsyntaxique nous proposons enfin de combiner ces deux méthodes la qualitéde nos approches de validation de relations syntaxiques a été évaluée en utilisantdes courbes roc\n",
      "comparaison de distances et noyaux classiques par degré équivalence des ordres induits\n",
      "le choix une mesure pour comparer les données est au coeur destâches de recherche information et apprentissage automatique nous considéronsici ce problème dans le cas où seul ordre induit par la mesure importe,et non les valeurs numériques qu'elle fournit  cette situation est caractéristiquedes moteurs de recherche de documents par exemple nous étudions dans cecadre les mesures de comparaison classiques pour données numériques tellesque les distances et les noyaux les plus courants nous identifions les mesureséquivalentes qui induisent toujours le même ordre  pour les mesures non équivalentes,nous quantifions leur désaccord par des degrés équivalence basés surle coefficient de kendall généralisé nous étudions les équivalences et quasiéquivalencesà la fois sur les plans théorique et expérimental\n",
      "constraint programming for data mining\n",
      "nan\n",
      "construction de descripteurs pour classer à partir exemples bruités\n",
      "en classification supervisée la présence de bruit sur les valeurs desdescripteurs peut avoir des effets désastreux sur la performance des classifieurset donc sur la pertinence des décisions prises au moyen de ces modèles traiterce problème lorsque le bruit affecte un attribut classe a été très étudié il estplus rare de s'intéresser au bruit sur les autres attributs c'est notre contextede travail et nous proposons la construction de nouveaux descripteurs robusteslorsque ceux des exemples originaux sont bruités les résultats expérimentauxmontrent la valeur ajoutée de cette construction par la comparaison des qualitésobtenues (e.g. précision lorsque on utilise les méthodes de classification àpartir de différentes collections de descripteurs\n",
      "contrôle des observations pour la gestion des systèmes de flux de données\n",
      "les systèmes analyse de flux de données prennent de plus en plusd'importance dans un contexte où les données circulant sur les réseaux sont deplus en plus volumineuses et où la volonté de réagir au plus vite en temps réel,devient un besoin nécessaire afin de permettre des analyses aussi rapides etefficaces que possible il convient de pouvoir contrôler les flots de données et defocaliser les traitements sur les données pertinentes le protocole présenté dansce papier donne au module de traitement des capacités action et de contrôle surles observations remontantes en fonction de état de l'analyse la diminutiondes flux résultant de telles focalisations permet des traitements beaucoup plusefficaces plus pertinents et moins consommateurs de ressources les premiersrésultats montrent un réel gain de performances sur nos applications (facteur100)\n",
      "correspondances de galois pour la manipulation de contextes flous multi-valués\n",
      "analyse formelle de concepts est une méthode fondée sur la correspondancede galois et qui permet de construire des hiérarchies de conceptsformels à partir de tableaux de données binaires cependant de nombreux problèmesréels abordés en fouille de données comportent des données plus complexes.afin de traiter de tels problèmes nous proposons une conversion de donnéesfloues multi-valuées en attributs histogrammes et une correspondance degalois adaptée à ce format notre propos est illustré avec un jeu de donnéessimples enfin nous évaluons brièvement les résultats et les apports de cettecorrespondance de galois par rapport à approche classique\n",
      "dbfrequentqueries  extraction de requêtes fréquentes\n",
      "nan\n",
      "de utilisation de analyse de données symboliques dans les systèmes multi-agents\n",
      "exploitation en temps réel de connaissances complexes est un défidans de nombreux domaines tels que le web sémantique la simulation ou lessystèmes multi-agents (sma) dans le paradigme multi-agents des travaux ré-cents montrent que les communications multi-parties cmp) offrent des oppor-tunités intéressantes en termes de réalisme des communications diffusion desconnaissances et sémantique des actes de langage cependant ces travaux seheurtent à la difficulté de mise en oeuvre des cmp pour lesquelles les supportsde communications classiques sont insuffisants dans cet article nous propo-sons utiliser le formalisme de analyse de données symboliques ads) pourmodéliser les informations et les besoins des agents nous appuyons le routagedes messages sur cette modélisation dans le cadre un environnement de com-munication pour les systèmes multi-agents afin illustrer notre propos nousutiliserons exemple de la gestion des communications dans un poste d'appelsd'urgence nous présentons ensuite notre retour d'expérience et discutons lesperspectives ouvertes par la fertilisation croisée de ads et des sma\n",
      "définition une stratégie de résolution de problèmes pour un robot humanoïde\n",
      "nous avons développé un système dont le but est obtenir le logicielde commande un robot capable de simuler le comportement un humainplacé en situation de résolution de problèmes nous avons résolu ce problèmedans un environnement psychologique particulier où les comportements humainspeuvent être interprétés comme des ‘observables de leurs stratégies derésolution de problèmes notre solution contient de plus celle un autre problème,celui de construire une boucle complète commençant avec le comportementun groupe d'humains son analyse et son interprétation en termesd'observables humaines la définition des stratégies utilisées par les humains ycompris celles qui sont inefficaces) interprétation des observables humainesen terme de mouvements du robot la définition de ce qu'est une “stratégie derobot ” en terme de stratégies humaines la boucle est bouclée avec un langagede programmation capable de programmer ces stratégies robotiques qui deviennentainsi à leur tour des observables tout comme ont été les stratégieshumaines du début de la boucle nous expliquons comment nous avons été capablesdéfinir de façon objective ce que nous appelons une stratégie de robot.notre solution assemble deux facteurs différents un permet éviter lescomportements ‘inhumains et se fonde sur la moyenne des comportementsdes humains que nous avons observés autre fournit une sorte ‘d'humanité'au robot en lui permettant de dévier de cette moyenne par n fois écart typeobservé chez les humains qu'il doit simuler il devient alors possible de programmerdes comportements complètements humains\n",
      "demon  découverte de motifs séquentiels pour les puces adn\n",
      "prometteuses en terme de prévention de dépistage de diagnostic etd'actions thérapeutiques les puces à adn mesurent intensité des expressionsde plusieurs milliers de gènes dans cet article nous proposons une nouvelleapproche appelée demon pour extraire des motifs séquentiels à partir de don-nées issues des puces adn et qui utilise des connaissances du domaine\n",
      "demon-visualisation  un outil pour la visualisation des motifs séquentiels extraits à partir de données biologiques\n",
      "nan\n",
      "desesper un logiciel de pré-traitement de flux appliqué à la surveillance des centrales hydrauliques\n",
      "nan\n",
      "détection intrusions dans un environnement collaboratif sécurisé\n",
      "pour pallier le problème des attaques sur les réseaux de nouvelles ap-proches de détection anomalies ou abus ont été proposées ces dernières an-nées et utilisent des signatures attaques pour comparer une nouvelle requêteet ainsi déterminer s'il s'agit une attaque ou pas cependant ces systèmes sontmis à défaut quand la requête n'existe pas dans la base de signature généra-lement ce problème est résolu via une expertise humaine afin de mettre à jourla base de signatures toutefois il arrive fréquemment qu'une attaque ait déjàété détectée dans une autre organisation et il serait utile de pouvoir bénéficier decette connaissance pour enrichir la base de signatures mais cette information estdifficile à obtenir car les organisations ne souhaitent pas forcément indiquer lesattaques qui ont eu lieu sur le site dans cet article nous proposons une nouvelleapproche de détection intrusion dans un environnement collaboratif sécurisé.notre approche permet de considérer toute signature décrite sous la forme ex-pressions régulières et de garantir qu'aucune information n'est divulguée sur lecontenu des différents sites\n",
      "détection objets atypiques dans un flot de données  une approche multi-résolution\n",
      "nan\n",
      "détection de séquences atypiques basée sur un modèle de markov ordre variable\n",
      "récemment le nombre et le volume des bases de données séquentiellesbiologiques ont augmenté de manière considérable dans ce contexte identificationdes anomalies est essentielle la plupart des approches pour lesextraire se fondent sur une base apprentissage ne contenant pas d'outlier or,dans de très nombreuses applications les experts ne disposent pas une tellebase de plus les méthodes existantes demeurent exigeantes en mémoire cequi les rend souvent impossibles à utiliser nous présentons dans cet article unenouvelle approche basée sur un modèle de markov ordre variable et sur unemesure de similarité entre objets séquentiels nous ajoutons aux méthodes existantesun critère élagage pour contrôler la taille de espace de rechercheet sa qualité ainsi qu'une inégalité de concentration précise pour la mesure desimilarité conduisant à une meilleure détection des outliers nous démontronsexpérimentalement la validité de notre approche\n",
      "détermination du nombre des classes dans algorithme croki2 de classification croisée\n",
      "un des problèmes majeurs de la classification non supervisée est ladétermination ou la validation du nombre de classes dans la population ce problèmes'étend aux méthodes de bipartitionnement ou block clustering dans cepapier nous nous intéressons à algorithme croki2 de classification croiséedes tableaux de contingence proposé par govaert (1983) notre objectif est dedéterminer le nombre de classes optimal sur les lignes et les colonnes à traversun ensemble de techniques de validation de classes proposés dans la littératurepour les méthodes classiques de classification\n",
      "diagnostic multi-sources adaptatif application à la détection intrusion dans des serveursweb\n",
      "le but un système adaptatif de diagnostic est de surveiller et diagnostiquerun système tout en s'adaptant à son évolution ceci passe par adaptationdes diagnostiqueurs qui précisent ou enrichissent leur propre modèle poursuivre au mieux le système au fil du temps pour détecter les besoins adaptation,nous proposons un cadre de diagnostic multi-sources s'inspirant de lafusion d'information des connaissances fournies par le concepteur sur des relationsattendues entre les diagnostiqueurs mono-source forment un méta-modèledu diagnostic la compatibilité des résultats du diagnostic avec le méta-modèleest vérifiée en ligne lorsqu'une de ces relations n'est pas vérifiée les diagnostiqueursconcernés sont modifiés.nous appliquons cette approche à la conception un système adaptatif de détectiond'intrusion à partir un flux de connexions à un serveur web les évaluationsdu système mettent en évidence sa capacité à améliorer la détection desintrusions connues et à découvrir de nouveaux types d'attaque\n",
      "empreintes conceptuelles et spatiales pour la caractérisation des réseaux sociaux\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cet article propose une méthode reposant sur utilisation deanalyse formelle de concepts et des treillis de galois pour analyse desystèmes complexes des statistiques reposant sur ces treillis permettent decalculer la distribution conceptuelle des objets classifiés par le treillis.l'expérimentation sur des échantillons de trois réseaux sociaux en ligneillustre utilisation de ces statistiques pour la caractérisation globale et pour lefiltrage automatique de ces systèmes\n",
      "exploration de données de traçabilité issues de la rfid par apprentissage non-supervisé\n",
      "la rfid radio frequency identification est une technologie avancée enregistrementde données spatio-temporelles de traçabilité objectif de ce travail est de transformer cesdonnées spatio-temporelles en connaissances exploitables par les utilisateurs par intermé-diaire une méthode de classification automatique des données les systèmes rfid peuventêtre utilisés pour étudier les sociétés animales qui sont des systèmes dynamiques complexescaractérisés par beaucoup interactions entre les individus fresneau et al. 1989) le cadreapplicatif choisi pour ce travail est étude de la structure un groupe individus en interactionsociale et en particulier la division du travail au sein une colonie de fourmis1.la rfid générant importants volumes de données il est nécessaire de développer desméthodes appropriées afin en comprendre le sens nous proposons pour cela un algorithmede classification topographique non-supervisée pour exploration de ce type de données ca-pable de détecter les groupes individus exprimant le même comportement algorithmeds2l-som density-based simultaneous two-level - som cabanes et bennani 2008)) estcapable de détecter non seulement les groupes définis par une zone vide de donnée grâce àune estimation de la pertinence des connexions entre référents mais aussi les groupes défi-nis seulement par une diminution de densité grâce à une estimation de la densité autour desréférents pendant l'apprentissage\n",
      "exploration des corrélations dans un classifieur application au placement offres commerciales\n",
      "cet article présente une nouvelle méthode permettant explorer lesprobabilités délivrées par un modèle prédictif de classification augmentationde la probabilité occurrence de une des classes du problème étudié est analyséeen fonction des variables explicatives prises isolément la méthode proposéeest posée et illustrée dans un cadre général puis explicitement dédiée au classifieurbayesien naïf son illustration sur les données du challenge pakdd 2007montre que ce type exploration permet de créer des indicateurs performantsd'aide à la vente\n",
      "explorer3d  classification et visualisation de données\n",
      "nan\n",
      "extraction de motifs fermés dans des relations n-aires bruitées\n",
      "extraction de motifs fermés dans des relations binaires a été trèsétudiée cependant de nombreuses relations intéressantes sont n-aires avec n >2 et bruitées nécessité une tolérance aux exceptions) récemment ces deuxproblèmes ont été traités indépendamment nous introduisons notre propositionpour combiner de telles fonctionnalités au sein un même algorithme\n",
      "extraction de règles de corrélation décisionnelles\n",
      "dans cet article nous introduisons deux nouveaux concepts  les règlesde corrélation décisionnelles et les vecteurs de contingence le premier résulteun couplage entre les règles de corrélation et les règles de décision il permetde mettre en évidence des liens pertinents entre certains ensembles de motifsune relation binaire et les valeurs un attribut cible appartenant à cette mêmerelation en se basant à la fois sur la mesure du khi-carré et sur le support desmotifs extraits de par la nature du problème les algorithmes par niveaux fontque extraction des résultats a lieu avec des temps de réponse élevés et uneoccupation mémoire importante afin de palier à ces deux inconvénients nousproposons un algorithme basé sur ordre lectique et les vecteurs de contingence\n",
      "extraction efficace de règles graduelles\n",
      "les règles graduelles suscitent depuis quelques années un intérêt croissant.de telles règles de la forme “plus moins) a1 et  plus moins) an alorsplus moins) b1 et  plus moins) bn” trouvent application dans de nombreuxdomaines tels que la bioinformatique les contrôleurs flous les relevés de capteursou encore les flots de données ces bases souvent composées un grandnombre d'attributs restent un verrou pour extraction automatique de connaissances,car elles rendent inefficaces les techniques de fouille habituelles (règlesd'association clustering). dans cet article nous proposons un algorithme efficaced'extraction itemset graduels basé sur utilisation des treillis nous définissonsformellement les notions de gradualité ainsi que les algorithmes associés.des expérimentations menées sur jeux de données synthétiques et réelsmontrent intérêt de notre méthode\n",
      "fcp-growth une adaptation de fp-growth pour générer des règles association de classe\n",
      "nan\n",
      "fouille de données dans les bases relationnelles pour acquisition ontologies riches en hiérarchies de classes\n",
      "de par leur caractère structuré les bases de données relationnellessont des sources précieuses pour la construction automatisée d'ontologies ce-pendant une limite persistante des approches existantes est la production onto-logies de structure calquée sur celles des schémas relationnels sources dans cetarticle nous décrivons la méthode rtaxon dont la particularité est identifierdes motifs de catégorisation dans les données afin de produire des ontologiesplus structurées riches en hiérarchies la méthode formalisée combine analyseclassique du schéma relationnel et fouille des données pour identification destructures hiérarchiques\n",
      "fusion symbolique pour la recommandation de programmes télévisées\n",
      "nous proposons une approche générique pour la fusion informa-tions qui repose sur utilisation du modèle des graphes conceptuels et opé-ration de jointure maximale nous validons notre approche par le biais d'ex-périmentations ces expérimentations soulignent importance des heuristiquesmises en place\n",
      "générer des règles de classification par dopage de concepts formels\n",
      "la classification supervisée est une tâche de fouille de données (datamining) qui consiste à construire un classifieur à partir un ensemble exemplesétiquetés par des classes phase d'apprentissage et ensuite prédire les classesdes nouveaux exemples avec ce classifieur phase de classification) en classi-fication supervisée plusieurs approches ont été proposées dont approche ba-sée sur analyse de concepts formels apprentissage de concepts formelsest basé généralement sur la structure mathématique du treillis de galois outreillis de concepts) cependant la complexité exponentielle de génération untreillis de galois a limité les champs application de ces systèmes dans cetarticle nous présentons plusieurs méthodes de classification supervisée baséessur analyse de concepts formels nous présentons aussi le boosting dopage)de classifieurs une technique de classification innovante enfin nous proposonsle boosting de concepts formels une nouvelle méthode adaptative qui construitseulement une partie du treillis englobant les meilleurs concepts ces conceptssont utilisés comme étant des règles de classification les résultats expérimen-taux réalisés ont prouvé intérêt de la méthode proposée par rapport à cellesexistantes\n",
      "graphes des liens et anti liens statistiquement valides entre les mots un corpus textuel  test de randomisation tournebool sur le corpus reuters\n",
      "la définition du voisinage est un élément central en fouille de données et de nombreuses définitions ont été avancées nous en proposons ici une version statistique issue de notre test de randomisation tournebool qui permet à partir un tableau de relations binaires objets décrits/descripteurs établir quelles relations entre descripteurs sont dues au hasard et lesquelles ne le sont pas sans faire hypothèse sur les lois de répartitions sous-jacentes c'est à dire en tenant compte de lois de tous types sans avoir besoin de les spécifier ce test est basé sur la génération et exploitation un ensemble de matrices randomisées ayant les mêmes sommes marginales en lignes et colonnes que la matrice d'origine après une première application encourageante à un corpus textuel réduit nous avons opéré le passage à échelle adéquat pour traiter des corpus textuels de taille réelle comme celui des dépêches reuters nous caractérisons le graphe des mots de ce corpus au moyen indicateurs classiques comme le coefficient de clustering la distribution des degrés et de la taille des communautés etc une autre caractéristique de tournebool est qu'il permet aussi de dégager les \"anti liens\" entre mots à savoir les mots qui s'évitent plus qu'attendu du fait du hasard le graphe des liens et celui des anti-liens seront caractérisés de la même façon\n",
      "handling texts  a challenge for data mining\n",
      "the amount of data in free form by far surpasses the structured records in databases in theirnumber however standard learning algorithms require observations in the form of vectorsgiven a fixed set of attributes for texts there is no such fixed set of attributes the bag ofwords representation yields vectors with as many components as there are words in a language.hence the classification of documents represented as bag of word vectors demands efficientlearning algorithms the tcat model for the support vector machine joachims 2002 offers asound performance estimation for text classification.the huge mass of documents in principle offers answers to many questions and is oneof the most important sources of knowledge however information retrieval and text classi-fication deliver merely the document in which the answer can be found by a human reader ?not the answer itself hence information extraction has become an important topic if we canextract information from text we can apply standard machine learning to the extracted facts(craven et al 1998) first information extraction has to recognize named entities (see e.g.roessler morik 2005) second relations between these become the nucleus of events ex-tracting events from a complex web site with long documents allows to automatically discoverregularities which are otherwise hidden in the mass of sentences (see e.g. jungermann morik2008)\n",
      "analyse formelle de concepts pour extraction de connaissances dans les données expression de gènes\n",
      "analyse formelle de concepts (afc ganter etwille 1999)) est uneméthode pertinente extraction de connaissances à partir de données complexesd'expression de gènes blachon et al (2007) motameny et al (2008)) dans cepapier nous proposons extraire des groupes de gènes partageant un compor-tement similaire montrant des changements “significatifs” à travers divers envi-ronnements biologiques servant hypothèses à la fonction des gènes\n",
      "la carte ghsom comme alternative à la som pour analyse exploratoire de données\n",
      "objecif de cet article est de faire de la carte auto-organisatrice hiérarchique(ghsom un outil utilisable dans le cadre une démarche analyseexploratoire de données la visualisation globale est un outil indispensable pourrendre les résultats une segmentation intelligibles pour un utilisateur nousproposons donc différents outils de visualisation pour la ghsom équivalents àceux de la som\n",
      "la « créativité calculatoire » et les heuristiques créatives en synthèse de prédicats multiples\n",
      "nous présentons une approche à ce que nous appelons la « créativitécalculatoire » c'est-à-dire les procédés par lesquels une machine peut fairemontre une certaine créativité dans cet article nous montronsessentiellement que la synthèse de prédicats multiples en programmationlogique inductive ilp) et la synthèse de programmes à partir de spécificationsformelles (spsf) deux domaines de informatique qui s'attaquent à desproblèmes où la notion de créativité est centrale ont été amenés à ajouter àleur formalisme de base ilp pour l'un les tableaux de beth pour autre)toute une série d'heuristiques cet article présente une collectiond'heuristiques qui sont destinées à fournir au programme une forme decréativité calculatoire dans cette présentation accent est plutôt mis sur lesheuristiques de ilp mais lorsque cela était possible sans de trop longsdéveloppements nous avons aussi présenté quelques heuristiques de la spsf.l'outil indispensable de la créativité calculatoire est ce que nous appelons un‘générateur datouts dont une spécification forcément informelle commenous le verrons est fournie comme première conclusion aux exemples décritsdans le corps de l'article\n",
      "le logiciel syr pour analyse de données symboliques\n",
      "nan\n",
      "logiciel « dtmvic » data and text mining visualisation inférence classification\n",
      "nan\n",
      "management des connaissances dans le domaine du patrimoine culturel\n",
      "nan\n",
      "méthode de regroupement par graphe de voisinage\n",
      "ce travail s'inscrit dans la problématique de apprentissage non su-pervisé dans ce cadre se retrouvent les méthodes de classification automatiquenon paramétriques qui reposent sur hypothèse que plus des individus sontproches dans espace de représentation plus ils ont de chances de faire par-tie de la même classe cet article propose une nouvelle méthode de ce type quiconsidère la proximité à travers la structure fournie par un graphe de voisinage\n",
      "modèle de préférences contextuelles pour les analyses olap\n",
      "cet article présente un environnement pour la personnalisation desanalyses olap afin de réduire la charge de navigation de utilisateur nousproposons un modèle de préférences contextuelles qui permet de restituer lesdonnées en fonction des préférences de utilisateur et de son contexted'analyse\n",
      "modélisation des connaissances dans le cadre de bibliothèques numériques spécialisées\n",
      "nous présentons une application innovante de la modélisation desconnaissances au domaine des bibliothèques numériques spécialisées nous utilisonsla spécification experte de la tei text encoding initiative pour modéliserla connaissance apportée par les chercheurs qui travaillent sur des archivesmanuscrites nous montrons les limites de la tei dans le cas une approchediachronique du document cette dernière impliquant la construction simultanéede structures de données concurrentes nous décrivons un modèle qui présentele problème et permet envisager des solutions enfin nous justifions les structuresarborescentes sur lesquelles se base ce modèle\n",
      "okmed et wokm\n",
      "cet article traite de la problématique de la classification recouvrante(overlapping clustering et propose deux variantes de approche okm  okmedet wokm okmed généralise k-médoïdes au cas recouvrant il permet organiserun ensemble individus en classes non-disjointes à partir une matricede distances la méthode wokm weighted-okm) étend okm par une pondérationlocale des classes  cette variante autorise chaque individu à appartenir àplusieurs classes sur la base de critères différents des expérimentations sont réaliséessur une application cible  la classification de textes nous montrons alorsque okmed présente un comportement similaire à okm pour la métrique euclidienne,et offre la possibilité utiliser des métriques plus adaptées et obtenirde meilleures performances enfin les résultats obtenus avec wokm montrentun apport significatif de la pondération locale des classes\n",
      "online and adaptive anomaly detection detecting intrusions in unlabelled audit data streams\n",
      "nan\n",
      "partitionnement ontologies pour le passage à échelle des techniques alignement\n",
      "alignement ontologies est une tâche importante dans les systèmesd'intégration puisqu'elle autorise la prise en compte conjointe de ressourcesdécrites par des ontologies différentes en identifiant des appariements entreconcepts avec apparition de très grandes ontologies dans des domaines commela médecine ou l'agronomie les techniques d'alignement qui mettent souventen oeuvre des calculs complexes se trouvent face à un défi  passer à échelle.pour relever ce défi nous proposons dans cet article deux méthodes de partition-nement conçues pour prendre en compte le plus tôt possible objectif d'ali-gnement ces méthodes permettent de décomposer les deux ontologies à aligneren deux ensembles de blocs de taille limitée et tels que les éléments susceptiblesd'être appariés se retrouvent concentrés dans un ensemble minimal de blocs quiseront effectivement comparés les résultats des tests effectuées avec nos deuxméthodes sur différents couples ontologies montrent leur efficacité\n",
      "privacy and data mining new developments and challenges\n",
      "there is little doubt that data mining technologies create new challenges in the area of dataprivacy in this talk we will review some of the new developments in privacy-preserving datamining in particular we will discuss techniques in which data mining results can reveal per-sonal data and how this can be prevented we will look at the practically interesting situationswhere data to be mined is distributed among several parties we will mention new applica-tions in which mining spatio-temporal data can lead to identification of personal information.we will argue that methods that effectively protect personal data while at the same time pre-serve the quality of the data from the data analysis perspective are some of the principal newchallenges before the field\n",
      "probabilistic multi-classifier by svms from voting rule to voting features\n",
      "nan\n",
      "rdbtoonto  un logiciel dédié à apprentissage ontologies à partir de bases de données relationnelles\n",
      "rdbtoonto1 est un logiciel extensible qui permet élaborer des on-tologies précises à partir de bases de données relationnelles le processus sup-porté est largement automatisé de extraction des données à la génération dumodèle de ontologie et son instanciation pour affiner le résultat le processuspeut être orienté par des contraintes locales définies interactivement c'est aussiun cadre facilitant la mise en oeuvre de nouvelles méthodes d'apprentissage\n",
      "regroupement des définitions de sigles biomédicaux\n",
      "application présentée permet de regrouper les définitions de siglesissues des sciences du vivant par des mesures de proximité lexicale approcheautomatique) et une intervention de expert approche manuelle)\n",
      "résumé hybride de flux de données par échantillonnage et classification automatique\n",
      "face à la grande volumétrie des données générées par les systèmes informatiques,l'hypothèse de les stocker en totalité avant leur interrogation n'estplus possible une solution consiste à conserver un résumé de historique duflux pour répondre à des requêtes et pour effectuer de la fouille de données.plusieurs techniques de résumé de flux de données ont été développées tellesque l'échantillonnage le clustering etc selon le champ de requête ces résuméspeuvent être classés en deux catégories résumés spécialisés et résumés généralistesdans ce papier nous nous intéressons aux résumés généralistes notreobjectif est de créer un résumé de bonne qualité sur toute la période temporelle,qui nous permet de traiter une large panoplie de requêtes nous utilisons deuxalgorithmes  clustream et streamsamp idée consiste à les combiner afin detirer profit des avantages de chaque algorithme pour tester cette approche nousutilisons un benchmark de données réelles \"kdd_99\" les résultats obtenussont comparés à ceux obtenus séparément par les deux algorithmes\n",
      "softjaccard  une mesure de similarité entre ensembles de chaînes de caractères pour unification entités nommées\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parmi lesmesures de similarité classiques utilisables sur des ensemblesfigure indice de jaccard dans le cadre de cet article nous en proposons uneextension pour comparer des ensembles de chaînes de caractères cette mesurehybride permet de combiner une distance entre chaînes de caractères telle que ladistance de levenstein et indice de jaccard elle est particulièrement adaptéepourmettre en correspondance des champs composés de plusieurs chaînes de caractèrescomme par exemple lorsqu'on se propose unifier des noms d'entitésnommées\n",
      "spams une nouvelle approche incrémentale pour extraction de motifs séquentiels fréquents dans les data streams\n",
      "extraction de motifs séquentiels fréquents dans les datastreams est un enjeu important traité par la communauté des chercheursen fouille de données plus encore que pour les bases de données denombreuses contraintes supplémentaires sont à considérer de par la na-ture intrinsèque des streams dans cet article nous proposons un nouvelalgorithme en une passe  spams basé sur la construction incrémentale,avec une granularité très fine par transaction un automate appelé spa,permettant extraction des motifs séquentiels dans les streams infor-mation du stream est apprise à la volée au fur et à mesure de insertionde nouvelles transactions sans pré-traitement a priori les résultats ex-périmentaux obtenus montrent la pertinence de la structure utilisée ainsique efficience de notre algorithme appliqué à différents jeux de données\n",
      "svm incrémental et parallèle sur gpu\n",
      "nous présentons un nouvel algorithme incrémental et parallèle deséparateur à vaste marge svm ou support vector machine pour laclassification de très grands ensembles de données en utilisant le processeur dela carte graphique (gpus graphics processing units) les svms et lesméthodes de noyaux permettent de construire des modèles avec une bonneprécision mais ils nécessitent habituellement la résolution un programmequadratique ce qui requiert une grande quantité de mémoire et un long tempsd'exécution pour les ensembles de données de taille importante nousprésentons une extension de algorithme de least squares svm ls-svm)proposé par suykens et vandewalle pour obtenir un algorithme incrémental etparallèle le nouvel algorithme est exécuté sur le processeur graphique pourobtenir une bonne performance à faible coût les résultats numériques sur lesensembles de données de uci et delve montrent que notre algorithmeincrémental et parallèle est environ 70 fois plus rapide sur gpu que sur cpuet significativement plus rapide plus de 1000 fois que les algorithmesstandards tels que libsvm svm-perf et cb-svm\n",
      "taaable  système de recherche et de création par adaptation de recettes de cuisine\n",
      "taaable is a textual case-based reasoning system that according to requested/forbiddeningredients dish types and/or dish origins retrieves cooking recipes if no recipe satisifies theconstraints taaable adapts existing recipes by replacing some ingredients by other ones\n",
      "traminer une librairie r pour analyse de données séquentielles\n",
      "nan\n",
      "un algorithme stable de décomposition pour analyse des réseaux sociaux dynamiques\n",
      "les réseaux dynamiques soulèvent de nouveaux problèmes analyses.un outils efficace analyse doit non seulement permettre de décomposerces réseaux en groupes éléments similaires mais il doit aussi permettre la détectionde changements dans le réseau nous présentons dans cet article une nouvelleapproche pour analyse de tels réseaux cette technique est basée sur unalgorithme de décomposition de graphe en groupes chevauchants ou chevauchement).la complexité de notre algorithme est o(|e| · deg2max +|v | · log(|v |))).la faible sensibilité de cet algorithme aux changements structuraux du réseaupermet en détecter les modifications majeures au cours du temps\n",
      "un critère évaluation bayésienne pour la construction arbres de décision\n",
      "nous présentons dans cet article un nouvel algorithme automatiquepour apprentissage arbres de décision nous abordons le problème selon uneapproche bayésienne en proposant sans aucun paramètre une expression ana-lytique de la probabilité un arbre connaissant les données nous transformonsle problème de construction de arbre en un problème optimisation  nousrecherchons dans espace des arbres de décision arbre optimum au sens ducritère bayésien ainsi défini c'est à dire arbre maximum a posteriori map).l'optimisation est effectuée en exploitant une heuristique de pré-élagage desexpérimentations comparatives sur trente bases de uci montrent que notreméthode obtient des performances prédictives proches de celles de état de arttout en étant beaucoup moins complexes\n",
      "un nouvel algorithme de forêts aléatoires arbres obliques particulièrement adapté à la classification de données en grandes dimensions\n",
      "algorithme des forêts aléatoires proposé par breiman permet ob-tenir de bons résultats en fouille de données comparativement à de nombreusesapproches cependant en n'utilisant qu'un seul attribut parmi un sous-ensembled'attributs tiré aléatoirement pour séparer les individus à chaque niveau de arbre,cet algorithme perd de l'information ceci est particulièrement pénalisant avecles ensembles de données en grandes dimensions où il peut exister de nom-breuses dépendances entre attributs nous présentons un nouvel algorithme deforêts aléatoires arbres obliques obtenus par des séparateurs à vaste marge(svm) la comparaison des performances de notre algorithme avec celles dealgorithme de forêts aléatoires des arbres de décision c4.5 et de algorithmesvm montre un avantage significatif de notre proposition\n",
      "un prototype cross-lingue multi-métiers  vers la gestion sémantique de contenu entreprise au service du collaboratif opérationnel\n",
      "le domaine « qualité hygiène sécurité et environnement »(qhse représente à heure actuelle un vecteur de progrès majeur pourl'industrie européenne le prototype « semantic quality environment » sqe)introduit dans cet article vise à démontrer la validité une architecturesémantique cross-lingue vouée à la collaboration multi-métiers et multilingue,dans le cadre un système banalisé de gestion de contenu entreprise dédié àl'industrie navale européenne\n",
      "un système pour extraction de corrélations linéaires dans des données de génomique médicale\n",
      "nan\n",
      "une méthode de classification supervisée sans paramètre pour apprentissage sur les grandes bases de données\n",
      "dans ce papier nous présentons une méthode de classification super-visée sans paramètre permettant attaquer les grandes volumétries la méthodeest basée sur des estimateurs de densités univariés optimaux au sens de bayes,sur un classifieur bayesien naïf amélioré par une sélection de variables et unmoyennage de modèles exploitant un lissage logarithmique de la distribution aposteriori des modèles nous analysons en particulier la complexité algorith-mique de la méthode et montrons comment elle permet analyser des bases dedonnées nettement plus volumineuses que la mémoire vive disponible nous pré-sentons enfin les résultats obtenu lors du récent pascal large scale learningchallenge où notre méthode a obtenu des performances prédictives de premierplan avec des temps de calcul raisonnables\n",
      "une nouvelle approche pour la classification non supervisée en segmentation image\n",
      "la segmentation des images en régions est un problème crucial pourl'analyse et la compréhension des images parmi les approches existantes pourrésoudre ce problème la classification non supervisée est fréquemment em-ployée lors une première étape pour réaliser un partitionnement de espacedes intensités des pixels qu'il s'agisse de niveaux de gris de couleurs ou de ré-ponses spectrales) puisqu'elle ignore complètement les notions de voisinagedes pixels une seconde étape analyse spatiale étiquetage en composantesconnexes par exemple est ensuite nécessaire pour identifier les régions issuesde la segmentation la non prise en compte de information spatiale est une li-mite majeure de ce type d'approche ce qui a motivé de nombreux travaux où laclassification est couplée à autres techniques pour s'affranchir de ce problème.dans cet article nous proposons une nouvelle formulation de la classificationnon supervisée permettant effectuer la segmentation des images sans faire ap-pel à des techniques supplémentaires plus précisément nous élaborons une mé-thode itérative de type k-means où les données à partitionner sont les pixels eux-mêmes et non plus leurs intensités et où les distances des points aux centresdes classes ne sont plus euclidiennes mais topographiques la segmentation estalors un processus itératif et à chaque itération les classes obtenues peuvent êtreassimilées à des zones influence dans le contexte de la morphologie mathéma-tique ce parallèle nous permet de bénéficier des algorithmes efficaces proposésdans ce domaine tels que ceux basés sur les files d'attente) tout en y ajoutantle caractère itératif des méthodes de classification non supervisée considéréesici nous illustrons finalement le potentiel de approche proposée par quelquesrésultats préliminaires de segmentation sur des images artificielles\n",
      "utilisation de analyse factorielle des correspondances pour la recherche images à grande échelle\n",
      "nous nous intéressons à utilisation de analyse factorielle des cor-respondances afc) pour la recherche images par le contenu dans une base dedonnées images volumineuse nous adaptons afc méthode originellementdéveloppée pour analyse des données textuelles (adt) aux images en utili-sant des descripteurs locaux sift en adt afc permet de réduire le nombrede dimensions et de trouver des thèmes ici afc nous permettra de limiter lenombre images à examiner au cours de la recherche afin accélérer le tempsde réponse pour une requête pour traiter de grandes bases images, nous pro-posons une version incrémentale de algorithme afc ce nouvel algorithmedécoupe une base images en blocs et les charge dans la mémoire un aprèsl'autre nous présentons aussi intégration des informations contextuelles e.g.la mesure de dissimilarité contextuelle jegou et al. 2007 dans notre structurede recherche images. cela améliore considérablement la précision nous ex-ploitons cette intégration dans deux axes i) hors ligne la structure de voisinageest corrigée hors ligne et ii) à la volée la structure de voisinage des images estcorrigée au cours de la recherche sur un petit ensemble images).\n",
      "vers la simulation et la détection des changements des données évolutives usage du web\n",
      "dans le domaine des flux des données la prise en compte du tempss'avère nécessaire pour analyse de ces données car leur distribution sous-jacentepeut changer au cours du temps un exemple typique concerne les modèles desprofils de navigation des internautes notre objectif est analyser évolutionde ces profils celle-ci peut être liée au changement effectifs ou aux déplacementde clusters au cours du temps afin analyser la validité de notre approche,nous mettons en place uneméthodologie pour la simulation des données usageà partir de laquelle il est possible de contrôler occurrence des changements\n",
      "vers le traitement à grande échelle de données symboliques\n",
      "nan\n",
      "vers une utilisation améliorée de relations spatiales pour apprentissage de données dans les modèles graphiques\n",
      "nous nous intéressons dans cet article aux représentations des relationsspatiales pour extraction information et la modélisation des donnéesvisuelles en particulier dans le contexte de la catégorisation d'images nousmontrons comment la prise en compte une relation spatiale entre deux élémentsentraîne apparition une information supplémentaire entre ces élémentset le reste de ensemble à modéliser ce qui est rarement exploité explicitement.une représentation floue des relations dans unmodèle graphique est bien adaptéepour les algorithmes apprentissage utilisés actuellement et permet intégrerce type information complémentaire qui concerne absence une interactionplutôt que sa présence nous tentons évaluer les bénéfices de cette approchesur un problème de traitement d'images\n",
      "a spatial rough set for extracting the periurban fringe\n",
      "to date the availability of spatial data is increasing together withtechniques and methods adopted in geographical analysis despite this tendency,classifying in a sharp way every part of the city is more and more complicated.this is due to the growth of city complexity rough set theory maybe a useful method to employ in combining great amounts of data in order tobuild complex knowledge about territory it represents a different mathematicalapproach to uncertainty by capturing the indiscernibility two differentphenomena can be indiscernible in some contexts and classified in the sameway when combining available information about them several experiencesexist in the use of rough set theory in data mining knowledge analysis andapproximate pattern classification but the spatial component lacks in all theseresearch streams.this paper aims to the use of rough set methods in geographical analyses.this approach has been applied in a case of study comparing the resultsachieved by means of both map algebra technique and spatial rough set thestudy case area potenza province is particularly suitable for the application ofthis theory because it includes 100 municipalities with a different number ofinhabitants and morphologic features\n",
      "algorithmes rapides de boosting de svm\n",
      "les algorithmes de boosting de newton support vector machine (nsvm) proximal support vector machine psvm) et least-squares support vector machine ls-svm) que nous présentons visent à la classification de très grands ensembles de données sur des machines standard nous présentons une extension des algorithmes de nsvm psvm et ls-svm pour construire des algorithmes de boosting a cette fin nous avons utilisé un terme de régularisation de tikhonov et le théorème sherman-morrison- woodbury pour adapter ces algorithmes au traitement ensembles de données ayant un grand nombre de dimensions nous les avons ensuite étendus par construction algorithmes de boosting de nsvm psvm et ls-svm afin de traiter des données ayant simultanément un grand nombre individus et de dimensions les performances des algorithmes sont évaluées sur des grands ensembles de données de uci comme adult kddcup 1999 forest covertype reuters-21578 et rcv1-binary sur une machine standard (pc-p4 2,4 ghz 1024 mo ram)\n",
      "analyse exploratoire opinions cinématographiques  co-clustering de corpus textuels communautaires\n",
      "les sites communautaires sont un endroit privilégié pour s'exprimer et publier des opinions le site www.flixster.com est un exemple de site participatif sur lequel se rassemblent plus de 20 millions de cinéphiles qui partagent des commentaires sur les films qu'ils ont ou non aimés explorer les contenus autoproduits est un challenge pour qui veut comprendre les attentes des internautes par une méthode apprentissage non supervisée nous montrerons qu'il est possible de mieux comprendre le vocabulaire utilisé pour décrire des opinions en particulier grâce à une méthode de co-clustering nous montrerons qu'un rapprochement peut être fait entre des films particuliers sur la base de usage un vocabulaire particulier analyse des résultats peut conduire à retrouver une certaine typologie de films ou encore des rapprochements entre films cette étude peut être complémentaire avec des analyses linguistiques des corpus ou encore être exploitée dans un contexte applicatif de recommandation de contenus multimédias\n",
      "apport des traitements morpho-syntaxiques pour alignement des définitions par une classification svm\n",
      "cet article propose une méthode alignement automatique de définitions destinée à améliorer la fusion entre des terminologies spécialisées et un vocabulaire médical généraliste par un classifieur de type svm support vecteur machine et une représentation compacte et pertinente un couple de définitions par concaténation un ensemble de mesures de similarité afin de tenir compte de leur complémentarité auquelle nous ajoutons les longueurs de chacune des définitions trois niveaux syntaxiques ont été investigués le modèle fondé sur un apprentissage à partir des groupes nominaux de type noms-adjectifs aboutit aux meilleures performances\n",
      "approche annotation automatique des événements\n",
      "quotidiennement plusieurs agences de presse publient des milliers articles contenant plusieurs événements de toutes sortes (politiques économiques culturels etc) les preneurs de décision se trouvent face à ce grand nombre événements dont seulement quelques uns les concernent le traitement automatique de tels événements devient de plus en plus nécessaires pour cela nous proposons une approche qui se base sur apprentissage automatique et qui permet annoter les articles de presse pour générer un résumé automatique contenant les principaux événements nous avons validé notre approche par le développement du système \"annotev\"\n",
      "approche hybride de classification à base de treillis de galois application à la reconnaissance de visages\n",
      "la recherche dans le domaine de la reconnaissance de visages profite des solutions obtenues dans le domaine de apprentissage automatique le problème de classification de visages peut être considéré comme un problème apprentissage supervisé où les exemples apprentissage sont les visages étiquetés notre article introduit dans ce contexte une nouvelle approche hybride de classification qui utilise le paradigme apprentissage automatique supervisé ainsi en se basant sur le fondement mathématique des treillis de galois et leur utilisation pour la classification supervisée nous proposons un nouvel algorithme de classification baptisé citrec ainsi que son application pour la reconnaissance de visages originalité de notre approche provient de la combinaison de analyse formelle de concepts avec les approches de classification supervisée à inférence bayésienne ou à plus proches voisins une validation expérimentale est décrite sur un benchmark du domaine de la reconnaissance de visages\n",
      "approches de type n-grammes pour analyse de parcours de vie familiaux\n",
      "cet article porte sur analyse de parcours de vie représentés sous forme de séquences d'événements plus spécifiquement on examine les possibilités exploiter des codages de type n-grammes de ces séquences pour en extraire des connaissances en fait compte tenu de la simultanéité de certains événements une procédure stricte de n-grammes comme on peut par exemple appliquer sur des textes n'est pas applicable ici nous discutons diverses alternatives qui s'avèrent finalement plus proches de la fouille de séquences fréquentes les concepts discutés sont illustrés sur des données de enquête biographique rétrospective réalisée par le panel suisse de ménages en 2002 enfin on précisera sur quels aspects approche proposée peut apporter un éclairage complémentaire utile par rapport à autres techniques plus classiques analyse exploratoire de parcours de vie\n",
      "assignation automatique de solutions à des classes de plaintes liées aux ambiances intérieures polluées\n",
      "nous présentons dans cet article un système informatique pour le traitement des plaintes en lien avec des situations de pollution domestique écrites en français après la construction automatique une base de scenarii de plaintes un module de recherche apparie la plainte à traiter à la thématique de la plainte la plus similaire enfin il s'agit assigner au problème courant la solution correspondante au scénario de pollution auquel est affectée la plainte pertinente nous montrons ici intérêt de introduction dans appariement des textes de aspect sémantique géré par un dictionnaire généraliste de synonymes et en quoi il n'est pas réalisable pour notre problème particulier de construire une ontologie\n",
      "binary block gtm  carte auto-organisatrice probabiliste pour les grands tableaux binaires\n",
      "ce papier présente un modèle génératif et son estimation permettant la visualisation de données binaires notre approche est basée sur un modèle de mélange de lois de bernoulli par blocs et les cartes de kohonen probabilistes la méthode obtenue se montre à la fois parcimonieuse et pertinente en pratique\n",
      "cas utilisation réelle de nautilus  calculs indicateurs chez un opérateur télécom\n",
      "nautilus est un logiciel analyse de bases de données le but de cette application est de généraliser utilisation de données clients au sein des entreprises elle facilite accès aux données en permettant de visualiser et manipuler les données du sgbd sous forme de concepts métiers elle inclut un générateur de requêtes sql et un outil de gestion de tâches désignées pour agrégation de grands volumes de données le principe de fonctionnement est basé sur enchaînement de phases permettant la création des données analyse  importation des métadonnées du sgbd  construction un dictionnaire de des concepts métiers  spécification des champs à calculer les différents traitements tels que les jointures et alimentation des tables sont optimisés afin de rendre application utilisable sur des sgbd entreprise\n",
      "classification adaptative de séries temporelles  application à identification des gènes exprimés au cours du cycle cellulaire\n",
      "ce travail s'inscrit dans le cadre de étude de la division cellulaire assurant la prolifération des cellules une meilleure compréhension de ce phénomène biologique nécessite identification des gènes caractérisant chaque phase du cycle cellulaire le procédé identification est généralement basé sur un ensemble de gènes dits gènes de référence sélectionnés expérimentalement et considérés comme caractérisant les phases du cycle cellulaire les niveaux expression des gènes étudiés sont mesurés durant le cycle de la division cellulaire et permettent de construire des profils expression. chaque gène étudié est affecté à la phase du cycle cellulaire correspondant au groupe de gènes de référence le plus similaire cette approche classique souffre de deux limites une part les mesures de proximité les plus couramment utilisés entre profils expression de gènes sont basées sur les écarts en valeurs sans tenir compte de la forme des profils autre part dans la littérature il n'y a pas consensus quant à ensemble des gènes de référence à considérer dans cet article notre but est de proposer une classification adaptative basée sur un indice de dissimilarité incluant les proximités en valeurs et en forme des profils expression de gènes permettant identifier les phases expression des gènes étudiés et de présenter un nouvel ensemble de gènes de référence validé par une connaissance biologique\n",
      "classification de documents en réseaux petits mondes en vue apprentissage\n",
      "nan\n",
      "clustering en haute dimension par accumulation de clusterings locaux\n",
      "le clustering est une tâche fondamentale de la fouille de données ces dernières années les méthodes de type cluster ensembles ont été objet une attention soutenue il s'agit agréger plusieurs clusterings un jeu de données afin obtenir un clustering \"moyen\" les clusterings individuels peuvent être le résultat de différents algorithmes ces méthodes sont particulièrement utiles lorsque la dimensionalité des données ne permet pas aux méthodes classiques basées sur la distance et/ou la densité de fonctionner correctement dans cet article nous proposons une méthode pour obtenir des clusterings individuels à faible coût à partir de projections partielles du jeu de données nous évaluons empiriquement notre méthode et la comparons à trois méthodes de différents types nous constatons qu'elle donne des résultats sensiblement supérieurs aux autres\n",
      "clustering visuel semi-supervisé pour des systèmes en coordonnées en étoiles 3d\n",
      "dans cet article nous proposons une approche qui combine les méthodes statistiques avancées et la flexibilité des approches interactives manuelles en clustering visuel nous présentons interface semi-supervised visual clustering (ssvc) sa contribution principale est apprentissage une métrique de projection optimale pour la visualisation en coordonnées en étoiles ainsi que pour extension 3d que nous avons développée la métrique de distance de projection est apprise à partir des retours de utilisateur soit en termes de similarité/ dissimilarité entre les items soit par annotation directe interface ssvc permet de plus une utilisation hybride dans laquelle un ensemble de paramètres sont manuellement fixés par utilisateur tandis que les autres paramètres sont déterminés par un algorithme de distance optimale\n",
      "co-classification sous contraintes par la somme des résidus quadratiques\n",
      "dans de nombreuses applications une co-classification est plus facile à interpréter qu'une classification mono-dimensionnelle il s'agit de calculer une bi-partition ou collection de co-clusters  chaque co-cluster est un groupe objets associé à un groupe attributs et les interprétations peuvent s'appuyer naturellement sur ces associations pour exploiter la connaissance du domaine et ainsi améliorer la pertinence des partitions plusieurs méthodes de classification sous contraintes ont été proposées pour le cas mono-dimensionnel e.g. exploitation de contraintes \"must-link\" et \"cannot-link\" nous considérons ici la co-classification sous contraintes avec la gestion de telles contraintes étendues aux dimensions des objets et des attributs mais aussi expression de contraintes de contiguité dans le cas de domaines ordonnés nous proposons un algorithme itératif qui minimise la somme des résidus quadratiques et permet exploitation active des contraintes spécifiées par les analystes nous montrons la valeur ajoutée de ce type extraction sur deux applications en analyse du transcriptome\n",
      "conception de systèmes information spatio-temporelle adaptatifs avec astis\n",
      "les avancées technologiques récentes du web et du sans fil,conjuguées au succès des applications spatialisées grand public sont àl'origine un accès accru aux systèmes information spatio-temporelle(sist par une grande diversité d'utilisateurs munis des dispositifs accèset dans des contextes utilisation variés adapter ces systèmes à utilisateurdevient donc une nécessité un gage utilisabilité et de pérennité cet articleprésente une approche générique pour la conception et la génération desystèmes information spatio-temporelle adaptés à l'utilisateur appeléastis astis offre des modalités générales de mise en oeuvre del'adaptation à l'utilisateur visant tant le contenu que la présentation desapplications elle permet aux concepteurs intégrer ces modalitésadaptation dans des applications traitant des données spatio-temporelles.afin de définir les besoins et types adaptation propres à leur application ilsuffit aux concepteurs de créer des modèles conceptuels par spécialisation etinstanciation des modèles offerts par notre architecture\n",
      "data mining for activity extraction in video data\n",
      "the exploration of large video data is a task which is now possible because of the advances made on object detection and tracking data mining techniques such as clustering are typically employed such techniques have mainly been applied for segmentation/indexation of video but knowledge extraction of the activity contained in the video has been only partially addressed in this paper we present how video information is processed with the ultimate aim to achieve knowledge discovery of people activity in the video first objects of interest are detected in real time then in an off-line process we aim to perform knowledge discovery at two stages 1 finding the main trajectory patterns of people in the video 2 finding patterns of interaction between people and contextual objects in the scene an agglomerative hierarchical clustering is employed at each stage we present results obtained on real videos of the torino metro (italy)\n",
      "découverte de motifs séquentiels et de règles inattendus\n",
      "les travaux autour de extraction de motifs séquentiels se sont particulièrement focalisés sur la définition approches efficaces pour extraire en fonction une fréquence d'apparition des corrélations entre des éléments dans des séquences même si ce critère de fréquence est déterminant le décideur est également de plus en plus intéressé par des connaissances qui sont représentatives un comportement inattendu dans ces données erreurs dans les données fraudes nouvelles niches  ) dans cet article nous introduisons le problème de la détection de motifs séquentiels inattendus par rapport aux croyances du domaine nous proposons approche user dont objectif est extraire les motifs séquentiels et les règles inattendues dans une base de séquences\n",
      "délestage pour analyse multidimensionnelle de flux de données\n",
      "dans le contexte de la gestion de flux de données les données entrent dans le système à leur rythme des mécanismes de délestage sont à mettre en place pour qu'un tel système puisse faire face aux situations où le débit des données dépasse ses capacités de traitement le lien entre réduction de la charge et dégradation de la qualité des résultats doit alors être quantifié dans cet article nous nous plaçons dans le cas où le système est un cube de données dont la structure est connue a priori alimenté par un flux de données nous proposons un mécanisme de délestage pour les situations de surcharge et quantifions la dégradation de la qualité des résultats dans les cellules du cube nous exploitons inégalité de hoeffding pour obtenir une borne probabiliste sur écart entre la valeur attendue et la valeur estimée\n",
      "détection de groupes atypiques pour une variable cible quantitative\n",
      "une tâche importante en analyse des données est la compréhension de comportements inattendus ou atypiques de groupes individus quelles sont les catégories individus qui gagnent de particulièrement forts salaires ou au contraire quelles sont celles qui ont de très faibles salaires  nous présentons le problème extraction de tels groupes atypiques vis-à-vis une variable cible quantitative comme par exemple la variable \"salaire\" et plus particulièrement pour les faibles et fortes valeurs un intervalle déterminé par l'utilisateur il s'agit donc de rechercher des conjonctions de variables dont la distribution diffère significativement de celle de ensemble apprentissage pour les faibles et fortes valeurs de intervalle de cette variable cible une adaptation une mesure statistique existante intensité d'inclination nous permet de découvrir de tels groupes atypiques cette mesure nous libère de étape de transformation des variables quantitatives à savoir étape de discrétisation suivie un codage disjonctif complet nous proposons donc un algorithme extraction de tels groupes avec des règles élagage pour réduire la complexité du problème cet algorithme a été développé et intégré au logiciel extraction de connaissances weka nous terminons par un exemple extraction sur la base de données ipums du bureau de recensement américain\n",
      "discretization of continuous features by resampling\n",
      "les arbres de décision sont largement utilisés pour générer des classificateurs à partir un ensemble de données le processus de construction est une partitionnement récursif de ensemble d'apprentissage dans ce contexte les attributs continus sont discrétisés il s'agit alors pour chaque variable à discrétiser de trouver ensemble des points de coupure dans ce papier nous montrons que la recherche des ces points de coupure par une méthode de ré-échantillonnage comme le bootstrap conduit à des meilleurs résultats nous avons testé cette approche avec les méthodes principales de discrétisation comme mdlpc fusbin fusinter contrast chi-merge et les résultats sont systématiquement meilleurs en utilisant le bootstrap nous exposons ces principaux résultats et ouvrons de nouvelles pistes pour la construction arbres de décision\n",
      "echantillonnage adaptatif de jeux de données déséquilibrés pour les forêts aléatoires\n",
      "dans nombre d'applications les données présentent un déséquilibre entre les classes la prédiction est alors souvent détériorée pour la classe minoritaire pour contourner cela nous proposons un échantillonnage guidé lors des itérations successives une forêt aléatoire par les besoins de l'utilisateur\n",
      "echantillonnage pour extraction de motifs séquentiels  des bases de données statiques aux flots de données\n",
      "depuis quelques années la communauté fouille de données s'est intéressée à la problématique de extraction de motifs séquentiels à partir de grandes bases de données en considérant comme hypothèse que les données pouvaient être chargées en mémoire centrale cependant cette hypothèse est mise en défaut lorsque les bases manipulées sont trop volumineuses dans cet article nous étudions une technique échantillonnage basée sur des réservoirs et montrons comment cette dernière est particulièrement bien adaptée pour résumer de gros volumes de données nous nous intéressons ensuite à la problématique plus récente de la fouille sur des données disponibles sous la forme un flot continu et éventuellement infini \"data stream\") nous étendons approche échantillonnage à ce nouveau contexte et montrons que nous sommes à même extraire des motifs séquentiels de flots tout en garantissant les taux erreurs sur les résultats les différentes expérimentations menées confirment nos résultats théoriques\n",
      "echantillonnage spatio-temporel de flux de données distribués\n",
      "ces dernières années sont apparues de nombreuses applications utilisant des données potentiellement infinies provenant de façon continue de capteurs distribués on retrouve ces capteurs dans des domaines aussi divers que la météorologie établir des prévisions) le domaine militaire surveiller des zones sensibles) analyse des consommations électriques transmettre des alertes en cas de consommation anormale), pour faire face à la volumétrie et au taux arrivée des flux de données des traitements sont effectués à la volée sur les flux en particulier si le système n'est pas assez rapide pour traiter toutes les données un flux il est possible de construire des résumés de l'information cette communication a pour objectif de faire un premier point sur nos travaux échantillonnage dans un environnement de flux de données fortement distribués notre approche est basée sur la théorie des sondages analyse des données fonctionnelles et la gestion de flux de données cette approche sera illustrée par un cas réel  celui des mesures de consommations électriques\n",
      "enhancing personal file retrieval in semantic file systems with tag-based context\n",
      "recently tagging systems are widely used on the internet on desktops tags are also supported by some semantic file systems and desktop search tools in this paper we focus on personal tag organization to enhance personal file retrieval our approach is based on the notion of context a context is a set of tags assigned to a file by a user based on tag popularity and relationships between tags our proposed algorithm creates a hierarchy of contexts on which a user can navigate to retrieve files in an effective manner\n",
      "étude comparative de deux approches de classification recouvrante  moc vs okm\n",
      "la classification recouvrante désigne les techniques de regroupements de données en classes pouvant s'intersecter particulièrement adaptés à des domaines application actuels (eg recherche d'information bioinformatique quelques modèles théoriques de classification recouvrante ont été proposés très récemment parmi lesquels le modèle moc banerjee et al 2005a)) utilisant les modèles de mélanges et approche okm cleuziou 2007)) consistant à généraliser algorithme des k-moyennes la présente étude vise une part à étudier les limites théoriques et pratiques de ces deux modèles et autre part à proposer une formulation de approche okm en terme de modèles de mélanges gaussiens laissant ainsi entrevoir des perspectives intéressantes quant à la variabilité des schémas de recouvrements envisageables\n",
      "étude de interaction entre variables pour extraction des règles influence\n",
      "cet article présente une méthode efficace pour extraction de règles influence quantitatives positives et négatives ces règles influence introduisent une nouvelle sémantique qui vise à faciliter analyse un volume important de données cette sémantique fixe la direction de la règle entre deux variables en positionnant au préalable une comme étant influent et autre comme étant l'influé elle permet de ce fait exprimer la nature de influence  positive en maximisant le nombre éléments en commun ou négative en maximisant le nombre éléments qui violent l'influé notre approche s'appuie sur une stratégie qui comporte cinq étapes dont deux exécutées en parallèle ces deux étapes constituent les étapes clé de notre approche la première combine une méthode élagage et de regroupement tabulaire basée sur les tableaux de contingence cette dernière construit et classe les zones potentiellement intéressantes la seconde injecte la sémantique et évalue le degré influence que produirait introduction une nouvelle variable sur un ensemble de variables en utilisant une nouvelle mesure d'intérêt influence. cette étape vient affiner les résultats de la première étape et permet de se focaliser sur des zones valides par rapport aux contraintes spécifiées enfin un système de règles influence jugées intéressantes est construit basé sur la juxtaposition des résultats des deux étapes clé de notre approche\n",
      "évaluation des critères asymétriques pour les arbres de décision\n",
      "pour construire des arbres de décision sur des données déséquilibrées des auteurs ont proposés des mesures entropie asymétriques le problème de évaluation de ces arbres se pose ensuite cet article propose évaluer la qualité arbres de décision basés sur une mesure entropie asymétrique\n",
      "explsa  utilisation informations syntaxico-sémantiques associées à lsa pour améliorer les méthodes de classification conceptuelle\n",
      "analyse sémantique latente lsa - latent semantic analysis est aujourd'hui utilisée dans de nombreux domaines comme la modélisation cognitive les applications éducatives mais aussi pour la classification approche présentée dans cet article consiste à ajouter des informations grammaticales à lsa différentes méthodes pour exploiter ces informations grammaticales sont étudiées dans le cadre une tâche de classification conceptuelle\n",
      "extraction itemsets compacts\n",
      "extraction itemsets fréquents est un sujet majeur de ecd et son but est de découvrir des corrélations entre les enregistrements un ensemble de données cependant le support est calculé en fonction de la taille de la base dans son intégralité dans cet article nous montrons qu'il est possible de prendre en compte des périodes difficiles à déceler dans organisation des données et qui contiennent des itemsets fréquents sur ces périodes nous proposons ainsi la définition des itemsets compacts qui représentent un comportement cohérent sur une période spécifique et nous présentons algorithme deico qui permet leur découverte\n",
      "extraction un modèle numérique de terrain à partir de photographies par drone\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dans le suivi et la modélisation de érosion en montagne lareprésentation fine du relief est une composante importante en effet laconnaissance des zones de concentration des eaux notamment à traversl'apparition de rigoles élémentaires est fondamentale pour bien décrire lesconnectivités entre les zones de mobilisation des sédiments sur le versant et leréseau hydrographique stabilisé la résolution au sol permise par lesphotographies aériennes classiques ne permet pas accéder à unereprésentation 3d suffisamment fine des ravines élémentaires nous testonsl'utilisation de photographies stéréoscopiques à résolution centimétrique prisesà basse altitude par un drone pour obtenir un mnt précis la question majeureconcerne les règles à suivre pour un meilleur compromis entre précision etfacilité d'élaboration et évaluation de importance relative de chaque étapesur la qualité finale de la restitution la zone étude est située dans lesbadlands de draix alpes de haute provence)\n",
      "extraction de motifs séquentiels multidimensionnels clos sans gestion ensemble de candidats\n",
      "extraction de motifs séquentiels permet de découvrir des corrélations entre événements au cours du temps introduisant plusieurs dimensions d'analyse les motifs séquentiels multidimensionnels permettent de découvrir des motifs plus pertinents mais le nombre de motifs obtenus peut devenir très important c'est pourquoi nous proposons dans cet article de définir une représentation condensée garantie sans perte information  les motifs séquentiels multidimensionnels clos extraits ici sans gestion ensemble de candidats\n",
      "extraction et exploitation des annotations contextuelles\n",
      "dans la perspective offrir un web sémantique des travaux ont cherché à automatiser extraction des annotations sémantiques à partir de textes pour représenter au mieux la sémantique que vise à transmettre une page web dans cet article nous proposons une approche extraction des annotations qui représentent le plus précisément possible le contenu un document nous proposons de prendre en compte la notion de contexte modélisé par des relations contextuelles émanant à la fois de la structure et de la sémantique du texte\n",
      "extraction et validation par croisement des relations une ontologie de domaine\n",
      "nan\n",
      "fiasco  un nouvel algorithme extraction itemsets fréquents dans les flots de données\n",
      "nous présentons dans cet article un nouvel algorithme permettant la construction et la mise à jour incrémentale du fia  fiasco notre algorithme effectue un seul passage sur les données et permet de prendre en compte les nouveaux batches itemset par itemset et pour chaque itemset item par item\n",
      "fouille de données audio pour la classification automatique de mots homophones\n",
      "cet article présente une contribution à la modélisation acoustique des mots à partir de grands corpus oraux faisant appel aux techniques de fouilles de données en transcription automatique de nombreuses erreurs concernent des mots fréquents homophones deux paires de mots quasi-)homophones à/a et et/est sont sélectionnées dans les corpus pour lesquels sont définis et examinés 41 descripteurs acoustiques permettant potentiellement de les distinguer 17 algorithmes de classification mis à épreuve pour la discrimination automatique de ces deux paires de mots donnent en moyenne 77% de classification correcte sur les 5 meilleurs algorithmes en réduisant le nombre de descripteurs à 10 sélectionnés par algorithme le plus performant) les résultats de classification restent proches du résultat obtenu avec 41 attributs cette comparaison met en évidence le caractère discriminant de certains attributs qui pourront venir enrichir à la fois la modélisation acoustique et nos connaissances des prononciations de l'oral\n",
      "from mining the web to inventing the new sciences underlying the internet\n",
      "as the internet continues to change the way we live find information communicate and do business it has also been taking on a dramatically increasing role in marketing and advertising unlike any prior mass medium the internet is a unique medium when it comes to interactivity and offers ability to target and program messaging at the individual level coupled with its uniqueness in the richness of the data that is available for measurability in the variety of ways to utilize the data and in the great dependence of effective marketing on applications that are heavily data-driven makes data mining and statistical data analysis modeling and reporting an essential mission-critical part of running the on-line business however because of its novelty and the scale of data sets involved few companies have figured out how to properly make use of this data in this talk i will review some of the challenges and opportunities in the utilization of data to drive this new generation of marketing systems i will provide several examples of how data is utilized in critical ways to drive some of these capabilities the discussion will be framed with themore general framework of grand challenges for data mining  pragmatic and technical i will conclude this presentation with a consideration of the larger issues surrounding the internet as a technology that is ubiquitous in our lives yet one where very little is understood at the scientific level in defining and understanding many of the basics the internet enables  community personalization and the new microeconomics of the web this leads to an overview of the new yahoo  research organization and its aims  inventing the new sciences underlying what we do on the internet focusing on areas that have received little attention in the traditional academic circles some illustrative examples will be reviewed to make the ultimate goals more concrete\n",
      "génération de séquence résumé par une nouvelle approche basée sur le soft computing\n",
      "cet article propose une approche abstraction des séquences vidéo basée sur le soft computing etant donné une longueur cible du condensé vidéo on cherche les segments vidéo qui couvrent le maximum du visuel de la vidéo originale en respectant la longueur du condensé\n",
      "gradients de prototypicalité conceptuelle et lexicale\n",
      "longtemps les ontologies ont été limitées à des domaines scientifiques et techniques favorisant au passage essor du concept de « connaissances universelles et objectives » avec émergence et engouement actuel pour les sciences cognitives couplés à application des ontologies à des domaines relatifs aux sciences humaines et sociales (shs) la subjectivité des connaissances devient une dimension incontournable qui se doit être intégrée et prise en compte dans le processus ingénierie ontologique (io) objectif de nos travaux est de développer la notion ontologie pragmatisée vernaculaire de domaine (opvd) le principe sous-jacent à de telles ressources consiste à considérer que chaque ontologie est non seulement propre à un domaine mais également à un endogroupe donné doté une pragmatique qui est fonction tant de la culture que de apprentissage et de état émotionnel du dit endogroupe cette pragmatique qui traduit un processus appropriation et de personnalisation de ontologie considérée est qualifiée à aide de deux mesures  un gradient de prototypicalité conceptuelle et un gradient de prototypicalité lexicale\n",
      "hypersmooth  calcul et visualisation de cartes de potentiel interactives\n",
      "le groupe de recherche hypercarte propose hypersmooth,un nouvel outil cartographique pour analyse spatiale de phénomènessociaux économiques mettant en oeuvre une méthode de calcul depotentiel objectif est de pouvoir représenter de façon continue et enchangeant échelle analyse une information statistiqueéchantillonnée sur toutes sortes de maillages réguliers ou non le défitechnologique est de fournir un outil accessible sur le web interactifet rapide ceci malgré le coût élevé du calcul et qui assure laconfidentialité des données nous présentons notre solution basée surune architecture client serveur  le serveur calcule les cartes depotentiel en utilisant des techniques optimisation particulières alorsque le client est en charge de la visualisation et du paramétrage deanalyse, et les deux parties communiquent via un protocole web\n",
      "industrialiser le data mining  enjeux et perspectives\n",
      "informatique décisionnelle est un secteur en forte croissance dans toutes les entreprises les techniques classiques reporting simple & olap) qui s'intéressent essentiellement à présenter les données sont aujourd'hui très largement déployées le data mining commence à se répandre apportant des capacités de prévision à forte valeur ajoutée pour les entreprises les plus compétitives ce développement est rendu possible par la disponibilité croissante de masses de données importantes et la puissance de calcul dorénavant disponible cependant la mise en ijuvre industrielle des projets de data mining pose des contraintes tant théoriques quels algorithmes utiliser pour produire des modèles analyses exploitant des milliers de variables pour des millions d'exemples qu'opérationnelles comment mettre en production et contrôler le bon fonctionnement de centaines de modèles) je présenterai ces contraintes issues des besoins des entreprises  je montrerai comment exploiter des résultats théoriques provenant des travaux de vladimir vapnik pour produire des modèles robustes  je donnerai des exemples applications réelles en gestion de la relation client et en analyse de qualité je conclurai en présentant quelques perspectives utilisation du texte et des réseaux sociaux)\n",
      "intégration de contraintes dans les cartes auto-organisatrices\n",
      "le travail présenté dans cet article décrit une nouvelle version des cartes topologiques que nous appelons crtm cette version consiste à modifier algorithme de kohonen de telle façon à ce qu'il contrôle les violations des contraintes lors de la construction de la topologie de la carte nous validons notre approche sur des données connues de la littérature en utilisant des contraintes artificielles une validation supplémentaire sera faite sur des données réelles issues images médicales pour la classification des mélanomes chez humain sous contraintes médicales\n",
      "intégration de la structure dans un modèle probabiliste de document\n",
      "en fouille de textes comme en recherche d'information différents modèles de type probabiliste vectoriel ou booléen se sont révélés bien adaptés pour représenter des documents textuels mais ces modèles présentent inconvénient de ne pas tenir compte de la structure du document or la plupart des informations disponibles aujourd'hui sur internet ou dans des bases documentaires sont fortement structurées dans cet article nous proposons étendre le modèle probabiliste de représentation des documents de façon à tenir compte du poids une certaine catégorie éléments structurels  les balises représentant la structure logique et la structure de mise en forme ce modèle a été évalué à aide de la collection de la campagne évaluation inex 2006\n",
      "interprétation automatique itinéraires à partir un corpus de récits de voyages pilotée par un usage pédagogique\n",
      "de larges corpus à fort ancrage territorial deviennent disponibles sousforme numérique dans les médiathèques et plus particulièrement dans les médiathèquesde dimension régionale les défis qu'offrent ces gigas octets de documentsbruts sont énormes en terme de traitement automatique des contenus.nous proposons dans cet article deux modèles computationnels et une méthodecomplète permettant de réaliser un traitement automatique afin extraire des itinérairesdans des textes relatant des récits de voyage le premier modèle est unmodèle des attendus il s'intéresse au concept itinéraire et adopte le point devue du pédagogue et fait intervenir très tôt les usages envisagés le deuxièmemodèle est un modèle d'extraction il permet de modéliser expression du déplacementdans des textes du genre récit de voyage nous proposons alors uneméthode automatique pour  une part extraire et interpréter automatiquementles déplacements un récit et autre part passer des déplacements à itinéraire,c'est-à-dire alimenter de manière automatique le modèle des attendus à partir dumodèle d'extraction nous montrons également comment les itinéraires extraitsinterviennent soit dans la phase de construction activités pédagogiques soitdirectement comme matériau dans une activité d'apprentissage nous présentonsenfin ¼r un prototype pour interprétation itinéraires dans des récitsde voyages qui implémente notre approche il prend en entrée un texte brut etfournit interprétation de itinéraire décrit dans le texte il permet également devisualiser sur un fond cartographique itinéraire extrait\n",
      "interprétation images basée sur une approche évolutive guidée par une ontologie\n",
      "les approches de fouille et interprétation images consistant à considérer les pixels de façon indépendante ont montré leurs limites pour analyse images complexes pour résoudre ce problème de nouvelles méthodes s'appuient sur une segmentation préalable de image qui consiste en une agrégation des pixels connexes afin de former des régions homogènes au sens un certain critère cependant le lien est souvent complexe entre la connaissance de expert sur les objets qu'il souhaite identifier dans image et les paramètres nécessaires à étape segmentation permettant de les identifier dans cet article la connaissance de expert est modélisée dans une ontologie qui est ensuite utilisée pour guider un processus de segmentation par une approche évolutive cette méthode trouve automatiquement des paramètres de segmentation permettant identifier les objets décrits par expert dans l'ontologie\n",
      "khiops outil de préparation et modélisation des données pour la fouille des grandes bases de données\n",
      "khiops est un outil de préparation des données et de modélisation pour apprentissage supervisé et non supervisé outil permet évaluer de façon non paramétrique la corrélation entre tous types de variables dans le cas non supervisé et importance prédictive des variables et paires de variables dans le cas de la classification supervisée ces évaluations sont effectuées au moyen de modèles de discrétisation dans le cas numérique et de groupement de valeurs dans le cas catégoriel ce qui permet de rechercher une représentation des données efficace au moyen un recodage des variables outil produit également un modèle de scoring pour les tâches apprentissage supervisé selon un classifieur bayesien naif avec sélection de variables et moyennage de modèles outil est adapté à analyse des grandes bases de données avec des centaines de milliers individus et des dizaines de milliers de variables et a permis de participer avec succès à plusieurs challenges internationaux récents\n",
      "intelligence collective géospatiale au service du diagnostic de territoire  geodoc\n",
      "le diagnostic de territoire constitue une étape obligatoire dans toutprojet aménagement ou dans toute volonté politique de modifier durablementl'espace les décideurs politiques doivent avoir une vision objective des actionsà mener en fondant leurs réflexions sur des études et des documents ;qu'ils soient à caractère géographique ou non il est donc fondamentald'améliorer accès et la consultation par les décideurs stratégiques de ce quel'on peut appeler des documents géographiques le but de cet article est deprésenter certains concepts et solutions technologiques qui peuvent être utilisésafin de mieux organiser de naviguer dans) et de visualiser ces documents ilpropose une mise en perspective commune de certaines de ces approches surlaquelle est fondée la conception une première maquette un outil de visualisation(et de navigation de documents géographiques nommé geodoc\n",
      "la prise en compte de la dimension temporelle dans la classification de données\n",
      "dans un contexte ingénierie de la connaissance analyse des données relationnelles évolutives est une question centrale la représentation de ce type de données sous forme de graphe optimisé en facilite analyse et interprétation par utilisateur non expert cependant ces graphes peuvent rapidement devenir trop complexes pour être étudiés dans leur globalité il faut alors les décomposer de manière à en faciliter la lecture et analyse. pour cela une solution est de les simplifier dans un premier temps en un graphe réduit dont les sommets représentent chacun un groupe distinct de sommets  acteurs ou termes du domaine étudié dans un second temps il faut les décomposer en instances un graphe par période afin de prendre en compte la dimension temporelle.la plateforme de veille stratégique tétralogie développée dans notre laboratoire permet de synthétiser les données relationnelles évolutives sous forme de matrices de cooccurrence 3d et visugraph son module de visualisation permet de les représenter sous forme de graphes évolutifs.visugraph assimile les différentes périodes à des repères temporels et chaque sommet est placé en fonction de son degré appartenance aux différentes périodes ce prototype est aussi doté un module de la classification interactive de données relationnelles basé sur une technique de markov clustering qui conduit à une visualisation sous forme de graphe réduit nous proposons ici de prendre en compte la dimension temporelle dans notre processus de classification des données ainsi par la visualisation successive des différentes instances il devient plus facile analyser évolution des classes au niveau intra mais aussi au niveau inter classes\n",
      "le fia un nouvel automate permettant extraction efficace itemsets fréquents dans les flots de données\n",
      "le fia frequent itemset automaton est un nouvel automate qui permet de traiter de façon efficace la problématique de extraction des itemsets fréquents dans les flots de données cette structure de données est très compacte et informative et elle présente également des propriétés incrémentales intéressantes pour les mises à jour avec une granularité très fine algorithme développé pour la mise à jour du fia effectue un unique passage sur les données qui sont prises en compte tout abord par batch (i.e. itemset par itemset) puis pour chaque itemset item par item nous montrons que dans le cadre une approche prédictive et par intermédiaire de la bordure statistique le fia permet indexer les itemsets véritablement fréquents du flot en maximisant le rappel et en fournissant à tout moment une information sur la pertinence statistique des itemsets indexés avec la p-valeur\n",
      "le forage de réseaux sociaux\n",
      "exploitation des réseaux sociaux pour extraction de connaissances n'est pas nouvelle les anthropologues sociologues et épidémiologies se sont déjà penchés sur la question c'est probablement le succès du moteur de recherche google qui a vulgarisé utilisation des parcours aléatoires des réseaux sociaux pour ordonnancement par pertinence plusieurs applications ont depuis vu naissance la découverte des communautés dans les réseaux sociaux est aussi une nouvelle tendance de recherche très prisée durant cet exposé nous parlerons de analyse des réseaux sociaux la découverte de communautés et présenterons quelques applications dont ordonnancement dans les bases de données\n",
      "le logiciel sodas  avancées récentes un outil pour analyser et visualiser des données symboliques\n",
      "nan\n",
      "les cartes cognitives hiérarchiques\n",
      "une carte cognitive fournit une représentation graphique un réseau influence entre des concepts les cartes cognitives de dimensions importantes ont inconvénient être difficiles à appréhender interpréter et exploiter cet article présente un modèle de cartes cognitives hiérarchiques permettant au concepteur effectuer des regroupements de concepts qui sont ensuite utilisés dans un mécanisme permettant à utilisateur obtenir des vues partielles et synthétiques une carte\n",
      "mesures hiérarchiques pondérées pour évaluation un système semi-automatique annotation de génomes utilisant des arbres de décision\n",
      "annotation une protéine consiste entre autres à lui attribuer une classe dans une hiérarchie fonctionnelle celle-ci permet organiser les connaissances biologiques et utiliser un vocabulaire contrôlé pour estimer la pertinence des annotations des mesures telles que la précision le rappel la spécificité et le fscore sont utilisées cependant ces mesures ne sont pas toujours bien adaptées à évaluation de données hiérarchiques car elles ne permettent pas de distinguer les erreurs faites aux différents niveaux de la hiérarchie nous proposons ici une représentation formelle pour les différents types erreurs adaptés à notre problème\n",
      "méthodologie evaluation intelligente des concepts ontologiques\n",
      "un des problèmes majeurs dans la gestion des ontologies est son évaluation cet article traite évaluation des concepts ontologiques qui sont extraits de pages web pour cela nous avons proposé une méthodologie évaluation des concepts basée trois critères révélateurs  \"le degré de crédibilité\" \"le degré de cohésion\" et \"le degré d'éligibilité\" chaque critère correspond à un apport de connaissance pour la tâche évaluation notre méthode évaluation assure une évaluation qualitative grâce aux associations de mots ainsi qu'une évaluation quantitative par le biais des trois degrés nos résultats et discussions avec les experts et les utilisateurs ont montré que notre méthode facilite la tâche évaluation\n",
      "méthodologie de définition de e-services pour la gestion des connaissances à partir un plateau de créativité  application au e-learning instrumental\n",
      "en s'appuyant sur la théorie de l'activité nous avons mis au point une méthodologie de gestion des connaissances à base de e-services sur un plateau de créativité visant à faire piloter le processus de fabrication métier par celui des usages nous avons testé avec la réalisation un e-service apprentissage instrumental de pièces de musique à la guitare (e-guitare)\n",
      "mining implications from lattices of closed trees\n",
      "we propose a way of extracting high-confidence association rules from datasets consisting of unlabeled trees the antecedents are obtained through a computation akin to a hypergraph transversal whereas the consequents follow from an application of the closure operators on unlabeled trees developed in previous recent works of the authors we discuss in more detail the case of rules that always hold independently of the dataset since these are more complex than in itemsets due to the fact that we are no longer working on a lattice\n",
      "modélisation conceptuelle des trajectoires\n",
      "une perception intelligente du mouvement objets mobiles(personnes voitures colis etc. est à la base de nombreuses applications parexemple le suivi une distribution postale à travers le monde optimisation dutrafic routier ou étude de la migration d'animaux) les systèmes de gestion debases de données actuels n'offrent ni les concepts ni les fonctions nécessaires àune analyse sémantique du mouvement se limitant au stockage et àl'interrogation de positions spatiales individuelles hors contexte temporel destravaux de recherche précédents ont introduit et développé le concept objetmobile ou spatio-temporel dans cet article nous allons plus loin en proposantle concept de trajectoire comme unité sémantique de mouvement sur laquellese construit la vision applicative nous proposons de décrire les trajectoires auniveau conceptuel avec leurs aspects géométriques temporels et sémantiqueset leurs composants structurels  point de départ point d'arrivée arrêts etdéplacements intermédiaires chaque élément trajectoire arrêt déplacementvoire partie de déplacement peut recevoir des annotations sémantiques sousforme de valeurs attributs ou de liens vers des objets de la base approchede modélisation décrite dans cet article est basée sur les patrons demodélisation qui permettent une solution générique pour modéliser lescaractéristiques standard des trajectoires tout en étant ouverte auxcaractéristiques spécifiques à application envisagée enfin implémentationdans une base de données relationnelle étendue est présentée\n",
      "nouvelle approche pour la recherche images par le contenu\n",
      "on utilise analyse factorielle des correspondances afc) pour la recherche images par le contenu en s'inspirant directement de son utilisation en analyse des données textuelles (adt) afc permet ici de réduire les dimensions du problème et de sélectionner des indicateurs pertinents pour la recherche par le contenu en adt afc est appliquée à un tableau de contingence croisant mots et documents la première étape consiste donc à définir des « mots visuels » dans les images analogue des mots dans les textes) ces mots sont construits à partir des descripteurs locaux sift) des images la méthode a été testée sur la base caltech4 sivic et al. 2005 sur laquelle elle fournit de meilleurs résultats qualité des résultats de recherche et temps d'exécution que des méthodes plus classiques comme tf*idf/rocchio (rocchio 1971 ou plsa (hofmann 1999a 1999b) enfin pour passer à échelle et améliorer la qualité de recherche nous proposons un nouveau prototype de recherche qui utilise des fichiers inversés basés sur la qualité de représentation des images sur les axes après avoir fait une afc chaque fichier inversé est associé à une partie un axe positive ou négative et contient des images ayant une bonne qualité de représentation sur cet axe les tests réalisés montrent que ce nouveau prototype réduit le temps de recherche sans perte de qualité de résultat et dans certains cas améliore le taux de précision par rapport à la méthode exhaustive\n",
      "ontologies et raisonnement à partir de cas  application à analyse des risques industriels\n",
      "analyse de risques est un processus visant à décrire les scénarios conduisant à des phénomènes dangereux et à des accidents potentiels sur une installation industrielle pour réaliser une analyse de risques un expert dispose de nombreuses ressources  rapports études de dangers bases d'accidents etc ces ressources sont cependant souvent difficiles à exploiter parce qu'elles ne sont pas suffisamment structurées ni formalisées dans le cadre du projet kmgr knowledge management pour la gestion des risques) mené en partenariat avec institut national de environnement industriel et des risques (ineris) nous proposons de traiter ce problème en développant un système de recherche information basé sur des ontologies et de le compléter par un système de raisonnement à partir de cas ràpc) pour tenir compte des expériences passées\n",
      "optimisation du primal pour les svm\n",
      "apprentissage de svm par optimisation directe du primal est très étudié depuis quelques temps car il ouvre de nouvelles perspectives notamment pour le traitement de données structurées nous proposons un nouvel algorithme de ce type qui combine de façon originale un certain nombre de techniques et idées comme la méthode du sous-gradient optimisation de fonctions continues non partout différentiables et une heuristique de shrinking\n",
      "optimisation incrémentale de réseaux de neurones rbf pour la régression via un algorithme évolutionnaire  rbf-gene\n",
      "les réseaux de neurones rbf sont excellents régresseurs ils sont cependant difficiles à utiliser en raison du nombre de paramètres libres  nombre de neurones poids des connexions  des algorithmes évolutionnaires permettent de les optimiser mais ils sont peu nombreux et complexes.nous proposons ici un nouvel algorithme rbf-gene qui permet optimiser la structure et les poids du réseau grâce à une inspiration biologique il est compétitif avec les autres techniques de régression mais surtout évolution peut choisir dynamiquement le nombre de neurones et la précision des différents paramètres\n",
      "pondération locale des variables en apprentissage numérique non-supervisé\n",
      "dans cet article nous proposons une nouvelle approche de pondérations des variables durant un processus apprentissage non supervisé cette méthode se base sur algorithme « batch » des cartes auto-organisatrices estimation des coefficients de pondération se fait en parallèle avec la classification automatique ces pondérations sont locales et associées à chaque référent de la carte auto-organisatrice elles reflètent importance locale de chaque variable pour la classification les pondérations locales sont utilisées pour la segmentation de la carte topologique permettant ainsi un découpage plus riche tenant compte des pertinences des variables les résultats de évaluation montrent que approche proposée comparée à autres méthodes de classification offre une segmentation plus fine de la carte et de meilleure qualité\n",
      "prétraitement des bases de données de réactions chimiques pour la fouille de schémas de réactions\n",
      "un grand nombre de réactions chimiques sont aujourd'hui répertoriées dans des bases de données les chimistes aimeraient pouvoir fouiller les graphes moléculaires contenus dans ces données pour en extraire des schémas de réactions fréquents deux obstacles s'opposent à cela  une part la manière dont les chimistes représentent les réactions par des graphes ne permet pas aux techniques de fouille de graphes extraire les schémas de réactions fréquents autre part les bases de données contiennent des descriptions de réactions souvent incomplètes ambiguës ou erronées le présent article décrit un processus de prétraitement opérationnel qui permet de filtrer compléter puis transformer le contenu une base de réactions en des données fiables constituées de graphes abstraits répondant au problème de la fouille de schémas de réactions le processus place ainsi les bases de réactions à portée des techniques de fouille de graphes comme en attestent les résultats expérimentaux\n",
      "principes analyse des données symboliques et application à la détection anomalies sur des ouvrages publics\n",
      "analyse des données symboliques a pour objectif de fournir des résultatscomplémentaires à ceux fournis par la fouille de données classique encréant des concepts issus de données simples ou complexes puis en analysantces concepts par des descriptions symboliques où les variables expriment lavariation des instances de ces concepts en prenant des valeurs intervalle histogrammesuites munies de règles et de taxonomies etc\n",
      "processus acquisition un dictionnaire de sigles et de leurs définitions à partir un corpus\n",
      "le logiciel présenté dans cet article s'appuie sur une approche acquisition de sigles à partir de données textuelles\n",
      "proposition une nouvelle approche de détection intrusions basée sur les règles associatives génériques de classification\n",
      "les systèmes de détection intrusions sdis) ont pour objectif la sécurité des réseaux informatiques dans ce papier nous proposons une nouvelle approche de détection intrusions basée sur des règles associatives génériques de classification pour améliorer la qualité de la détection intrusions.\n",
      "proposition pour intégration de analyse spatiale et de analyse multidimensionnelle\n",
      "introduction de information spatiale dans les modèlesmultidimensionnels a donné naissance au concept de spatial olap solap).dans cet article nous montrons en quoi les spécificités de informationgéographique et de analyse spatiale ne sont pas entièrement prises en comptedans analyse et les modèles multidimensionnels solap pour pallier ceslimites nous proposons le concept de dimension géographique et décrivons lesdifférents types de hiérarchies associées nous proposons introduction denouveaux opérateurs qui permettent adapter les opérateurs analyse spatialeau paradigme multidimensionnel enfin nous présentons notre prototype quioffre une interface web de navigation spatiale et multidimensionnelle etpermet intégration de ces nouveaux concepts\n",
      "recherche adaptative de structures de régulation génétique\n",
      "nous avons proposé un algorithme original de fouille de données licorn afin inférer des relations de régulation coopérative à partir de données d'expression licorn donne de bons résultats s'il est appliqué à des données de levure mais le passage à échelle sur des données plus complexes (e.g. humaines est difficile dans cet article nous proposons une extension de licorn afin qu'il puisse gérer une contrainte de co-régulation adaptative une évaluation préliminaire sur des données de transcriptome de tumeurs de vessie montre que les réseaux significatifs sont obtenus à aide une contrainte de corégulation adaptative de manière beaucoup plus efficace et qu'ils ont des performances de prédiction équivalentes voire meilleures que celles obtenues par licorn\n",
      "recherche images par noyaux sur graphes de régions\n",
      "dans le cadre de la recherche interactive images dans une base de données nous nous intéressons à des mesures de similarité image qui permettent améliorer apprentissage et utilisables en temps réel lors de la recherche les images sont représentées sous la forme de graphes adjacence de régions floues pour comparer des graphes valués nous employons des noyaux de graphes s'appuyant sur des ensembles de chaînes extraites des graphes comparés nous proposons un cadre général permettant emploi de différents noyaux et différents types de chaînes(sans cycle avec boucles autorisant des appariements inexacts nous avons effectué des comparaisons sur deux bases issues de columbia et caltech et montré que des chaînes de très faible dimension longueur inférieur à 3 sont les plus efficaces pour retrouver des classes d'objets\n",
      "recherche information personnalisée dans les bibliothèques numériques scientifiques\n",
      "dans cet article nous présentons nos travaux sur la recherche information personnalisée dans les bibliothèques numériques nous utilisons des profils utilisateurs qui représentent des intérêts et des préférences des utilisateurs les résultats de recherche peuvent être retriés en tenant compte des besoins informations spécifiques de différentes personnes ce qui donne une meilleure précision nous étudions différentes méthodes basées sur les citations sur le contenu textuel des documents et des approches hybrides les résultats des expérimentations montrent que nos approches sont efficaces et applicables dans le cadre des bibliothèques numériques\n",
      "recherche de motifs spatio-temporels de cas atypiques pour le trafic routier urbain\n",
      "un large panel de domaines application utilise des réseaux de capteurs géoréférencés pour mesurer divers évènements les séries temporelles fournies par ces réseaux peuvent être utilisées dans le but de dégager des connaissances sur les relations spatio-temporelles de activité mesurée dans cet article nous proposons une méthode permettant abord de détecter des situations atypiques au sens de l'occurrence puis de construire des motifs spatio-temporels relatant leur propagation sur un réseau le cas étudié est celui du trafic routier urbain notre raisonnement se fonde sur application de la méthode space-time principal component analysis stpca) et de la combinaison entre information mutuelle et algorithme isomap les résultats expérimentaux exécutés sur des données réelles de trafic routier démontrent efficacité de la méthode introduite à identifier la propagation de cas atypiques fournissant ainsi un outil performant de prédiction de la circulation intraday à court et moyen terme\n",
      "requêtes alternatives dans le contexte un entrepôt de données génomiques\n",
      "afin aider les biologistes à annoter des génomes ce qui nécessite l'analyse le croisement et la comparaison de données provenant de sources diverses nous avons conçu un entrepôt de données de génomique microbienne nous présentons la structure globale flexible de entrepôt et son architecture multi-niveaux et définissons des correspondances entre ces niveaux nous introduisons ensuite la notion de requête alternative et montrons comment le système peut construire ensemble des requêtes alternatives à une requête initiale pour cela nous introduisons un mécanisme interrogation qui repose sur architecture multi-niveaux et donnons un algorithme de calcul des requêtes alternatives\n",
      "segmentation hiérarchique des cartes topologiques\n",
      "dans ce papier nous présentons une nouvelle mesure de similarité pour la classification des référents de la carte auto-organisatrice qui sera réalisée à aide une nouvelle approche de classification hiérarchique 1) la mesure de similarité est composée de deux termes  la distance de ward pondérée et la distance euclidienne pondérée par la fonction de voisinage sur la carte topologique 2) un algorithme à base de fourmis artificielles nommé anttree sera utilisé pour segmenter la carte auto-organisatrice.cet algorithme a avantage de prendre en compte le voisinage entre les référents et de fournir une hiérarchie des référents avec une complexité proche du nlog(n) la segmentation incluant la nouvelle mesure est validée sur plusieurs bases de données publiques\n",
      "semantics of spatial window over spatio-temporal data stream\n",
      "dans les systèmes dsms data stream management systems) les données en entrée sont infinies et les requêtes sur celles-ci sont actives tout le temps dans le but de satisfaire ces caractéristiques le fenêtrage temporel est largement utilisée pour convertir le flux infini de données sous forme de relations finies mais cette technique est inadaptée pour de nombreuses applications émergentes en particulier les services de localisation de nombreuses requêtes ne peuvent pas être traitées en utilisant le fenêtrage temporel ou seraient traitées plus e\u000ecacement à aide un fenêtrage basé sur espace fenêtrage spatial) dans cet article nous analysons la nécessité un fenêtrage spatial sur des flux de données spatio-temporels et proposons sur la base du langage de requêtes cql continuous query language) une syntaxe et une sémantique associées au fenêtrage spatial\n",
      "sémantique et réutilisation ontologie générique\n",
      "dans ce papier nous enrichissons la méthode terminae de construction ontologie à partir de textes en proposant une semi-automatisation de la construction du modèle conceptuel nous présentons un algorithme permettant la conceptualisation un terme en s'appuyant sur les informations linguistiques contenues dans ontologie générique de référence\n",
      "som pour la classification automatique non supervisée de documents textuels basés sur wordnet\n",
      "dans cet article nous proposons la méthode des som cartes auto-organisatrices de kohonen pour la classification non supervisée de documents textuels basés sur les n-grammes la même méthode basée sur les synsets de wordnet comme termes pour la représentation des documents est étudiée par la suite ces combinaisons sont évaluées et comparées\n",
      "stratégies de classification non supervisée basées sur fenêtres superposées  application aux données usage du web\n",
      "un problème majeur se pose dans le domaine des flux de données  la distribution sous-jacente des données peut changer sur le temps dans cet article nous proposons trois stratégies de classification non supervisée basée sur des fenêtres superposées notre objectif est de pouvoir repérer ces changements dans le temps notre approche est appliquée sur un benchmark de données réelles et les conclusions obtenues sont basées sur deux indices de comparaison de partitions\n",
      "structure inference of bayesian networks from data a new approach based on generalized conditional entropy\n",
      "we propose a novel algorithm for extracting the structure of a bayesian network from a dataset our approach is based on generalized conditional entropies a parametric family of entropies that extends the usual shannon conditional entropy our results indicate that with an appropriate choice of a generalized conditional entropy we obtain bayesian networks that have superior scores compared to similar structures obtained by classical inference methods\n",
      "suppression des itemsets clés non essentiels en classification basée sur les règles association\n",
      "en classification basée sur les règles d'association les itemsets clés sont essentiels  la suppression des itemsets non clés n'affecte pas la précision du classifieur en construction ce travail montre que parmi ces itemsets clés on peut s'intéresser seulement à ceux de petites tailles plus loin encore il étudie une généralisation une propriété importante des itemsets non clés et montre que parmi les itemsets clés de petites tailles il y a ceux qui ne sont pas significatifs pour la classification ces itemsets clés sont dits non essentiels ils sont définis via un test de \u001f2 les expériences menées sur les grands jeux de données montrent que optimisation par la suppression de ces itemsets est correcte et efficace\n",
      "système multi-agent argumentatif pour la classification des connaissances cruciales\n",
      "dans cet article nous proposons une approche multi-agent argumentative permettant automatiser la résolution des conflits entre décideurs dans un système aide à identification des connaissances cruciales nommé k-dss en effet des divergences concernant la crucialité des connaissances peuvent apparaître entre les décideurs et aboutir ainsi à des incohérences dans la base commune de connaissances la rendant inexploitable notre objectif à travers ce travail est de proposer une approche argumentative permettant de résoudre les conflits entre décideurs afin de concevoir cette approche nous nous appuyons sur la théorie multi-agents pour représenter les acteurs humains par des agents logiciels connaissant leurs préférences et leurs règles de décision et pouvant ainsi argumenter leurs choix ou mettre à jour leurs croyances en fonction des arguments qu'ils reçoivent des autres agents décideurs\n",
      "un algorithme de classification topographique non supervisée à deux niveaux simultanés\n",
      "une des questions les plus importantes pour la plupart des applications réelles de la classification est de déterminer un nombre approprié de groupes (clusters) déterminer le nombre optimal de groupes est un problème difficile puisqu'il n'y a pas de moyen simple pour connaître ce nombre sans connaissance a priori dans cet article nous proposons un nouvel algorithme de classification non supervisée à deux niveaux appelé s2l-som simultaneous twolevel clustering - self organizing map) qui permet de déterminer automatiquement le nombre optimal de groupes pendant apprentissage une carte auto-organisatrice estimation du nombre correct de groupes est en relation avec la stabilité de la segmentation et la validité des groupes générés pour mesurer cette stabilité nous utilisons une méthode de sous-échantillonnage le principal avantage de algorithme proposé comparé aux méthodes classiques de classification est qu'il n'est pas limité à la détection de groupes convexes mais est capable de détecter des groupes de formes arbitraires la validation expérimentale de cet algorithme sur un ensemble de problèmes fondamentaux pour la classification montre sa supériorité sur les méthodes standards de classification à deux niveaux comme som+k-moyennes et som+hierarchical- agglomerative-clustering\n",
      "un cyber cartogramme gravitationnel pour analyse visuelle de données spatiotemporelles complexes\n",
      "le cartogramme présenté dans cet article est destiné à faciliterl'analyse visuelle de données spatiotemporelles complexes pour cela il offrela possibilité de représenter simultanément les trois dimensions nécessaires àtoute forme analyse géographique que sont les dimensions spatiale où),thématique quoi) et temporelle (quand) à partir de trois composantes principales 1) une représentation unidimensionnelle 1d) de espace géographiquede forme semi-circulaire centrée sur une origine (ex le canada  2) desentités géographiques (ex pays qui viennent graviter autour de cette origineen fonction de valeurs attributaires  et 3) une ligne de temps interactive permettantd'explorer la dimension temporelle de information représentée lacombinaison de ces trois composantes offre de multiples potentialités pourl'analyse spatio-temporelle de différentes formes de proximités qu'elles soientéconomiques culturelles sociales ou démographiques les fonctionnalités etpotentialités de ce cartogramme développé en source ouverte sont illustrées àpartir exemples issus de atlas cybercartographique du commerce canadien.cet article reprend les grandes lignes une communication présentée lors de laconférence sageo 2007\n",
      "un modèle espace vectoriel de concepts pour noyaux sémantiques\n",
      "les noyaux ont été largement utilisés pour le traitement de données textuelles comme mesure de similarité pour des algorithmes tels que les séparateurs à vastemarge (svm) le modèle de espace vectoriel vsm) a été amplement utilisé pour la représentation spatiale des documents cependant le vsm est une représentation purement statistique dans ce papier nous présentons un modèle espace vectoriel de concepts cvsm) qui se base sur des connaissances linguistiques a priori pour capturer le sens des documents nous proposons aussi un noyau linéaire et un noyau latent pour cet espace le noyau linéaire exploite les concepts linguistiques pour extraction du sens alors que le noyau latent combine les concepts statistiques et linguistiques en effet le noyau latent utilise des concepts latents extraits par analyse sémantique latente lsa) dans le cvsm les noyaux sont évalués sur une tâche de catégorisation de texte dans le domaine biomédical le corpus ohsumed bien connu pour sa difficulté de catégorisation a été utilisé les résultats ont montré que les performances de catégorisation sont améliorées dans le csvm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "un modèle et une algèbre pour les systèmes de gestion ontologies\n",
      "nous présentons ici une approche pour la gestion de bases ontologies basée sur un modèle comprenant outre la définition formelle des concepts sous forme axiomes de logique de description) autres éléments descriptifs (termes commentaires et arguments) ainsi que leurs liens alignement avec des concepts autres ontologies adaptation ou la combinaison ontologies se font grâce à une algèbre comprenant des opérations telles que la sélection la projection union ou la jointure ontologies ces opérations agissent au niveau des axiomes des éléments descriptifs et des liens alignement.\n",
      "un nouveau système immunitaire artificiel pour apprentissage non supervisé\n",
      "nous proposons dans ce papier un nouveau système immunitaire artificiel sia) appelé système nk pour la détection de comportement du soi non soi avec une approche non supervisée basée sur le mécanisme de cellule nk naturel killer) dans ce papier le système nk est appliqué à la détection de fraude en téléphonie mobile\n",
      "un processus acquisition information pour les besoins de enrichissement des bdg\n",
      "les données constituent élément central un système information géographiques sig) et leur coût est souvent élevé en raison de investissement substantiel qui permet leur production cependant ces données sont souvent restreintes à un service ou pour une catégorie d'utilisateurs ce qui a fait ressortir la nécessité de proposer des moyens enrichissement en informations pertinentes pour un nombre plus important d'utilisateurs nous présentons dans ce papier notre approche enrichissement de données qui se déroule selon trois étapes  une identification de segments et de thèmes associés une délégation et enfin un filtrage textuel un processus de raffinement est également offert notre approche globale a été intégrée à un sig son évaluation a été accomplie montrant ainsi sa performance\n",
      "un système de vote pour la classification de textes opinion\n",
      "les tâches de classification textuelle ont souvent pour objectif de regrouper thématiquement différents textes dans cet article nous nous sommes intéressés à la classification de documents en fonction des opinions et jugements de valeurs qu'ils contiennent approche proposée est fondée sur un système de vote utilisant plusieurs méthodes de classification\n",
      "une aide à la découverte de mappings dans somerdfs\n",
      "dans cet article nous nous intéressons à la découverte de mises en correspondance entre ontologies distribuées modélisant les connaissances de pairs du système de gestion de données p2p somerdfs plus précisément nous montrons comment exploiter les mécanismes de raisonnement mis en oeuvre dans somerdfs pour aider à découvrir des mappings entre ontologies ce travail est réalisé dans le cadre du projet mediad en partenariat avec france telecom r&d\n",
      "une approche ensembliste inspirée du boosting en classification non supervisée\n",
      "en classification supervisée de nombreuses méthodes ensemblistes peuvent combiner plusieurs hypothèses de base afin de créer une règle de décision finale plus performante ainsi il a été montré que des méthodes comme le bagging ou le boosting pouvaient se révéler intéressantes tant dans la phase apprentissage qu'en généralisation dès lors il est tentant de vouloir s'inspirer des grands principes une méthode comme le boosting en classification non supervisée or il convient préalablement de se confronter aux difficultés connues de la thématique des ensembles de regroupeurs correspondance des classes agrégation des résultats qualité puis introduire idée du boosting dans un processus itératif cet article propose une méthode ensembliste inspirée du boosting qui à partir un partitionnement flou obtenu par les c-moyennes floues (fuzzy-c-means) va insister itérativement sur les exemples difficiles pour former une partition dure finale plus pertinente\n",
      "une approche ontologique pour automatiser le contrôle de conformité dans le domaine du bâtiment\n",
      "cet article présente la méthode et le système c3r pour vérifier de façon semi-automatique la conformité un projet de construction par rapport à des normes du bâtiment les projets de construction sont représentés par des graphes rdf et les normes par des requêtes sparql  le processus de contrôle consiste en appariement des requêtes et des graphes son efficacité repose sur acquisition de connaissances ontologiques et sur un processus extraction de connaissances guidé par ce but spécifique de contrôle de conformité qui prend en compte les connaissances ontologiques acquises elle repose ensuite sur des méta-connaissances acquises auprès des experts du cstb qui permettent de guider le contrôle lui-même  les requêtes représentant les normes sont annotées et organisées selon ces annotations ces annotations sont également utilisées dans les interactions avec utilisateur de c3r pour expliquer les résultats du processus de validation en particulier en cas d'échec\n",
      "une j-mesure orientée pour élaguer des modèles de chroniques\n",
      "nan\n",
      "une mesure de similarité contextuelle pour aide à la navigation dans un treillis\n",
      "la recherche information et la navigation dans les pages web s'avèrent complexes du fait du volume croissant des données et de leur manque de structure la formalisation conceptuelle un contexte associé à une ontologie rend possible amélioration de ce processus nous définissons un contexte conceptuel comme étant association un treillis de concepts construit à partir de pages web avec des ontologies la recherche et la navigation peuvent alors s'effectuer à plusieurs niveaux abstraction  le niveau des données le niveau conceptuel et le niveau sémantique cet article s'intéresse essentiellement au niveau conceptuel grâce à une représentation par les treillis de concepts des documents selon les termes qu'ils ont en commun notre objectif est de proposer une mesure de similarité permettant à utilisateur de mieux naviguer dans le treillis en effet une bonne interprétation du treillis devrait passer par un choix rigoureux des concepts objets relations et propriétés les plus intéressants pour faciliter la navigation il faut pouvoir indiquer à utilisateur les concepts les plus pertinents par rapport au concept correspondant à sa requête ou pouvoir lui proposer un point de départ originalité de notre proposition réside dans le fait de considérer un lien sémantique entre les concepts du treillis basé sur une extension des mesures de similarité utilisées dans le cadre des ontologies afin de permettre une meilleure exploitation de ce treillis nous présentons les résultats expérimentaux de application de cette mesure sur des treillis construits à partir de pages web dans le domaine du tourisme\n",
      "une nouvelle approche du boosting face aux données bruitées\n",
      "la réduction de erreur en généralisation est une des principales motivations de la recherche en apprentissage automatique de ce fait un grand nombre de travaux ont été menés sur les méthodes agrégation de classifieurs afin d'améliorer par des techniques de vote les performances un classifieur unique parmi ces méthodes agrégation, le boosting est sans doute le plus performant grâce à la mise à jour adaptative de la distribution des exemples visant à augmenter de façon exponentielle le poids des exemples mal classés cependant en cas de données fortement bruitées cette méthode est sensible au sur-apprentissage et sa vitesse de convergence est affectée dans cet article nous proposons une nouvelle approche basée sur des modifications de la mise à jour des exemples et du calcul de erreur apparente effectuées au sein de algorithme classique adaboost une étude expérimentale montre intérêt de cette nouvelle approche appelée approche hybride face à adaboost et à brownboost une version adaboost adaptée aux données bruitées\n",
      "une nouvelle méthode divisive en classification non supervisée pour des données symboliques intervalles\n",
      "dans cet article nous présentons une nouvelle méthode de classification non supervisée pour des données symboliques intervalles il s'agit de extension une méthode de classification non supervisée classique à des données intervalles la méthode classique suppose que les points observés sont la réalisation un processus de poisson homogène dans k domaines convexes disjoints de rp la première partie de la nouvelle méthode est une procédure monothétique divisive la règle de coupure est basée sur une extension à des données intervalles du critère de classification des hypervolumes étape élagage utilise un test statistique basé sur le processus de poisson homogène le résultat est un arbre de décision la seconde partie de la méthode consiste en une étape de recollement qui permet dans certains cas améliorer la classification obtenue à la fin de la première partie de l'algorithme la méthode est évaluée sur un ensemble de données réelles\n",
      "une proposition pour extraction de relations non prédicatives\n",
      "les relations sémantiques généralement reconnues par les méthodes extraction sont portées par des structures de type prédicats-arguments or information recherchée est souvent répartie sur plusieurs phrases pour détecter ces relations dites complexes nous proposons un modèle de représentation des connaissances basé sur les graphes conceptuels\n",
      "utilisation du web sémantique pour la gestion une liste de diffusion une cop\n",
      "cet article décrit une approche de création semi-automatique ontologies et annotations sémantiques à partir de messages électroniques échangés dans une liste de diffusion dédiée au support informatique les ressources sémantiques générées permettront identifier les questions fréquemment posées faq) à travers une recherche guidée par cette ontologie\n",
      "vers des machines à vecteurs de support “actionnables”  une approche fondée sur le classement\n",
      "une des principales critiques que on puisse faire aux séparateurs à vaste marge svm) est le manque intelligibilité des résultats en effet il s'agit une technique \"boite noire\" qui ne fournit pas explications ni indices quant aux raisons une classification les résultats doivent être pris tels quels en faisant confiance au système qui les a produits pourtant selon notre expérience pratique les experts du domaine préfèrent largement une méthode apprentissage avec explications et recommandation actions plutôt qu'une boite noire aussi performante et prédictive soit-elle dans cette thématique nous proposons une nouvelle approche qui consiste a rendre les svm plus \"actionnables\" ce but est atteint en couplant des modèles de classement des résultats des svm à des méthodes apprentissage de concepts nous présentons une application de notre méthode sur diverses données dont des données médicales concernant des patients de l'athérosclérose nos résultats empiriques semblent très prometteurs et montrent utilité de notre approche quant à intelligibilité et actionnabilité des résultats produits par svm\n",
      "vers exploitation de grandes masses de données\n",
      "une tendance lourde depuis la fin du siècle dernier est augmentation exponentielle du volume des données stockées cette augmentation ne se traduit pas nécessairement par une information plus riche puisque la capacité à traiter ces données ne progresse pas aussi rapidement avec les technologies actuelles un difficile compromis doit être trouvé entre le coût de mise en oeuvre et la qualité de information produite nous proposons une approche industrielle permettant augmenter considérablement notre capacité à transformer des données en information grâce à automatisation des traitements et à la focalisation sur les seules données pertinentes\n",
      "vers intégration de la prédiction dans les cubes olap\n",
      "nan\n",
      "vers une fouille sémantique des brevets  application au domaine biomédical\n",
      "les brevets sont une source information très riche puisque ce sont des documents qui servent à décrire les inventions accès aux documents de brevets en ligne est possible grâce aux efforts des offices nationaux de la propriété intellectuelle par ailleurs ayant des objectifs différents la présentation de ces documents a pris des formes variées loin être unifiées ce papier présente une méthode et un système permettant analyse de brevets \"patent mining\" pour générer des annotations sémantiques idée principale est de pouvoir prendre en considération la structure des brevets pour pouvoir trouver un lien entre le contenu du brevet et les concepts des différentes ontologies\n",
      "visualisation des motifs séquentiels extraits à partir un corpus en ancien français\n",
      "cet article présente une interface permettant de visualiser des motifs séquentiels extraits à partir de données textuelles en ancien français\n",
      "visualisation et classification des parcours de vie\n",
      "cet article propose une méthodologie pour la visualisation et la classification des parcours de vie plus spécifiquement nous considérons les parcours de vie individus suisses nés durant la première moitié du xxème siècle en utilisant les données provenant de enquête biographique rétrospective menée en 2002 par le panel suisse de ménages nous nous sommes concentrés sur ces événements du parcours de vie  le départ du foyer parental la naissance du premier enfant le premier mariage et le premier divorce a partir des données de base sur ces événements nous discutons de leur transformation en séquences d'états nous présentons ensuite notre méthodologie pour extraire de la connaissance des parcours de vie cette méthodologie repose sur des distances calculées par un algorithme optimal matching ces distances sont ensuite utilisées pour la classification des parcours de vie et leur visualisation à aide de techniques de « multi dimensional scaling » cet article s'intéresse en particulier aux problématiques entourant application de ces méthodes aux données de parcours de vie\n",
      "web content data mining  la classification croisée pour analyse textuelle un site web\n",
      "notre objectif dans cet article est analyse textuelle un site web indépendamment de son usage notre approche se déroule en trois étapes la première étape consiste au typage des pages afin de distinguer les pages de navigation ou pages « auxiliaires » des pages de contenu la deuxième étape consiste au prétraitement du contenu des pages de contenu afin de représenter chaque page par un vecteur de descripteurs la dernière étape consiste au block clustering ou la classification simultanée des lignes et des colonnes de la matrice croisant les pages aux descripteurs de pages afin de découvrir des biclasses de pages et de descripteurs application de cette approche au site de tourisme de metz prouve son efficacité et son applicabilité ensemble de classes de pages groupés en thèmes facilitera analyse ultérieure de usage du site\n",
      "alignement de ressources sémantiques à partir de règles\n",
      "ce papier présente une approche automatique pour aligner des ressources sémantiques alignement se traduit par la mise en correspondance des entités (termes concepts rôles appartenant à des ressources un même domaine qui peuvent avoir des niveaux de formalisation différents les entités correspondantes sont de même nature et un coefficient caractérise leur degré de ressemblance.l'approche proposée est fondée sur des règles appariement entre les entités des deux ressources dans une première phase ces règles appariement sont identifiées empiriquement des algorithmes combinant les différentes règles identifiées sont ensuite définis afin établir des correspondances entre les entités des ressources considérées.ce papier présente un ensemble de règles appariement exploitant des éléments situés à différents niveaux conceptuels cet ensemble constitue un cadre pour alignement automatique des ressources sémantiques les résultats une première expérimentation qui a porté sur alignement de deux ressources du domaine de accidentologie sont également présentés\n",
      "annotation et navigation de données archéologiques\n",
      "dans cet article nous proposons un cadre et un outil pour annotation et la navigation de données archéologiques objectif principal est de structurer les annotations de façon à permettre une navigation incrémentale où utilisateur peut à partir un ensemble objets initialement retournés par une requête découvrir des liens approximatifs avec autres objets de la base approche a été implémentée et est en cours de validation\n",
      "annotation sémantique floue de tableaux guidée par une ontologie\n",
      "nous présentons dans cet article différentes étapes de annotation de tableaux de données à aide une ontologie tout d'abord nous distinguons les colonnes de données numériques et symboliques les données symboliques sont ensuite annotées de manière floue à aide des termes de l'ontologie cette annotation nous permet de déduire le type des colonnes de données symboliques pour trouver le type des colonnes de données numériques nous utilisons à la fois le titre de la colonne et les valeurs numériques et unités présentes dans la colonne chaque étape de notre annotation est validée expérimentalement\n",
      "application des réseaux bayésiens à analyse des facteurs impliqués dans le cancer du nasopharynx\n",
      "apprentissage de la structure des réseaux bayésien à partir de données est un problème np-difficile une nouvelle heuristique de complexité polynômiale intitulée polynomial max-min skeleton (pmms) a été proposée en 2005 par tsamardinos et al et validée avec succès sur de nombreux bancs essai pmms présente en outre avantage être performant avec des jeux de données réduits néanmoins comme tous les algorithmes sous contraintes celui-ci échoue lorsque des dépendances fonctionnelles déterministes) existent entre des groupes de variables il ne s'applique par ailleurs qu'aux données complètes aussi dans cet article nous apportons quelques modifications pour remédier à ces deux problèmes après validation sur le banc essai asia nous appliquons aux données une étude épidémiologique cas-témoins du cancer du nasopharynx npc) de 1289 observations 61 variables et 5% de données manquantes issues un questionnaire objectif est de dresser un profil statistique type de la population étudiée et apporter un éclairage utile sur les différents facteurs impliqués dans le npc\n",
      "apport du web sémantique dans la réalisation un moteur de recherche géo-localisé à usage des entreprises\n",
      "la recherche une entreprise sur le web relative à un savoir-faire particulier n'est pas une tâche toujours facile à mener les outils mis à la disposition de internaute ne donnent pas entièrement satisfaction un côté les moteurs de recherche éprouvent des difficultés à faire ressortir clairement le résultat escompté de autre côté les annuaires spécialisés type pages jaunes sont tributaires une organisation figée nuisant à leur efficacité face à ce constat nous nous proposons de créer un nouveau moteur spécialisé dans la recherche d'entreprise associant web sémantique et géo-localisation cette approche novatrice nécessite implémentation une ontologie ayant pour objectif la formalisation des connaissances du domaine cette tâche a mis en évidence intérêt des structures économiques maintenues par l'insee et leur utilisation au sein de l'ontologie les nomenclatures économiques ont été retenues pour gérer la classification des activités et produits pouvant être dispensés par les entreprises la structure des unités administratives telle que gérée au sein du fichier sirene s'est avérée judicieuse pour répondre à la problématique de géo-localisation des entreprises une opération de désambiguïsation est réalisée en associant à chaque noeud activité les mots clés et synonymes lui correspondant enfin nous comparons les résultats obtenus par notre moteur à ceux obtenu par le principal moteur de recherche activités géo-localisées en france  les pages jaunes que ce soit au niveau de la précision et du rappel notre moteur obtient des résultats significativement meilleurs\n",
      "apprentissage actif émotions dans les dialogues homme-machine\n",
      "la prise en compte des émotions dans les interactions homme-machine permet de concevoir des systèmes intelligents capables de s'adapter aux utilisateurs les techniques de redirection appels dans les centres téléphoniques automatisés se basent sur la détection des émotions dans la parole les principales difficultés pour mettre en oeuvre de tels systèmes sont acquisition et étiquetage des données apprentissage cet article propose application de deux stratégies apprentissage actif à la détection émotions dans des dialogues en interaction homme-machine étude porte sur des données réelles issues de utilisation un serveur vocal et propose des outils adaptés à la conception de systèmes automatisés de redirection appels.\n",
      "apprentissage semi-supervisé de fonctions ordonnancement\n",
      "nous présentons dans cet article un algorithme inductif semi-supervisé pour la tâche ordonnancement bipartite les algorithmes semi–supervisés proposés jusqu'à maintenant ont été étudiés dans le cadre strict de la classification récemment des travaux ont été réalisés dans le cadre transductif pour étendre les modèles existants en classification au cadre ordonnancement. originalité de notre approche est qu'elle est capable inférer un ordre sur une base test non– utilisée pendant la phase d'apprentissage ce qui la rend plus générique qu'une méthode transductive pure les résultats empiriques sur la base cacm contenant les titres et les résumés du journal communications of the association for computer machinery montrent que les données non–étiquetées sont bénéfiques pour apprentissage de fonctions ordonnancement.\n",
      "apprentissage statistique de la topologie un ensemble de données étiquetées\n",
      "découvrir la topologie un ensemble de données étiquetées dans un espace euclidien peut aider à construire un meilleur système de décision dans ce papier nous proposons un modèle génératif basé sur le graphe de delaunay de plusieurs prototypes représentant les données étiquetées dans le but extraire de ce graphe la topologie des classes\n",
      "approche connexionniste pour extraction de profils cas-témoins du cancer du nasopharynx à partir des données issues une étude épidémiologique\n",
      "dans cet article nous présentons un système de découverte de connaissances à partir de données issues une étude épidémiologique cas-témoins du cancer du nasopharynx (npc) ces données étant obtenues par une collecte de questionnaires elles ont une part la particularité être qualitatives et autre part de présenter des valeurs manquantes prenant en compte ces deux dernières contraintes le système que nous proposons suit une démarche exploration de données qui consiste à 1) définir une procédure de codage des données qualitatives en présence de valeurs manquantes  2) étudier les propriétés de algorithme des cartes auto-organisatrices de kohonen et son adaptation à ce type de données dans un cadre de découverte et de visualisation de groupes homogènes des cas cancer / non-cancer  3) post-traiter le resultat de cet algorithme par une classification automatique pour optimiser le nombre de groupes ainsi trouvés et 4) donner une interprétation sémantique des profils extraits de chaque groupe objectif général de cette étude est éclater le profil statistique global de la population étudiée en un ensemble de profils types cancer ou non-cancer et extraire pour chaque profil ensemble de variables explicatives du npc à partir une cartographie bidimensionnelle\n",
      "approche logique pour la réconciliation de références\n",
      "le problème de réconciliation de références consiste à décider si deux descriptions provenant de sources distinctes réfèrent ou non à la même entité du monde réel dans cet article nous étudions ce problème quand le schéma des données est décrit en rdfs étendu par certaines primitives de owl-dl nous décrivons et montrons intérêt une approche logique basée sur des règles de réconciliation qui peuvent être générées automatiquement à partir des axiomes du schéma ces règles traduisent de façon déclarative les dépendances entre réconciliations qui découlent de la sémantique du schéma les premiers résultats ont été obtenus sur des données réelles dans le cadre du projet picsel 3 en collaboration avec france telecom r&d\n",
      "calcul et représentation efficace de cubes de données pour une visualisation orientée pixel\n",
      "les cubes de données fournissent une aide non négligeable lorsqu'il s'agit interroger des entrepôts de données un cube de données représente un pré-calcul de toutes les requêtes olap et ainsi améliore leur temps de réponses les approches proposées jusqu'à présent réduisent les temps de calcul et entrée sortie mais leur utilisation reste très coûteuse autres travaux de recherche se sont intéressés à la visualisation de données pour les exploiter de façon interactive.nous proposons une adaptation de la représentation condensée des cubes de données basée sur le modèle partitionnel cette technique nous permet de calculer efficacement un cube de données et de représenter les liens entre les données pour la visualisation la visualisation proposée dans cet article est basée sur des techniques de visualisation orientée pixel et sur des techniques de diagramme de liens entre noeuds pour offrir à la fois une vision globale et locale pour l'exploitation cette nouvelle approche utilise une part les calculs efficaces de cubes de données et autre part les techniques avancées de visualisation\n",
      "caractérisation des transitions temporisées dans les logs de conversation de services web\n",
      "la connaissance du protocole de conversation un service web est importante pour les utilisateurs et les fournisseurs car il en modélise le comportement externe  mais il n'est souvent pas spécifié lors de la conception notre travail s'inscrit dans une thématique extraction du protocole de conversation un service existant à partir de ses données d'exécution nous en étudions un sous-problème important qui est la découverte des transitions temporisées (ie les changements état liés à des contraintes temporelles) nous proposons un cadre formel aboutissant à la définition des expirations propres qui représentent un équivalent dans les logs des transitions temporisées a notre connaissance ceci représente la première contribution à la résolution de ce problème\n",
      "cartographie de organisation  une approche topologique des connaissances\n",
      "la gestion des connaissances est devenue aujourd'hui un enjeu majeur pour toute organisation celle-ci a pour but de capitaliser et de rendre accessible à ses acteurs la connaissance détenue par organisation cet article s'intéresse particulièrement à la visualisation à deux niveaux de ces connaissances macroscopique - relatif aux connaissances globales détenues par organisation - et microscopique – relatif aux connaissances locales détenues par chaque membre organisationnel) la caractérisation des connaissances détenues par les acteurs repose sur quatre dimensions complémentaires (formelle conative cognitive et socio-cognitive) les deux types de visualisation proposés s'appuient sur les cartes auto-organisatrices et permettent une navigation dans différentes représentations des connaissances de organisation\n",
      "choix des conclusions et validation des règles issues arbres de classification\n",
      "cet article traite de la validation de règles dans un contexte de ciblage où il s'agit de déterminer les profils type des différentes valeurs de la variable à prédire les concepts de analyse statistique implicative fondée sur la différence entre nombre observé de contre-exemples et nombre moyen que produirait le hasard s'avèrent particulièrement bien adaptés à ce contexte le papier montre comment les notions indice et intensité implication de gras s'appliquent aux règles produites par les arbres de décision et présente des alternatives inspirées de résidus utilisés en modélisation de tables de contingence nous discutons ensuite sur un jeu de données réelles deux usages de ces indicateurs de force implication pour les règles issues d'arbres il s'agit une part de évaluation individuelle des règles et autre part de leur utilisation comme critère pour le choix de la conclusion de la règle\n",
      "classement des fragments de documents xml par une méthode aide à la décision\n",
      "vu accroissement constant du volume information accessible en ligne sous format xml il devient primordial de proposer des modèles adaptés à la recherche information dans les documents xml tandis que la recherche information classique repose sur indexation du contenu des documents la recherche information dans les documents xml tente améliorer la qualité des résultats en tirant profit de la sémantique véhiculée par la structure des documents dans cet article nous présentons une méthode de classement des items éléments xml retournés lors une recherche dans une collection de documents xml le classement repose sur la prise en compte un ensemble de critères discriminants la particularité de notre approche réside dans la façon dont nous les utilisons  nous employons une méthode décisionnelle pour classer les items en les comparant deux-à-deux là où en général une fonction de scoring globale est utilisée\n",
      "classification de fonctions continues à aide une distribution et une densité définies dans un espace de dimension infinie\n",
      "il n'est pas rare que des données individu soient caractérisées par une distribution continue et non une seule valeur ces données fonctionnelles peuvent être utilisées pour classer les individus une solution élémentaire est de réduire les distributions à leurs moyennes et variances une solution plus riche a été proposée par diday 2002) et mise en oeuvre par vrac et al 2001) et cuvelier et noirhomme-fraiture (2005) elle utilise des points de coupures dans les distributions et modélise ces valeurs conjointes par une distribution multidimensionnelle construite à aide une copule nous avons montré dans un précédent travail que si cette technique apporte de bons résultats la qualité de la classification dépend néanmoins du nombre et de emplacement des coupures les questions du choix du nombre et de emplacement des coupures restaient des questions ouvertes nous proposons une solution à ces questions lorsque le nombre de coupures tend vers l'infini en proposant une nouvelle distribution de probabilité adaptée à espace de dimension infinie que forment les données fonctionnelles nous proposons aussi une densité de probabilité adaptée à la nature de cette distribution en utilisant la dérivée directionnelle de gâteaux la direction choisie pour cette dérivée est celle de la dispersion des fonctions à classer les résultats sont encourageants et offrent des perspectives multiples dans tous les domaines où une distribution de données fonctionnelles est nécessaire\n",
      "classification de grands ensembles de données avec un nouvel algorithme de svm\n",
      "le nouvel algorithme de boosting de least-squares support vector machine ls-svm) que nous présentons vise à la classification de très grands ensembles de données sur des machines standard les méthodes de svm et de noyaux permettent obtenir de bons résultats en ce qui concerne la précision mais la tâche apprentissage pour de grands ensembles de données demande une grande capacité mémoire et un temps relativement long nous présentons une extension de algorithme de ls-svm proposé par suykens et vandewalle pour le boosting de ls-svm a cette fin nous avons ajouté un terme de régularisation de tikhonov et utilisé la formule de sherman-morrison-woodbury pour traiter des ensembles de données ayant un grand nombre de dimensions nous avons ensuite étendu par application du boosting de ls-svm afin de traiter des données ayant simultanément un grand nombre individus et de dimensions les performances de algorithme sont évaluées sur les ensembles de données de l'uci twonorm ringnorm reuters-21578 et ndc sur une machine standard (pc-p4 3ghz 512 mo ram)\n",
      "classification supervisée de séquences biologiques basée sur les motifs et les matrices de substitution\n",
      "la classification des séquences biologiques est un des importants défis ouverts dans la bioinformatique tant pour les séquences protéiques que pour les séquences nucléiques cependant la présence de ces données sous la forme de chaînes de caractères ne permet pas de les traiter par les outils standards de classification supervisée qui utilisent souvent le format relationnel pour remédier à ce problème de codage plusieurs travaux se sont basés sur extraction des motifs pour construire une nouvelle représentation des séquences biologiques sous la forme un tableau binaire nous décrivons une nouvelle approche qui étend les méthodes précédents par utilisation de matrices de substitution dans les cas des séquences protéiques nous présentons ensuite une étude comparative qui prend en compte effet de chaque méthode sur la précision de la classification mais aussi le nombre attributs générés et le temps de calcul\n",
      "clustering  from model-based approaches to heuristic algorithms\n",
      "les méthodes du clustering ont pour but de diviser un ensemble large) objets dans un petit nombre de groupes homogènes (clusters) basé sur des données relevées ou observées qui décrivent les dis-)similarités qui existent entre les objets – en espérant que ces clusters soient utiles pour application concernée il existe une multitude d'approches et cette contribution présente quelques-unes qui sont les plus importantes ou actuelles\n",
      "combinaison des cartes topologiques mixtes et des machines à vecteurs de support  une application pour la prédiction de perte de poids chez les obèses\n",
      "cet article présente un modèle pour aborder les problèmes de classement difficiles en particulier dans le domaine médical ces problèmes ont souvent la particularité avoir des taux erreurs en généralisations très élevés et ce quelles que soient les méthodes utilisées pour ce genre de problèmes nous proposons utiliser un modèle de classement combinant le modèle de partitionnement des cartes topologiques mixtes et les machines à vecteurs de support (svm) le modèle non supervisé est dédié à la visualisation et au partitionnement des données composées de variables quantitatives et/ou qualitatives le deuxième modèle supervisé est dédié au classement la combinaison de ces deux modèles permet non seulement améliorer la visualisation des données mais aussi en les performances en généralisation ce modèle ct-svm) consiste à entraîner des cartes auto-organisatrices pour construire une partition organisée des données constituée de plusieurs sous-ensembles qui vont servir à reformuler le problème de classement initial en sous-problème de classement pour chaque sous-ensemble on entraîne un classeur svm spécifique pour la validation expérimentale de notre modèle ct-svm), nous avons utilisé quatre jeux de données la première base est un extrait une grande base médicale sur étude de obésité réalisée à hôpital hôtel-dieu de paris et les trois dernières bases sont issues de la littérature\n",
      "construction coopérative de carte de thèmes  vers une modélisation de activité socio-sémantique\n",
      "nous présentons dans cette contribution un cadre de modélisation recourant conjointement au modèle hypertopic cahier et al. 2004 pour la représentation des connaissances de domaine et au modèle seeme herrmann et al. 1999 pour la représentation de l'activité ces deux approches apparaissent complémentaires et nous montrons comment elles peuvent être combinées pour mieux ancrer sur les plans formel et méthodologique les approches de cartographie collective des connaissances\n",
      "construction ontologie à partir de corpus de textes\n",
      "cet article présente une méthode semi-automatique de construction ontologie à partir de corpus de textes sur un domaine spécifique cette méthode repose en premier lieu sur un analyseur syntaxique partiel et robuste des textes et en second lieu sur utilisation de analyse formelle de concepts \"fca\" pour la construction de classes objets en un treillis de galois la construction de l'ontologie c'est à dire une hiérarchie de concepts et d'instances est réalisée par une transformation formelle de la structure du treillis cette méthode s'applique dans le domaine de l'astronomie\n",
      "construction et analyse de résumés de données évolutives  application aux données usage du web\n",
      "la manière dont une visite est réalisée sur un site web peut changer en raison de modifications liées à la structure et au contenu du site lui-même ou bien en raison du changement de comportement de certains groupes utilisateurs ou de émergence de nouveaux comportements ainsi les modèles associés à ces comportements dans la fouille usage du web doivent être mis à jour continuellement afin de mieux refléter le comportement actuel des internautes une solution proposée dans cet article est de mettre à jour ces modèles à aide des résumés obtenus par une approche évolutive des méthodes de classification\n",
      "construction incrémentale et visualisation de graphes de voisinage par des fourmis artificielles\n",
      "cet article décrit un nouvel algorithme incrémental nommé antgraph pour la construction de graphes de voisinage il s'inspire du comportement autoassemblage observé chez des fourmis réelles où ces dernières se fixent progressivement à un support fixe puis successivement aux fourmis déjà fixées afin de créer une structure vivante nous utilisons ainsi une approche à base de fourmis artificielles où chaque fourmi représente une donnée nous indiquons comment ce comportement peut être utilisé pour construire de manière incrémentale un graphe à partir une mesure de similarité entre les données nous montrons finalement que notre algorithme obtient de meilleurs résultats en comparaison avec le graphe de voisins relatifs notamment en terme de temps de calcul\n",
      "découverte de chroniques à partir de séquences événements pour la supervision de processus dynamiques\n",
      "ce papier adresse le problème de la découverte de connaissances temporelles à partir des données datées générées par le système de supervision un processus de fabrication par rapport aux approches existantes qui s'appliquent directement aux données notre méthode extraction des connaissances se base sur un modèle global construit à partir des données approche de modélisation adoptée dite stochastique considère les données datées comme une séquence occurrences de classes événements discrets cette séquence est représentée sous les formes duales une chaîne de markov homogène et une superposition de processus de poisson algorithme proposé appelé bjt4r permet identifier les motifs séquentiels les plus probables entre deux classes événements discrets et les représentent sous la forme de modèles de chroniques ce papier présente les premiers résultats de application de cet algorithme sur des données générées par un processus de fabrication de semi-conducteur un site de production du groupe stmicroelectronics\n",
      "des fonctions oubli intelligentes dans les entrepôts de données\n",
      "les entrepôts de données stockent des quantités de données de plus en plus massives et arrivent vite à saturation un langage de spécifications de fonctions oubli est défini pour résoudre ce problème dans le but offrir la possibilité effectuer des analyses sur historique des données les spécifications définissent des résumés par agrégation et par échantillonnage à conserver parmi les données à \"oublier\" cette communication présente le langage de spécifications ainsi que les principes et les algorithmes pour assurer de façon mécanique la gestion des fonctions oubli.\n",
      "détermination du niveau de consommation des abonnés en téléphonie mobile par la théorie des ensembles flous\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la détermination du niveau de consommation chez les clients est essentielle pour tout objectif de segmentation stratégique et de churn nous présentons sur un cas réel utilisation de la théorie des ensembles flous pour la définition une fonction appartenance permettant d'évaluer de manière précise le niveau de consommation des abonnés en téléphonie mobile\n",
      "ensemble prédicteur fondé sur les cartes auto-organisatrices adapté aux données volumineuses\n",
      "le stockage massif des données noie information pertinente et engendre des problèmes théoriques liés à la volumétrie des données disponibles ces problèmes dégradent la capacité prédictive des algorithmes extraction des connaissances à partir des données dans cet article nous proposons une méthodologie adaptée à la représentation et à la prédiction des données volumineuses a cette fin suite à un partitionnement des attributs des groupes attributs non-corrélés sont créés qui permettent de contourner les problèmes liés aux espaces de grandes dimensions un ensemble est alors mis en place apprenant chaque groupe par une carte auto-organisatrice outre la prédiction ces cartes ont pour objectif une représentation pertinente des données enfin la prédiction est réalisée par un vote des différentes cartes une expérimentation est menée qui confirme le bien-fondé de cette approche\n",
      "evaluation une approche de classification conceptuelle\n",
      "objectif de ce travail est évaluer la perte information au sens de inertie entre des méthodes de partitionnement ou de classification hiérarchiques et une approche de classification conceptuelle nous voulons répondre à la question suivante  aspect simpliste du processus monothétique une méthode conceptuelle implique-t-il des partitions de moins bonne qualité au sens du critère de inertie  nous proposons de réaliser cette expérience sur 6 bases de l'uci trois de ces bases sont des tableaux de données quantitatives les trois autres sont des tableaux de données qualitatives\n",
      "evaluation supervisée de métrique  application à la préparation de données séquentielles\n",
      "de nos jours le statisticien n'a plus nécessairement le contrôle sur la récolte des données le besoin une analyse statistique vient dans un second temps une fois les données récoltées par conséquent un travail est à fournir lors de la phase de préparation des données afin de passer une représentation informatique à une représentation statistique adaptée au problème considéré dans cet article nous étudions un procédé de sélection une bonne représentation en nous basant sur des travaux antérieurs nous proposons un protocole évaluation de la pertinence une représentation par intermédiaire une métrique dans le cas de la classification supervisée ce protocole exploite une méthode de classification non paramétrique régularisée garantissant automaticité et la fiabilité de l'évaluation nous illustrons le fonctionnement et les apports de ce protocole par un problème réel de préparation de données de consommation téléphonique nous montrons également la fiabilité et interprétabilité des décisions qui en résultent\n",
      "evolution de ontologie et gestion des annotations sémantiques inconsistantes\n",
      "les ontologies et les annotations sémantiques sont deux composants importants dans un système de gestion des connaissances basé sur le web sémantique dans environement dynamique et distribué du web sémantique les ontologies et les annotations pourraient être changées pour s'adapter à évolution de organisation concernée ces changements peuvent donc entraîner des inconsistances à détecter et traiter dans cet article nous nous focalisons principalement sur évolution des annotations sémantiques en soulignant le contexte où les modifications de ontologie entraînent des inconsistances sur ces annotations nous présentons une approche basée sur des règles permettant de détecter les inconsistances dans les annotations sémantiques devenues obsolètes par rapport à ontologie modifiée nous décrivons aussi les stratégies évolution nécessaires pour guider le processus de résolution de ces inconsistances grâce à des règles correctives\n",
      "extension sémantique du modèle de similarité basé sur la proximité floue des termes\n",
      "le modèle flou de proximité repose sur hypothèse que plus les occurrences des termes une requête se trouvent proches dans un document plus ce dernier est pertinent cette mesure floue est très avantageuse dans le traitement des documents à textes courts toutefois elle ne tient pas compte de la sémantique des termes nous présentons dans cet article intégration une métrique conceptuelle au modèle de proximité floue des termes pour la formalisation de notre propre modèle\n",
      "extraction entités dans des collections évolutives\n",
      "nous nous intéressons à extraction entités nommées avec comme but exploiter un ensemble de rapports pour en extraire une liste de partenaires à partir une liste initiale nous utilisons un premier ensemble de documents pour identifier des schémas de phrase qui sont ensuite validés par apprentissage supervisé sur des documents annotés pour en mesurer efficacité avant être utilisés sur ensemble des documents à explorer cette approche est inspirée de celle utilisée pour extraction de données dans les documents semi-structurés wrappers) et ne nécessite pas de ressources linguistiques particulières ni de larges collections de tests notre collection de documents évoluant annuellement nous espérons de plus une amélioration de notre extraction dans le temps\n",
      "extraction de connaissances adaptation par analyse de la base de cas\n",
      "en raisonnement à partir de cas adaptation un cas source pour résoudre un problème cible est une étape à la fois cruciale et difficile à réaliser une des raisons de cette difficulté tient au fait que les connaissances adaptation sont généralement dépendantes du domaine d'application c'est ce qui motive la recherche sur acquisition de connaissances adaptation (aca) cet article propose une approche originale de aca fondée sur des techniques extraction de connaissances dans des bases de données (ecbd) nous présentons cabamaka une application qui réalise aca par analyse de la base de cas en utilisant comme technique apprentissage extraction de motifs fermés fréquents ensemble du processus extraction des connaissances est détaillé puis nous examinons comment organiser les résultats obtenus de façon à faciliter la validation des connaissances extraites par l'analyste\n",
      "extraction de données sur internet avec retroweb\n",
      "ce document décrit retroweb une boite à outils qui permet extraction de données structurées à partir de pages web notre solution est semi-automatique car les données à extraire sont préalablement dé\u0002nies par l'utilisateur intérêt de cette approche est qu'elle permet extraction de données ciblées et conformes aux besoins de application utilisatrice (migrateur moteur de recherche outil de veille) retroweb se caractérise aussi par une grande facilité utilisation car il ne nécessite aucune connaissance de langage particulier la définition des règles extraction se faisant directement de manière interactive dans le navigateur internet ce document décrit les trois principaux processus de notre méthode\n",
      "extraction de séquences multidimensionnelles convergentes et divergentes\n",
      "les motifs séquentiels sont un domaine de la fouille de données très étudié depuis leur introduction par agrawal et srikant.même s'il existe de nombreux travaux (algorithmes domaines d'application) peu entre eux se situent dans un contexte multidimensionnel avec la prise en compte de ses spécificités  plusieurs dimensions relations hiérarchiques entre les éléments de chaque dimension etc dans cet article nous proposons une méthode originale pour extraire des connaissances multidimensionnelles définies sur plusieurs niveaux de hiérarchies mais selon un certain point de vue  du général au particulier ou vice et versa nous définissons ainsi le concept de séquences multidimensionnelles convergentes ou divergentes ainsi que algorithme associé m2s_cd basé sur le paradigme \"pattern growth\" des expérimentations sur des jeux de données synthétiques et réelles montrent intérêt de notre approche aussi bien en terme de robustesse des algorithmes que de pertinence des motifs extraits\n",
      "extraction des top-k motifs par approximer-et-pousser\n",
      "cet article porte sur extraction de motifs sous contraintes globales contrairement aux contraintes usuelles comme celle de fréquence minimale leur vérification est problématique car elle entraine de multiples comparaisons entre les motifs typiquement la localisation des k motifs maximisant une mesure d'intérêt ie satisfaisant la contrainte top-k est difficile pourtant cette contrainte globale se révèle très utile pour trouver les motifs les plus significatifs au regard un critère choisi par l'utilisateur dans cet article nous proposons une méthode générale extraction de motifs sous contraintes globales appelée approximer-et-pousser cette méthode peut être vue comme une méthode de relaxation une contrainte globale en une contrainte locale évolutive nous appliquons alors cette approche à extraction des top-k motifs selon une mesure d'intérêt les expérimentations montrent efficacité de approche approximer-et-pousser\n",
      "filtrage des sites web à caractère violent par analyse du contenu textuel et structurel\n",
      "dans cet article nous proposons une solution pour la classification et le filtrage des sites web à caractère violent a la différence de la majorité de systèmes commerciaux basés essentiellement sur la détection de mots indicatifs ou utilisation une liste noire manuellement collectée notre solution baptisée \"webangels filter\" s'appuie sur un apprentissage automatique par des techniques de data mining et une analyse conjointe du contenu textuel et structurel de la page web les résultats expérimentaux obtenus lors de évaluation de notre approche sur une base de test sont assez bons comparé avec des logiciels parmi les plus populaires \"webangels filter\" montre sa performance en terme de classification\n",
      "finding interesting queries in relational databases\n",
      "la découverte de motifs dans des bases de données relationnelles quelconques est un problème intéressant pour lequel il existe très peu de méthodes efficaces nous présentons un cadre dans lequel des paires de requêtes sur les données sont utilisées comme des motifs et nous discutons du problème de la découverte associations utiles entre elles plus spécifiquement nous considérons des petites sous-classes de requêtes conjonctives qui nous permettent de découvrir des motifs intéressants de manière efficace\n",
      "fusion des approches visuelles et contextuelles pour annotation des images médicales\n",
      "dans le contexte de la recherche information sur internet nous proposons une architecture annotation automatique des images médicales extraites à partir des documents de santé en ligne notre système est conçu pour extraire des informations médicales spécifiques (ie modalité médicale région anatomique à partir du contenu et du contexte des images nous proposons une architecture de fusion des approches contenu/contexte adaptée aux images médicales approche orientée sur le contenu des images consiste à annoter des images inconnues par la catégorisation des représentations visuelles compactes nous utilisons en même temps le contexte des images les régions textuelles ainsi que des ontologies médicales spécialement adaptées aux informations recherchées finalement nous démontrons qu'en fusionnant les décisions des deux approches nous améliorons les performances globales du système annotation.\n",
      "génération et enrichissement automatique de listes de patrons de phrases pour les moteurs de questions-réponses\n",
      "nous utilisons un algorithme amorce mutuelle riloff et jones 99) entre des couples de termes une relation et des patrons de phrase à partir de couples amorce, le système génère des listes de patrons qui sont ensuite enrichies de façon semi-supervisée puis utilisées pour trouver de nouveaux couples ces couples sont à leur tour réutilisés pour générer par itérations successives de nouveaux patrons originalité de étude réside dans interprétation du rappel estimé comme la couverture un patron sur ensemble des exemples auxquels il s'applique\n",
      "intégration des connaissances utilisateurs pour des analyses personnalisées dans les entrepôts de données évolutifs\n",
      "dans cet article nous proposons une approche évolution de schéma dans les entrepôts de données qui permet aux utilisateurs intégrer leurs propres connaissances du domaine afin enrichir les possibilités analyse de l'entrepôt nous représentons cette connaissance sous la forme de règles de type \"si-alors\" ces règles sont utilisées pour créer de nouveaux axes analyse en générant de nouveaux niveaux de granularité dans les hiérarchies de dimension notre approche est fondée sur un modèle formel entrepôts de données évolutif qui permet de gérer la mise à jour des hiérarchies de dimension\n",
      "interestingness in data mining\n",
      "interestingness measures play an important role in data mining regardless of the kind of patterns being mined these measures are intended for selecting and ranking patterns according to their potential interest to the user good measures also allow the time and space cost of the mining process to be reduced measuring the interestingness of discovered patterns is an active and important area of data mining research although much work has been conducted in this area so far there is no widespread agreement on a formal definition of interestingness in this context based on the diversity of definitions presented to date interestingness is perhaps best treated as a broad concept which emphasizes conciseness coverage reliability peculiarity diversity novelty surprisingness utility and actionability this presentation reviews interestingness measures for rules and summaries classifies them from several perspectives compares their properties identifies their roles in the data mining process gives strategies for selecting appropriate measures for applications and identifies opportunities for future research in this area\n",
      "émergence de connaissances dans les communautés de pratique\n",
      "cet article est le résultat une recherche sur le processus peu explicité dans la littérature de création de connaissances dans les communautés de pratique nous commençons par établir une définition de travail pour ce concept de communauté de pratique qui permet échange et le partage de connaissances au sein de groupes de plus en plus virtuels nous analysons ensuite les communautés de pratique sous angle de la théorie de émergence nous proposons alors la modélisation un outil de support pour ces communautés qui améliore les échanges entre les membres et favorise émergence de nouvelles connaissances cet outil manipule les connaissances implicites ainsi qu'explicites et propose des possibilités pour la publication et la recherche d'informations de plus il s'adapte à chaque membre de la communauté par un processus de personnalisation\n",
      "outil sdet pour le complètement des données descriptives liées aux bases de données géographiques\n",
      "enrichissement des bases de données est un moyen visant à offrir un supplément informationnel aux utilisateurs dans le cas des données géographiques cette activité représente de nos jours un problème crucial sa résolution permettrait de meilleures prises de décisions ne reposant pas uniquement sur les informations limitées notre outil sdet semantic data enrichment tool vient proposer une solution enrichissement faisant du système information géographiques sig) initial une source riche informations.\n",
      "les itemsets essentiels fermés  une nouvelle représentation concise\n",
      "devant accroissement constant des grandes bases de données plusieurs travaux de recherche en fouille de données s'orientent vers le développement de techniques de représentation compacte ces recherches se développent suivant deux axes complémentaires  extraction de bases génériques de règles association et extraction de représentations concises itemsets fréquentsdans ce papier nous introduisons une nouvelle représentation concise exacte des itemsets fréquents elle se situe au croisement de chemins de deux autres représentations concises à savoir les itemsets fermés et ceux dits essentiels idée intuitive est de profiter du fait que tout opérateur de fermeture induit une fonction surjective dans ce contexte nous introduisons un nouvel opérateur de fermeture permettant de calculer les fermetures des itemsets essentiels ceci a pour but avoir une représentation concise de taille réduite tout en permettant extraction des supports négatif et disjonctif un itemset en plus de son support conjonctif un nouvel algorithme appelé d-closure permettant extraire les itemsets essentiels fermés est aussi présenté étude expérimentale que nous avons menée a permis de confirmer que la nouvelle approche présente un bon taux de compacité comparativement aux autres représentations concises exactes\n",
      "logiciel aide à évaluation des catégorisations\n",
      "les méthodes de classification automatique sont employées dans des domaines variés et de nombreux algorithmes ont été proposés dans la littérature au milieu de cette \"jungle\" il semble parfois difficile à un simple utilisateur de choisir quel algorithme est le plus adapté à ses besoins depuis le milieu des années 90 une nouvelle thématique de recherches appelée clustering validity tente de répondre à ce genre interrogation en proposant des indices pour juger de la qualité des catégorisations obtenues mais le choix est parfois difficile entre ces indices et il peut s'avérer délicat de prendre la bonne décision c'est pourquoi nous proposons un logiciel adapté à cette problématique d'évaluation\n",
      "mesure entropie asymétrique et consistante\n",
      "les mesures entropie dont la plus connue est celle de shannon ont été proposées dans un contexte de codage et de transmission d'information néanmoins dès le milieu des années soixante elles ont été utilisées dans autres domaines comme apprentissage et plus particulièrement pour construire des graphes induction et des arbres de décision usage brut de ces mesures n'est cependant pas toujours bien approprié pour engendrer des modèles de prédiction ou explication pertinents cette faiblesse résulte des propriétés des entropies en particulier le maximum nécessairement atteint pour la distribution uniforme et insensibilité à la taille de l'échantillon nous commençons par rappeler ces propriétés classiques nous définissons ensuite une nouvelle axiomatique mieux adaptée à nos besoins et proposons une mesure empirique entropie plus flexible vérifiant ces axiomes\n",
      "mesure non symétrique pour évaluation de modèles utilisation pour les jeux de données déséquilibrés\n",
      "les critères servant à évaluation de modèles apprentissage supervisé ainsi que ceux utilisés pour bâtir des arbres de décision sont pour la plupart symétriques de manière pragmatique cela signifie que chacune des modalités de la variable endogène se voit assigner une importance identique or dans nombre de cas pratiques cela n'est pas le cas ainsi on peut notamment prendre exemple de jeux de données fortement déséquilibrés pour lesquels objectif principal est identification des objets représentatifs de la modalité minoritaire aide au diagnostic identification de phénomènes inhabituels  fraudes pannes) dans ce type de situation il apparaît clairement qu'assigner une importance identique aux erreurs de prédiction ne constitue pas la meilleure des solutions nous proposons dans cet article un critère pouvant servir à la fois pour évaluation de modèles apprentissage supervisé ou encore de critère utilisé pour bâtir des arbres de décision prenant en compte cet aspect non symétrique de importance associée à chacune des modalités de la variable endogène nous proposons ensuite une évolution des modèles de type forêts aléatoires utilisant ce critère pour les jeux de données fortement déséquilibrés\n",
      "méthodes statistiques et modèles thermiques compacts\n",
      "dans le domaine thermique la plupart des études reposent sur des modèles à éléments finis cependant le coût en calcul et donc en temps de ces méthodes ont renforcé le besoin de modèles plus compacts le réseau rc équivalent est la solution la plus souvent utilisée toutefois ses paramètres doivent souvent être ajustés à aide de mesures ou de simulation dans ce contexte identification de système les méthodes statistiques seront comparées aux méthodes classiquement utilisées pour la prédiction thermique\n",
      "navigation et appariement objets géographiques dans une ontologie\n",
      "aci fodomust se propose élaborer un processus de fouille de données multi-stratégies pour la reconnaissance automatique objets géographiques sur des images satellitaires ou aériennes ces dernières sont segmentées afin isoler des polygones définis par un ensemble de descripteurs de bas niveaux afin de leur affecter une sémantique on applique dans un premier temps une classification si aucun objet géographique n'est identifié on tente alors un appariement du polygone avec les concepts une ontologie objets géographiques un algorithme de navigation dans ontologie et une mesure de comparaison sémantique ont ainsi été développés paramétrables selon le contexte d'appariement cette mesure évalue la pertinence un appariement et comprend une composante locale comparaison au niveau du concept et une composante globale combinaison linéaire de mesures locales) la méthode proposée a été développée en java et intégrée à la plate-forme fodomust les premières expérimentations et évaluations humaines sont très encourageantes\n",
      "notion de conversation dans les communications interpersonnelles instantanées sur ip\n",
      "dans cet article nous étudions la contribution des techniques de fouille de données à amélioration des services de communications instantanées sur ip tel que la messagerie instantanée im) et la téléphonie sur ip (toip)\n",
      "okm  une extension des k-moyennes pour la recherche de classes recouvrantes\n",
      "dans cet article nous abordons le problème de la classification ou clustering dans le but de découvrir des classes avec recouvrements malgré quelques avancées récentes dans ce domaines motivées par des besoins applicatifs importants traitements des données multimédia par exemple) nous constatons absence de solutions théoriques à ce problème notre étude consiste alors à proposer une nouvelle formulation du problème de classification par partitionnement adaptée à la recherche un recouvrement des données en classes objets similaires cette approche se fonde sur la dé\u0002nition un critère objectif de qualité un recouvrement et une solution algorithmique visant à optimiser ce critère nous proposons deux évaluations de ce travail permettant une part appréhender le fonctionnement global de algorithme sur des données simples vitesse de convergence visualisation des résultats et autre part évaluer quantitativement le bénéfice une telle approche sur une application de classification de documents textuels\n",
      "optimal histogram representation of large data sets fisher vs piecewise linear approximation\n",
      "histogram representation of a large set of data is a good way to summarize and visualize data and is frequently performed in order to optimize query estimation in dbms in this paper we show the performance and the properties of two strategies for an optimal construction of histograms on a single real valued descriptor on the base of a prior choice of the number of buckets the first one is based on the fisher algorithm while the second one is based on a geometrical procedure for the interpolation of the empirical distribution function by a piecewise linear function the goodness of fit is computed using the wasserstein metric between distributions we compare the proposed method performances against some existing ones on artificial and real datasets\n",
      "partitionnement un réseau de sociabilité à fort coefficient de clustering\n",
      "afin de comparer organisation sociale une paysannerie médiévale avant et après la guerre de cent ans nous étudions la structure de réseaux sociaux construits à partir un corpus de contrats agraires faibles diamètres et fort clustering révèlent des graphes en petit monde comme beaucoup de grands réseaux interaction étudiés ces dernières années ces graphes sont sans échelle typique les distributions des degrés de leurs sommets sont bien ajustées par une loi de puissance tronquée par une coupure exponentielle ils possèdent en outre un club-huppé c'est à dire un noyau dense et de faible diamètre regroupant les individus à forts degrés la forme particulière des éléments propres du laplacien permet extraire des communautés qui se répartissent en étoile autour du club huppé\n",
      "peut-on capturer la sémantique à travers la syntaxe  - découverte des règles exception simultanée\n",
      "objectif de la fouille de données est la découverte sophistiquée de connaissances lisibles surprenantes et possiblement utiles les aspects surprenant et utile font partie de la sémantique et nécessitent utilisation des connaissances du domaine ce qui cause souvent le problème acquisition de la connaissance notre découverte des règles exception simultanée peut être une réponse à ce problème nous envisageons de trouver les connaissances surprenantes et possiblement utiles à travers notre forme de paire de règles exception. les autres méthodes inventées concernent index évaluation et la recherche exhaustive plusieurs applications médicales seront présentées sur lesquelles nos propositions ont été appliquées\n",
      "préservation de intimité dans les protocoles de conversations\n",
      "le travail présenté dans cet article rentre dans le cadre de la gestion des données privées en vue de la substitution appelée remplaçabilité dynamique des services web trois contributions sont apportées 1) modélisation des politiques privées spécifiant les règles utilisation des données privées prenant en compte des aspects se rapportant aux services web 2) étendre les protocoles de conversations des services web par le modèle proposé afin apporter les primitives nécessaires pour analyse des protocoles en présence de ces règles 3) définition un mécanisme analyse de la remplaçabilité un service par un autre en vue de ses politiques privées\n",
      "ras  un outil pour annotation de documents basée sur les liens de citation\n",
      "ras reference annotation system est un outil annotation de documents cet outil est le résultat de implémentation de notre approche annotation basée sur le contexte de citation approche est indépendante du contenu et utilise un regroupement thématique des références construit à partir une classification floue non-supervisée outil présenté dans cet article a été expérimentée et évaluée avec la base de documents scientifiques citeseer\n",
      "ré-ordonnancement pour apprentissage de transformations de documents html\n",
      "notre objectif est de transformer les documents web vers un schéma médiateur xml défini a priori c'est une étape nécessaire pour de nombreuses tâches de recherche information concernant le web sémantique les documents semi-structurés le traitement de sources hétérogènes etc elle permet associer une structure sémantiquement riche à des documents dont le formats ne contient que des informations de présentation nous proposons de traiter ce problème comme un problème apprentissage structuré en le formalisant comme une transformation arbre en arbre.notre méthode de transformation comporte deux étapes dans une première étape une grammaire hors-contexte probabiliste permet de générer un ensemble de solutions candidates dans une deuxième étape ces solutions candidates sont ordonnées grâce à un algorithme de ré-ordonnancement à base de perceptron à noyau cette étape ordonnancement nous permet utiliser de manière efficace des caractéristiques complexes définies à partir du document entrée et de la solution candidate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "réduction de dimension pour analyse de données vidéo\n",
      "les données vidéo ont la particularité être très volumineuses alors qu'elles contiennent peu information sémantique pour les analyser il faut réduire la quantité information dans espace de recherche les données vidéo sont souvent considérées comme ensemble des pixels une succession images analysées séquentiellement dans cet article nous proposons utiliser une analyse en composantes principales acp) pour réduire la dimensionnalité des informations sans perdre la nature tridimensionnelle des données initiales nous commençons par considérer des sous-séquences dont le nombre de trames est le nombre de dimensions dans espace de représentation nous appliquons une acp pour obtenir un espace de faible dimension où les points similaires sémantiquement sont proches la sous-séquence est ensuite divisée en blocs tridimensionnels dont on projette ellipsoïde inertie dans le premier plan factoriel nous déduisons enfin le mouvement présent dans les blocs à partir des ellipses ainsi obtenues nous présenterons les résultats obtenus pour un problème de vidéosurveillance\n",
      "régression floue et crédibiliste par svm pour la classification des images sonar\n",
      "la classification des images sonar est une grande importance par exemple pour la navigation sous-marine ou pour la cartographie des fonds marins en effet le sonar offre des capacités imagerie plus performantes que les capteurs optiques en milieu sous-marin la classification de ce type de données rencontre plusieurs difficultés en raison des imprécisions et incertitudes liées au capteur et au milieu de nombreuses approches ont été proposées sans donner de bons résultats celles-ci ne tenant pas compte des imperfections des données pour modéliser ce type de données il est judicieux utiliser les théories de incertain comme la théorie des sous-ensembles flous ou la théorie des fonctions de croyance les machines à vecteurs de supports sont de plus en plus utilisées pour la classification automatique aux vues leur simplicité et leurs capacités de généralisation il est ainsi possible de proposer une approche qui tient compte de ces imprécisions et de ces incertitudes au coeur même de algorithme de classification approche de la régression par svm que nous avons introduite permet cette modélisation des imperfections nous proposons ici une application de cette nouvelle approche sur des données réelles particulièrement complexes dans le cadre de la classification des images sonar\n",
      "segmentation thématique par calcul de distance thématique\n",
      "dans cet article nous présentons une approche de la segmentation thématique fondée sur une représentation en vecteurs sémantiques des phrases et des calculs de distance entre ces vecteurs les vecteurs sémantiques sont générés par le système sygfran un analyseur morpho-syntaxique et conceptuel de la langue française la segmentation thématique s'effectue elle en recherchant des zones de transition au sein du texte grâce aux vecteurs sémantiques évaluation de cette méthode s'est faite sur les données du défi deft'06\n",
      "sémantique et contextes conceptuels pour la recherche information\n",
      "cet article propose une méthodologie de recherche information qui utilise analyse conceptuelle conjointement avec la sémantique dans le but de fournir des réponses contextuelles à des requêtes sur le web le contexte conceptuel défini dans cet article peut être global – c'est-à-dire stable – ou instantané – c'est-à-dire borné par le contexte global notre méthodologie consiste en une première phase de pré traitement permettant de construire le contexte global et une seconde phase de traitement en ligne des requêtes des utilisateurs associées au contexte instantané notre processus de recherche information est illustré à travers une expérimentation dans le domaine du tourisme\n",
      "sous-bases k-faibles pour des règles association valides au sens de la confiance\n",
      "nous introduisons la notion de sous-base k-faible pour les règles association valides au sens de la confiance ces sous-bases k-faibles sont caractérisées en termes opérateurs de fermeture correspondant à des familles de moore k-faiblement hiérarchiques\n",
      "spoid  extraction de motifs séquentiels pour les bases de données incomplètes\n",
      "les bases de données issues du monde réel contiennent souvent de nombreuses informations non renseignées durant le processus extraction de connaissances dans les bases de données une phase de traitement spécifique de ces données est souvent nécessaire permettant de les supprimer ou de les compléter lors de extraction de séquences fréquentes ces données incomplètes sont la plupart du temps occultées ceci conduit parfois à élimination de plus de la moitié de la base et information extraite n'est plus représentative nous proposons donc de ne plus éliminer les enregistrements incomplets mais utiliser information partielle qu'ils contiennent la méthode proposée ignore en fait temporairement certaines données incomplètes pour les séquences recherchées les expérimentations sur jeux de données synthétiques montrent la validité de notre proposition aussi bien en terme de qualité des motifs extraits que de robustesse aux valeurs manquantes\n",
      "syrqus - recherche par combinaison de graphes rdf\n",
      "nous nous intéressons à un mécanisme permettant la construction de réponses combinés à partir de plusieurs graphes rdf nous imposons par souci de cohérence que cette combinaison soit réalisée uniquement si les graphes rdf ne se contredisent pas pour déterminer la non-contradiction entre deux graphes rdf nous utilisons une mesure de similarité calculée au moment de ajout de documents rdf dans la base de documents\n",
      "traitement de données de consommation électrique par un système de gestion de flux de données\n",
      "avec le développement de compteurs communicants les consommations énergie électrique pourront à terme être télérelevées par les fournisseurs électricité à des pas de temps pouvant aller jusqu'à la seconde ceci générera des informations en continu à un rythme rapide et en quantité importante les systèmes de gestion de flux de données (sgfd) aujourd'hui disponibles sous forme de prototypes ont vocation à faciliter la gestion de tels flux cette communication décrit une étude expérimentale pour analyser les avantages et limites de utilisation de deux prototypes de sgfd stream et telegraphcq pour la gestion de données de consommation électrique\n",
      "traitement et exploration du fichier log du serveur web pour extraction des connaissances  web usage mining\n",
      "le but dans ce travail consiste à concevoir et réaliser un outil logiciel en utilisant les concepts du web usage mining pour offrir aux web masters ensemble des connaissances y inclut les statistiques sur leurs sites afin de prendre les décisions adéquates il s'agit en fait extraire de information à partir du fichier log du serveur web hébergeant le site web et de prendre les décisions pour découvrir les habitudes des internautes et de répondre à leurs besoins en adaptant le contenu la forme et agencement des pages web\n",
      "un algorithme multi-agent de classification pour la construction ontologies dynamiques\n",
      "la construction ontologies à partir de textes reste une tâche coûteuse en temps qui justifie émergence de ontology learning notre système dynamo s'inscrit dans cette mouvance en apportant une approche originale basée sur une architecture multi-agent adaptative en particulier article présente le coeur de notre approche un algorithme distribué de classification hiérarchique qui s'applique sur les résultats un analyseur syntaxique cet algorithme est évalué et comparé à un algorithme centralisé plus conventionnel forts de ces résultats nous discutons ses limites et dressons en perspective les aménagements à effectuer pour aller vers une solution complète de construction ontologies.\n",
      "un cadre théorique pour la gestion de grandes bases de motifs\n",
      "les algorithmes de fouille de données sont maintenant capables de traiter de grands volumes de données mais les utilisateurs sont souvent submergés par la quantité de motifs générés en outre dans certains cas que ce soit pour des raisons de confidentialité ou de coûts les utilisateurs peuvent ne pas avoir accès directement aux données et ne disposer que des motifs les utilisateurs n'ont plus alors la possibilité approfondir à partir des données initiales le processus de fouille de façon à extraire des motifs plus spécifiques pour remédier à cette situation une solution consiste à gérer les motifs ainsi dans cet article nous présentons un cadre théorique permettant à un utilisateur de manipuler en post-traitement une collection de motifs préalablement extraite nous proposons de représenter la collection sous la forme un graphe qu'un utilisateur pourra ensuite exploiter à aide opérateurs algébriques pour y retrouver des motifs ou en chercher de nouveaux\n",
      "un outil pour la visualisation de relations entre gènes\n",
      "la reconstruction de réseaux de gènes est un des défis majeurs de la post-génomique a partir de données expression issues de puces à adn différentes techniques existent pour inférer des réseaux de gènes nous proposons dans ce papier une approche pour la visualisation de réseaux interactions entre gènes à partir de données expression. originalité de notre approche est de superposer des règles avec des sémantiques différentes au sein un même support visuel et de ne générer que les règles qui impliquent des gènes dits centraux ceux-ci sont spécifiés en amont par les experts et permettent de limiter la génération des règles aux seuls gènes qui intéressent les spécialistes une implémentation a été réalisée dans le logiciel libre mev de institut tigr\n",
      "un segmenteur de texte en phrases guidé par utilisateur\n",
      "ce programme effectue une segmentation en phrases un texte contrairement aux procédures classiques nous n'utilisons pas annotations préliminaires et tirons parti un apprentissage guidé par l'utilisateur\n",
      "une approche de classification non supervisée basée sur la détection de singularités et la corrélation de séries temporelles pour la recherche états  application à un bioprocédé fed-batch\n",
      "nous proposons dans cet article une méthode de clustering qui combine analyse dynamique et analyse statistique pour caractériser des états il s'agit une méthode de fouille de données qui travaille sur des ensembles de séries temporelles pour détecter des états ces états représentent les informations les plus significatives du système objectif de cette méthode non supervisée est extraire de la connaissance à partir de analyse des séries temporelles multiples elle s'appuie sur la détection de singularités dans les séries temporelles et sur analyse des corrélations des séries entre les intervalles définis par ces singularités pour application présentée les séries temporelles sont des signaux biochimiques mesurés durant un bioprocédé cette approche est donc utilisée pour confirmer et enrichir la connaissance des experts du domaine des bioprocédés sans utiliser la connaissance a priori de ces experts elle est appliquée à la recherche états physiologiques dans un bioprocédé de type fed-batch\n",
      "une approche non paramétrique bayésienne pour estimation de densité conditionnelle sur les rangs\n",
      "nous nous intéressons à estimation de la distribution des rangs une variable cible numérique conditionnellement à un ensemble de prédicteurs numériques pour cela nous proposons une nouvelle approche non paramétrique bayesienne pour effectuer une partition rectangulaire optimale de chaque couple (cible prédicteur uniquement à partir des rangs des individus nous montrons ensuite comment les effectifs de ces grilles nous permettent de construire un estimateur univarié de la densité conditionnelle sur les rangs et un estimateur multivarié utilisant hypothèse bayesienne naïve ces estimateurs sont comparés aux meilleures méthodes évaluées lors un récent challenge sur estimation une densité prédictive si estimateur bayésien naïf utilisant ensemble des prédicteurs se révèle peu performant estimateur univarié et estimateur combinant deux prédicteurs donne de très bons résultats malgré leur simplicité\n",
      "une approche sociotechnique pour le knowledge management km)\n",
      "cet article présente un cadre sociotechnique pour le km cette vision sociotechnique du km permet  1) écarter le km un souci commercial  2) faire le clivage des différentes technologies du km  et 3) de s'interroger sur les paradigmes associés aux composants social et technique du km c'est précisément ce dernier point que cet article développe afin identifier les mécanismes génériques du km plus précisément aspect social est décrit à travers approche organisationnelle du km approche managériale du km et approche biologique du km alors que aspect technique est décrit à travers approche ingénierie des connaissances et compétences du km ces approches nous conduisent aussi à donner un tableau comparatif entre ces visions organisationnelles managériales et biologiques du km\n",
      "une étude des algorithmes de construction architecture des réseaux de neurones multicouches\n",
      "le problème de choix architecture un réseau de neurones multicouches reste toujours très difficile à résoudre dans un processus de fouille de données ce papier recense quelques algorithmes de recherche architectures un réseau de neurones pour les tâches de classification il présente également une analyse théorique et expérimentale de ces algorithmes ce travail confirme les difficultés de choix des paramètres apprentissage (modèle nombre de couches nombre de neurones par couches taux apprentissage, algorithme apprentissage,...) communs à tout processus de construction de réseaux de neurones et les difficultés de choix de paramètres propres à certains algorithmes\n",
      "une extension de xquery pour la recherche textuelle information dans des documents xml\n",
      "nous présentons dans cet article une extension de xquery que nous avons développée pour interroger le contenu et la structure de documents xml cette extension consiste à intégrer dans xquery le langage nexi un sous-ensemble de xpath défini dans le cadre de initiative inex notre proposition est double  i) équiper nexi une sémantique floue ii) intégrer nexi dans xquery au moyen une métafonction appelée nexi ayant une requête nexi comme paramètre et une extension de la clause for de opérateur flwor de xquery de plus nous décrivons le prototype paramétrable que nous avons développé au dessus de deux moteurs xquery classiques  galax et saxon\n",
      "une méthode interprétation de scores\n",
      "cet article présente une méthode permettant interpréter la sortie un modèle de classification ou de régression interprétation se base sur importance de la variable et importance de la valeur de la variable cette approche permet interpréter la sortie du modèle pour chaque instance\n",
      "une méthode optimale évaluation bivariée pour la classification supervisée\n",
      "en préparation des données pour la classification supervisée les méthodes filtres usuellement utilisées pour la sélection de variables sont efficaces en temps de calcul néanmoins leur nature univariée ne permet pas de détecter les redondances ou les interactions constructives entre variables cet article présente une nouvelle méthode permettant évaluer importance prédictive jointe une paire de variables de façon automatique rapide et fiable elle est basée sur un partitionnement de chaque variable exogène en intervalles dans le cas numérique et groupes de valeurs dans le cas catégoriel la grille de données exogène résultante permet alors évaluer la corrélation entre la paire de variables exogènes et la variable endogène le meilleur partitionnement bivarié est recherché au moyen une approche bayésienne de la sélection de modèle les expérimentations démontrent les apports de la méthode notamment une amélioration significative des performances en classification\n",
      "une nouvelle approche de la programmation dc et dca pour la classification floue\n",
      "dans cet article nous nous intéressons à fuzzy c-means (fcm) une technique très connue pour la classification floue nous proposons un algorithme efficace basé sur la programmation dc difference of convexe functions et dca dc algorithm pour résoudre ce problème les expériences numériques comparatives avec algorithme standard fcm sur les données réelles montrent la robustesse la performance de cet nouvel algorithme dca et sa supériorité par rapport à fcm\n",
      "une nouvelle méthode alignement et de visualisation ontologies owl-lite\n",
      "dans ce papier une nouvelle plate-forme alignement et de visualisation des ontologies appelée pova prototype owl-lite visual alignment) est décrite le module alignement implémente une nouvelle approche alignement ontologies remédiant au problème de la circularité et de intervention de l'utilisateur\n",
      "une règle exception en analyse statistique implicative\n",
      "en fouille de règles certaines situations exceptionnelles défient le bon sens c'est le cas de la règle r  a --> c et b --> c et a et b --> non c une telle règle que nous étudions dans l'article est appelée règle d'exception a la suite des travaux précurseurs de e suzuki et y kodratoff (1999) qui ont étudié un autre type de règle d'exception nous cherchons ici à caractériser les conditions apparition de la règle r dans le cadre de analyse statistique implicative\n",
      "utilisation de wordnet dans la catégorisation de textes multilingues\n",
      "cet article est consacré au problème de la catégorisation multilingue qui consiste à catégoriser des documents de différentes langues en utilisant le même classifieur approche que nous proposons est basée sur idée étendre utilisation de wordnet dans la catégorisation monolingue vers la catégorisation multilingue\n",
      "validation des visualisations par axes principaux de données numériques et textuelles\n",
      "parmi les outils de visualisation de données multidimensionnelles figurent une part les méthodes fondées sur la décomposition aux valeurs singulières et autre part les méthodes de classification incluant les cartes auto-organisées de kohonen comment valider ces visualisations  on présente sept procédures de validation par bootstrap qui dépendent des données des hypothèses des outils  a le bootstrap partiel qui considère les réplications comme des variables supplémentaires b le bootstrap total de type 1 qui réanalyse les réplications avec changements éventuels de signes des axes c le bootstrap total de type 2 qui corrige aussi les interversions d'axes d le bootstrap total de type 3 sur lequel on insistera qui corrige les réplications par rotations procrustéenne e le bootstrap spécifique cas des hiérarchies individus statistiques et des données textuelles) f le bootstrap sur variables g les extensions des procédures précédentes à certaines cartes auto-organisées\n",
      "vers un algorithme multi-agents de clustering dynamique\n",
      "dans cet article nous présentons un algorithme multi-agents de clustering dynamique ce type de clustering doit permettre de gérer des données évolutives et donc être capable adapter en permanence les clusters construits\n",
      "vers un système hybride pour annotation sémantique images irm du cerveau\n",
      "cet article montre intérêt de combiner des méthodes numériques et symboliques pour obtenir une annotation sémantique des images irm du cerveau humain il s'agit identifier des structures anatomiques du cortex cérébral humain en utilisant conjointement des connaissances a priori de nature numérique et une ontologie des structures corticales du cerveau représentée en owl dl étendue par des règles swrl ces connaissances symboliques a priori représentées dans des langages standards du web deviennent non seulement partageables mais permettent aussi un raisonnement automatique qui aide utilisateur à la labellisation des structures anatomiques mises en évidence dans des images irm du cerveau un individu donné\n",
      "vers une base de connaissances biographique  extraction information et ontologie\n",
      "le projet b-ontology a pour but extraction organisation et exploitation de connaissances biographiques à partir de dépêches de presse sa réalisation requiert intégration de diverses technologies principalement extraction d'information les ontologies et bases de connaissances les techniques de data mining cet article propose un aperçu des choix réalisés dans le cadre du projet cette démarche permet également de définir un environnement outils utiles pour les applications extraction et de gestion de connaissances\n",
      "vers une nouvelle approche extraction des motifs séquentiels non-dérivables\n",
      "extraction de motifs séquentiels est un défi important pour la communauté fouille de données même si les représentations condensées ont montré leur intérêt dans le domaine des itemsets à heure actuelle peu de travaux considèrent ce type de représentation pour extraire des motifs cet article propose établir les premières bases formelles pour obtenir les bornes inférieures et supérieures du support une séquence s nous démontrons que ces bornes peuvent être dérivées à partir des sous-séquences de s et prouvons que ces règles de dérivation permettent la construction une nouvelle représentation condensée de ensemble des motifs fréquents les différentes expérimentations menées montrent que notre approche offre une meilleure représentation condensée que celles des motifs clos et cela sans perte d'information\n",
      "vers une plate-forme interactive pour la visualisation de grands ensembles de règles association\n",
      "la recherche de règles association est une question centrale en extraction de connaissances dans les données (ecd) dans cet article nous nous intéressons plus particulièrement à la restitution visuelle de règles pertinentes dans un corpus très important nous proposons ainsi un prototype basé sur une approche de type \"wrapper\" par intégration des phases extraction et de visualisation de l'ecd tout d'abord le processus extraction génère une base générique de règles et dans un second temps la tâche de visualisation s'appuie sur un processus de regroupement \"clustering\") permettant de grouper et de visualiser un sous-ensemble de règles association génériques le rendu visuel à écran exploite une représentation de type \"fisheye view\" de manière à obtenir simultanément une représentation globale des différents groupes de règles et une vue détaillée du groupe sélectionné\n",
      "visualisation de graphes avec tulip  exploration interactive de grandes masses de données en appui à la fouille de données et à extraction de connaissances\n",
      "cet article décrit une étude de cas exhibant les qualités de la plateforme de visualisation de graphes tulip démontrant apport de la visualisation à la fouille de données interactive et à extraction de connaissances le calcul un graphe à partir indices de similarité est un exemple typique où exploration visuelle et interactive de graphes vient en appui au travail de fouille de données nous penchons sur le cas où on souhaite étudier une collection de documents afin avoir une idée des thématiques abordées dans la collection\n",
      "visualisation exploratoire des résultats algorithmes arbre de décision\n",
      "nous présentons une méthode exploration des résultats des algorithmes apprentissage par arbre de décision comme c45) la méthode présentée utilise simultanément une visualisation radiale focus+context fisheye et hiérarchique pour la représentation et exploration des résultats des algorithmes arbre de décision utilisateur peut ainsi extraire facilement des règles induction et élaguer arbre obtenu dans une phase de post-traitement cela lui permet avoir une meilleure compréhension des résultats obtenus les résultats des tests numériques avec des ensembles de données réelles montrent que la méthode proposée permet une bien meilleure compréhension des résultats des arbres de décision\n",
      "webdocenrich  enrichissement sémantique flexible de documents semi-structurés\n",
      "webdocenrich est une approche enrichissement sémantique automatique de documents html hétérogènes qui exploite une description du domaine pour enrichir le contenu des documents et les représenter en xml\n",
      "accès aux connaissances orales par le résumé automatique\n",
      "le temps nécessaire pour écouter un flux audio est un facteur réduisant accès efficace àde grandes archives de parole une première approche la structuration automatique des données,permet utiliser un moteur de recherche pour cibler plus rapidement l'information leslistes de résultats générées sont longues dans un souci d'exhaustivité alors que pour des documentstextuels un coup oeil discrimine un résultat interessant un résultat non pertinant,il faut écouter audio dans son intégralité pour en capturer le contenu nous proposons doncutiliser le résumé automatique afin de structurer les résultats des recherches et en réduirela redondance\n",
      "affectation pondérée sur des données de type intervalle\n",
      "on s'intéresse à la construction arbres de décision sur des données symboliques de type intervalle en utilisant le critère de découpage binaire de kolmogorov-smirnov nous proposons une approche permettant affecter un individu à la fois aux deux noeuds fils générés par le partitionnement un noeud non terminal le but de cette méthode est de prendre en compte le positionnement de la donnée à classer par rapport à la donnée seuil de coupure\n",
      "aide en gestion hospitalière par visualisation des composantes de non-pertinence\n",
      "nan\n",
      "algorithme semi-interactif pour la sélection de dimensions\n",
      "nous présentons un algorithme génétique semi-interactif de sélectionde dimensions dans les grands ensembles de données pour la détectiond'individus atypiques (outliers) les ensembles de données possédant unnombre élevé de dimensions posent de nombreux problèmes aux algorithmesde fouille de données une solution est effectuer un pré-traitement afin de neretenir que les dimensions \"intéressantes\" nous utilisons un algorithmegénétique pour le choix du sous-ensemble de dimensions à retenir par ailleursnous souhaitons donner un rôle plus important à utilisateur dans le processusde fouille nous avons donc développé un algorithme génétique semi-interactifoù évaluation des solutions n'élimine pas complètement la fonctiond'évaluation mais la couple avec une évaluation de utilisateur. enfin,l'importante réduction du nombre de dimensions nous permet de visualiser lesrésultats de algorithme de détection d'outlier cette visualisation permet àl'expert des données étiqueter les éléments atypiques erreurs ou simplementdes individus différents de la masse)\n",
      "alignement extensionnel et asymétrique de hiérarchies conceptuelles par découverte implications entre concepts\n",
      "dans la littérature de nombreux travaux traitent de méthodes alignementd'ontologies ils utilisent pour la plupart des relations basées sur desmesures de similarité qui ont la particularité être symétriques cependant peude travaux évaluent intérêt utiliser des mesures appariement asymétriquesdans le but enrichir alignement produit ainsi nous proposons dans ce papierune méthode alignement extensionnelle et asymétrique basée sur la découvertedes implications significatives entre deux ontologies notre approche,basée sur le modèle probabiliste écart à indépendance appelé intensité implication,est divisée en deux parties consécutives  1) l'extraction à partir ducorpus textuel associé à ontologie et association des termes aux concepts;(2 la découverte et sélection des implications génératrices les plus significativesentre les concepts la méthode proposée est évaluée sur deux jeux de donnéesréels portant respectivement sur des profils entreprises et sur des cataloguesde cours d'universités les résultats obtenus montrent que on peut trouver desrelations pertinentes qui sont ignorées par un alignement basé seulement sur desmesures de similarité\n",
      "amélioration des indicateurs techniques pour analyse du marché financier\n",
      "la technique des motifs fréquents a été utilisée pour améliorer lepouvoir prédictif des stratégies quantitatives innovant dans le contexte desmarchés financiers notre méthode associe une signature aux configurations demarché fréquentes un système de « trading » automatique sélectionne lesmeilleures signatures par une procédure de « back testing » itérative et les utiliseen combinaison avec indicateur technique pour améliorer sa performance.l'application des motifs fréquents à cette problématique des indicateurstechniques est une contribution originale au sens du test t de student,notre méthode améliore nettement les approches sans signatures la techniquea été testé sur des données journalières type taux intérêt et actions notreanalyse des indicateurs (williams%r bn et croisement des moments a montréque qu'une approche par signatures est particulièrement bien adaptée auxstratégies à mémoire courte\n",
      "analyse du comportement des utilisateurs exploitant une base de données vidéo\n",
      "dans cet article nous présentons un modèle de fouille des usages dela vidéo pour améliorer la qualité de l'indexation nous proposons une approchebasée sur un modèle à deux niveaux représentant le comportement des utilisateursexploitant un moteur de recherche vidéo le premier niveau consiste àmodéliser le comportement lors de la lecture une vidéo unique comportementintra vidéo) le second à modéliser le comportement sur ensemble une session(comportement inter video) a partir de cette représentation nous avonsdéveloppé un algorithme de regroupement adapté à la nature particulière de cesdonnées analyse des usages de la vidéo nous permet affiner indexationvidéo sur la base de intérêt des utilisateurs\n",
      "annotation sémantique de pages web\n",
      "cet article présente un système automatique annotation sémantiquede pages web les systèmes annotation automatique existants sont essentiellementsyntaxiques même lorsque les travaux visent à produire une annotationsémantique la prise en compte informations sémantiques sur le domaine pourl'annotation un élément dans une page web à partir une ontologie supposed'aborder conjointement deux problèmes  1) identification de la structuresyntaxique caractérisant cet élément dans la page web et 2) identification duconcept le plus spécifique en termes de subsumption dans ontologie dontl'instance sera utilisée pour annoter cet élément notre démarche repose sur lamise en oeuvre une technique apprentissage issue initialement des wrappersque nous avons articulée avec des raisonnements exploitant la structure formellede ontologie.\n",
      "apprentissage de la structure des réseaux bayésiens à partir des motifs fréquents corrélés  application à identification des facteurs environnementaux du cancer du nasopharynx\n",
      "apprentissage de structure des réseaux bayésien à partir de donnéesest un problème np-difficile pour lequel de nombreuses heuristiques ont été proposées.dans cet article nous proposons une nouvelle méthode inspirée des travauxsur la recherche de motifs fréquents corrélés pour identifier les causalitésentre les variables algorithme opère en quatre temps  1) la découvertepar niveau des motifs fréquents corrélés minimaux  2) la construction ungraphe non orienté à partir de ces motifs  3) la détection des v_structures etl'orientation partielle du graphe  4) élimination des arêtes superflues par destests indépendance conditionnelle la méthode appliquée au réseau asia permetde retrouver la structure du graphe initial nous appliquons ensuite auxdonnées une étude épidémiologique cas-témoins du cancer du nasopharynx(npc) objectif est de dresser un profil statistique type de la population étudiéeet apporter un éclairage utile sur les différents facteurs impliqués dans lenpc\n",
      "approche entropique pour analyse de modèle de chroniques\n",
      "cet article propose utiliser entropie informationnelle pouranalyser des modèles de chroniques découverts selon une approchestochastique bouché et le goc 2005) il décrit une adaptation de algorithmetemporalid3 console et picardi 2003 permettant de découvrir des modèlesde chroniques à partir un ensemble apprentissage contenant des séquencesd'occurrences événements discrets ces séquences représentent des suitesd'alarmes générées par un système à base de connaissance de monitoring et dediagnostic de systèmes dynamiques on montre sur un exemple que approcheentropique complète approche stochastique en identifiant les classesévénements qui contribuent le plus significativement à la prédiction uneoccurrence une classe particulière\n",
      "arabase  base de données web pour exploitation en reconnaissance optique de écriture arabe\n",
      "nan\n",
      "arbres de décision multi modes et multi cibles\n",
      "nous présentons une nouvelle méthode induction arbre de décision appelée mumtree pour multi models tree utilisable pour les modes apprentissage supervisé non supervisé supervisé à plusieurs variables cibles nous présentons les différents principes nécessaires pour réaliser un tel arbre de décision nous illustrons ensuite sur un cas de modélisation multi-cibles les avantages de cette méthode par rapport à un arbre de décision classique\n",
      "archiview un outil de visualisation topographique des paramètres un hôpital\n",
      "nan\n",
      "biclustering of gene expression data based on local nearness\n",
      "the analysis of gene expression data in dna chips is an importanttool used in genomic research whose main objectives range from the study ofthe functionality of specific genes and their participation in biological processto the reconstruction of diseases's conditions and their subsequent prognosis.gene expression data are arranged in matrices where each gene corresponds toone row and every column represents one specific experimental condition thebiclustering techniques have the purpose of finding subsets of genes that showsimilar activity patterns under a subset of conditions our approach consists ofa biclustering algorithm based on local nearness the algorithm searches forbiclusters in a greedy fashion starting with two–genes biclusters and includingas much as possible depending on a distance threshold which guarantees thesimilarity of gene behaviors\n",
      "bordures statistiques pour la fouille incrémentale de données dans les data streams\n",
      "récemment la communauté extraction de connaissances s'est intéressée à de nouveaux modèles où les données arrivent séquentiellement sous la forme un flot rapide et continu ie les data streams une des particularités importantes de ces flots est que seule une quantité information partielle est disponible au cours du temps ainsi après différentes mises à jour successives il devient indispensable de considérer incertitude inhérente à information retenue dans cet article nous introduisons une nouvelle approche statistique en biaisant les valeurs supports pour les motifs fréquents cette dernière a avantage de maximiser un des deux paramètres précision ou rappel déterminés par utilisateur tout en limitant la dégradation sur le paramètre non choisi pour cela nous définissons les notions de bordures statistiques celles-ci constituent les ensembles de motifs candidats qui s'avèrent très pertinents à utiliser dans le cas de la mise à jour incrémentale des streams les différentes expérimentations effectuées dans le cadre de recherche de motifs séquentiels ont montré intérêt de approche et le potentiel des techniques utilisées\n",
      "carte auto-organisatrice probabiliste sur données binaires\n",
      "lesméthodes factorielles analyse exploratoire statistique définissentdes directions orthogonales informatives à partir un ensemble de données.elles conduisent par exemple à expliquer les proximités entre individus à l'aideun groupe de variables caractéristiques.dans le contexte du datamining lorsqueles tableaux de données sont de grande taille une méthode de cartographie synthétiques'avère intéressante ainsi une carte auto-organisatrice som) est uneméthode de partitionnement munie une structure de graphe de voisinage -surles classes- le plus souvent planaire des travaux récents sont développés pourétendre le som probabiliste generative topographic mapping gtm) aux modèlesde mélanges classiques pour données discrètes dans ce papier nous présentonset étudions un modèle génératif symétrique de carte auto-organisatricepour données binaires que nous appelons bernoulli aspect topological model(batm) nous introduisons un nouveau lissage et accélérons la convergence del'estimation par une initialisation originale des probabilités en jeu\n",
      "champs de markov conditionnels pour le traitement de séquences\n",
      "les modèles conditionnels du type modèles de markov entropiemaximale et champs de markov conditionnels apportent des réponses auxlacunes des modèles de markov cachés traditionnellement employés pour laclassification et la segmentation de séquences ces modèles conditionnels ontété essentiellement utilisés jusqu'à présent dans des tâches extractioninformation ou étiquetage morphosyntaxique cette contribution explorel'emploi de ces modèles pour des données de nature différente de type« signal » telles que la parole ou écriture en ligne nous proposons desarchitectures de modèles adaptées à ces tâches pour lesquelles nous avonsdérivé les algorithmes inférence et apprentissage correspondant nousfournissons des résultats expérimentaux pour deux tâches de classification etétiquetage de séquences\n",
      "choix du taux élagage pour extraction de la terminologie une approche fondée sur les courbes roc\n",
      "le choix du taux élagage est crucial dans le but acquérir une terminologiede qualité à partir de corpus de spécialité cet article présente uneétude expérimentale consistant à déterminer le taux élagage le plus adapté.plusieurs mesures évaluation peuvent être utilisées pour déterminer ce tauxtels que la précision le rappel et le fscore cette étude s'appuie sur une autremesure évaluation qui semble particulièrement bien adaptée pour extractionde la terminologie  les courbes roc receiver operating characteristics)\n",
      "classification un tableau de contingence et modèle probabiliste\n",
      "ces dernières années la classification croisée ou classification parblocs c'est-à-dire la recherche simultanée une partition des lignes et unepartition des colonnes un tableau de données est devenue un outil très utiliséen fouille de données dans ce domaine information se présente souvent sousforme de tableaux de contingence ou tableaux de co-occurrence croisant les modalitésde deux variables qualitatives dans cet article nous étudions le problèmede la classification croisée de ce type de données en nous appuyant sur un modèlede mélange probabiliste en utilisant approche vraisemblance classifiante,nous proposons un algorithme de classification croisée basé sur la maximisationalternée de la vraisemblance associée à deux mélanges multinomiaux classiqueset nous montrons alors que sous certaines contraintes restrictives on retrouveles critères du chi2 et de information mutuelle des résultats sur des donnéessimulées et des données réelles illustrent et confirment efficacité et intérêt decette approche\n",
      "classification de documents xml à partir une représentation linéaire des arbres de ces documents\n",
      "cet article présente un nouveau modèle de représentation pour la classificationde documents xml notre approche permet de prendre en compte soitla structure seule soit la structure et le contenu de ces documents idée estde représenter un document par ensemble des sous-chemins de arbre xmlde longueur comprise entre n et m deux valeurs fixées a priori ces cheminssont ensuite considérés comme de simples mots sur lesquels on peut appliquerdes méthodes standards de classification par exemple k-means nous évaluonsnotre méthode sur deux collections la collection inex et les rapports activitéde l'inria nous utilisons un ensemble de mesures bien connues dans le domainede la recherche information lorsque les classes sont connues a priorilorsqu'elles ne sont pas connues nous proposons une analyse qualitative desrésultats qui s'appuie sur les mots chemins) les plus caractéristiques des classesgénérées\n",
      "classification des comptes-rendus mammographiques à partir une ontologie radiologique en owl\n",
      "dans cet article nous proposons un système de classification descomptes-rendus mammographiques reposant sur une ontologie radiologiquedécrivant les signes radiologiques et les différentes classes de la classificationacr des systèmes birads dans le langage owl le système est conçu pour,extraire les faits issus des textes libres de comptes-rendus en étant dirigé parl'ontologie puis inférer la classe correspondante et en déduire attitude à tenirà partir de la classification acr ce travail présente la construction une ontologieradiologique mammaire dans le langage owl et son intérêt pour classerautomatiquement les comptes-rendus de mammographies\n",
      "classification non-supervisée de données relationnelles\n",
      "nan\n",
      "classifications hiérarchiques factorielles de variables\n",
      "on présente deux méthodes de classification hiérarchique ascendantede variables quantitatives et de fréquences chaque noeud de ces hiérarchiesregroupe deux classes de variables à partir une analyse factorielle particulièrebasée sur les variables représentatives de ces deux classes par cette méthode,on dispose à chaque pas un plan factoriel permettant de représenter àla fois les variables des deux classes fusionnées et ensemble des individus.ces derniers se positionnent dans ce plan suivant leurs valeurs pour les variablesconsidérées ainsi interprétation des noeuds obtenus s'effectue facilementà partir de examen de ces représentations factorielles la répartition desindividus observée dans chacun de ces plans factoriels permet également dedéfinir une segmentation des individus en total accord avec la hiérarchie desvariables obtenues on montre le fonctionnement des méthodes sur des exemplesréels\n",
      "clustering dynamique un flot de données  un algorithme incrémental et optimal de détection des maxima de densité\n",
      "extraction non supervisée et incrémentale de classes sur un flot dedonnées data stream clustering est un domaine en pleine expansion la plupartdes approches visent efficacité informatique la nôtre bien que se prêtantà un passage à échelle en mode distribué relève une problématiquequalitative applicable en particulier au domaine de la veille informationnelle :faire apparaître les évolutions fines les « signaux faibles » à partir des thématiquesextraites un flot de documents notre méthode germen localise defaçon exhaustive les maxima du paysage de densité des données à instant t,en identifiant les perturbations locales du paysage à t-1 et modifications defrontières induites par le document présenté son caractère optimal provient deson exhaustivité à une valeur du paramètre de localité correspond un ensembleunique de maxima et un découpage unique des classes qui la rend indépendantede tout paramètre initialisation et de ordre des données\n",
      "combinaison de approche inductive progressive) et linguistique pour étiquetage morphosyntaxique des corpus de spécialité\n",
      "les étiqueteurs morphosyntaxiques sont de plus en plus performantset cependant un véritable problème apparaît lorsque nous voulons étiqueterdes corpus de spécialité pour lesquels nous n'avons pas de corpus annotés lacorrection des ambiguïtés difficiles est une étape importante pour obtenir uncorpus de spécialité parfaitement étiqueté pour corriger ces ambiguïtés et diminuerle nombre de fautes nous utilisons une approche itérative appelée inductionprogressive cette approche est une combinaison apprentissage automatique,de règles rédigées par expert et de corrections manuelles qui secombinent itérativement afin obtenir une amélioration de étiquetage tout enrestreignant les actions de expert à la résolution de problèmes de plus en plusdélicats approche proposée nous a permis obtenir un corpus de biologiemoléculaire « correctement » étiqueté en utilisant ce corpus nous avons effectuéune étude comparative de quatre étiqueteurs supervisés\n",
      "comment formaliser les connaissances tacites une organisation  le cas de la conduite du changement à la sncf\n",
      "nan\n",
      "comparaison de deux modes de représentation de données faiblement structurées en sciences du vivant\n",
      "cet article présente deux modes de représentation de informationdans le cadre une problématique en sciences du vivant le premier appliqué àla microbiologie prévisionnelle s'appuie sur deux formalismes le modèle relationnelet les graphes conceptuels interrogés uniformément via une même interface.le second appliqué aux technologies des céréales utilise le seul modèlerelationnel cet article décrit les caractéristiques des données et compare les solutionsde représentation adoptées dans les deux systèmes\n",
      "comparaison de dissimilarités pour analyse de usage un site web\n",
      "obtention une classification des pages un site web en fonctiondes navigations extraites des fichiers \"logs\" du serveur peut s'avérer très utilepour évaluer adéquation entre la structure du site et attente des utilisateurs onconstruit une telle typologie en s'appuyant une mesure de dissimilarité entre lespages définie à partir des navigations le choix de la mesure la plus appropriéeà analyse du site est donc fondamental dans cet article nous présentons unsite de petite taille dont les pages sont classées en catégories sémantiques parun expert nous confrontons ce classement aux partitions obtenues à partir dediverses dissimilarités afin en étudier les avantages et inconvénients\n",
      "comparaison des mammographies par des méthodes apprentissage\n",
      "nan\n",
      "comparaison des mesures intérêt de règles association  une approche basée sur des graphes de corrélation\n",
      "le choix des mesures intérêt mi) afin évaluer les règles associationest devenu une question importante pour le post-traitement des connaissanceen ecd dans la littérature de nombreux auteurs ont discuté et comparéles propriétés des mi afin améliorer le choix des meilleures mesures cependant,il s'avère que la qualité une règle est contextuelle  elle dépend à la fois dela structure de données et des buts du décideur ainsi certaines mesures peuventêtre appropriées dans un certain contexte mais pas dans d'autres dans cet article,nous présentons une nouvelle approche contextuelle mise en applicationpar un nouvel outil arqat permettant à un décideur évaluer et de comparerle comportement des mi sur ses jeux de données spécifiques cette approche estbasée sur analyse visuelle un graphe de corrélation entre des mi objectives.nous employons ensuite cette approche afin de comparer et de discuter le comportementde trente-six mesures intérêt sur deux ensembles de données a prioritrès opposés  un premier dont les données sont fortement corrélées et un secondaux données faiblement corrélées alors que nous attendions des différences importantesentre les graphes de corrélation de ces deux jeux d'essai nous avonspu observer des stabilités de corrélation entre certaines mi qui sont révélatricesde propriétés indépendantes de la nature des données observées ces stabilitéssont récapitulées et analysées\n",
      "confrontation de points de vue dans le système porphyry\n",
      "nan\n",
      "credit scoring statistique et apprentissage\n",
      "basel 2 regulations brought new interest in supervised classification methodologies for predicting default probability for loans an important feature of consumer credit is that predictors are generally categorical logistic regression and linear discriminant analysis are the most frequently used techniques but are often unduly opposed vapnik's statistical learning theory explains why a prior dimension reduction eg by means of multiple correspondence analysis improves the robustness of the score function ridge regression linear svm pls regression are also valuable competitors predictive capability is measured by auc or gini's index which are related to the well known non-parametric wilcoxon-mann-whitney test among methodological problems reject inference is an important one since most samples are subject to a selection bias there are many methods none being satisfactory distinguish between good and bad customers is not enough especially for long-term loans the question is then not only “if” but “when” the customers default survival analysis provides new types of scores\n",
      "critère vt100 de sélection des règles association\n",
      "extraction de règles association génère souvent un grand nombrede règles pour les classer et les valider de nombreuses mesures statistiquesont été proposées  elles permettent de mettre en avant telles ou telles caractéristiquesdes règles extraites elles ont pour point commun être fonctioncroissante du nombre de transactions et aboutissent bien souvent àl'acceptation de toutes les règles lorsque la base de données est de grandetaille dans cet article nous proposons une mesure inspirée de la notion de valeur-test elle présente comme principale caractéristique être insensible à lataille de la base évitant ainsi écueil des règles fallacieusement significatives.elle permet également de mettre sur un même pied et donc de les comparer,des règles qui auront été extraites de bases de données différentes elle permetenfin de gérer différents seuils de signification des règles le comportement dela mesure est détaillé sur un exemple\n",
      "de analyse didactique à la modélisation informatique pour la conception un eiah en chirurgie orthopédique\n",
      "objet de la recherche présentée est de concevoir un environnementinformatique apprentissage qui permette de réduire écart entre la formationthéorique des chirurgiens et leur formation pratique qui se dérouleprincipalement sur le mode du compagnonnage article expose laméthodologie et quelques illustrations du travail didactique analyse desconnaissances et du système enseignement / apprentissage en milieuhospitalier chirurgie orthopédique ainsi que partie de la formalisationinformatique de cette connaissance cette modélisation permet la prise encompte dans environnement informatique de connaissances pragmatiquespour le diagnostic des connaissances de utilisateur en fonction des actionsqu'il effectue à interface pendant la résolution un problème pose de visdans le bassin) et la prise de décision didactique qui suit  quelle rétroactionfournir pour affiner le diagnostic et/ou permettre apprentissage souhaité\n",
      "définition et diffusion de signatures sémantiques dans les systèmes pair-à-pair\n",
      "les systèmes pair-à-pair (peer-to-peer p2p égal-à-égal se sont popularisésces dernières années avec les systèmes de partage de fichiers sur internet.de nombreuses recherches concernant optimisation de la localisationdes données ont émergé et constituent un axe de recherche très actif la priseen compte de la sémantique du contenu des pairs dans le routage des requêtespermet améliorer considérablement la localisation des données nous nousconcentrons sur approche planetp faisant usage de la notion de filtre de bloom,qui consiste à propager une signature sémantique des pairs filtres de bloom àtravers le réseau nous présentons cette approche et en proposons une amélioration la création de filtres de bloom dynamiques dans le sens où leur tailledépend de la charge des pairs nombre de documents partagés)\n",
      "des motifs séquentiels généralisés aux contraintes de temps étendues\n",
      "dans de nombreux domaines la recherche de connaissances temporellesest très appréciée des techniques ont été proposées aussi bien en fouille dedonnées qu'en apprentissage afin extraire et de gérer de telles connaissances,en les associant également à la spécification de contraintes temporelles (e.g. fenêtretemporelle maximale) notamment dans le contexte de la recherche de motifsséquentiels cependant ces contraintes sont souvent trop rigides ou nécessitentune bonne connaissance du domaine pour ne pas extraire des informationserronées c'est pourquoi nous proposons une approche basée sur la constructionde graphes de séquences afin de prendre en compte des contraintes de tempsplus souples ces contraintes sont relâchées par rapport aux contraintes de tempsprécédemment proposées elles permettent donc extraire plus de motifs pertinents.afin de guider analyse des motifs obtenus nous proposons égalementun niveau de précision des contraintes temporelles pour les motifs extraits\n",
      "eda  algorithme de désuffixation du langage médical\n",
      "nan\n",
      "enrichissement ontologies dans le secteur de eau douce en environnement internet distribué et multilingue\n",
      "nan\n",
      "esiea datalab logiciel de nettoyage et préparation de données\n",
      "nan\n",
      "exploration des paramètres discriminants pour les représentations vectorielles de la sémantique des mots\n",
      "les méthodes de représentation sémantique des mots à partir une analyse statistique sont basées sur des comptes de co-occurences entre mots et unités textuelles ces méthodes ont des paramétrages complexes notamment le type unité textuelle utilisée comme contexte ces paramètres déterminent fortement la qualité des résultats obtenus dans cet article nous nous intéressons au paramètrage de la technique dite hyperspace analogue to language hal).nous proposons une nouvelle méthode pour explorer ses paramètres discriminants cette méthode est basée sur analyse un graphe de voisinage une liste de mots de référence pré-classés nous expérimentons cette méthode et en donnons les premiers résultats qui renforcent et complètent des résultats issus de travaux précédents\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exploration interactive de bases de connaissances  un retour expérience\n",
      "la navigation au sein de bases de connaissances reste un problèmeouvert s'il existe plusieurs paradigmes de visualisation peu de travaux sur lesretours expérience sont disponibles dans le cadre de cet article nous noussommes intéressés aux différents paradigmes de navigation interactive au seinde bases documentaires annotées sémantiquement  accès à la base deconnaissances s'effectuant à travers ontologie du domaine d'application cesparadigmes ont été évalués dans le cadre une application industrielle(mécanique des fluides et échangeurs thermiques en fonction de critèresdéfinis par les utilisateurs analyse des retours expérience1 nous a permisde spécifier et de réaliser un nouveau navigateur dédié à la gestion dedocuments techniques annotés par une ontologie de domaine  le « eye tree »,navigateur de type « polar fisheye view »\n",
      "extension de algorithme cure aux fouilles de données volumineuses\n",
      "nan\n",
      "extraction automatique de champs numériques dans des documents manuscrits\n",
      "nous décrivons dans cet article une chaine de traitement complète etgénérique permettant extraire automatiquement les champs numériques numérosde téléphone codes clients codes postaux dans des documents manuscritslibres notre chaïne de traitement est constituée des trois étapes suivantes:localisation des champs numériques potentiels selon une approche markoviennesans reconnaissance chiffre ni segmentation reconnaissance des séquences extraites,et vérification des hypothèses de localisation / reconnaissance en vue delimiter la fausse alarme génerée lors de étape de localisation évaluation denotre système sur une base de 300 courriers manuscrits montre des performancesen rappel-précision intéressantes\n",
      "extraction objets vidéo  une approche combinant les contours actifs et le flot optique\n",
      "dans cet article nous présentons une méthode mixte de segmentationobjets visuels dans une séquence images une vidéo combinant à la foisune segmentation basée régions et estimation de mouvement par flot optique.l'approche développée est basé sur une minimisation une fonctionnelled'énergie e) qui fait intervenir les probabilités appartenance densité) avecune gaussienne en tenant compte des informations perceptuelles de couleur etde texture des régions d'intérêt pour améliorer la méthode de détection et desuivi nous avons étendu la formulation énergétique de notre modèle decontour actif en incluant une force supplémentaire issue du calcul du flot optique.nous montrons intérêt de cette approche mixte en terme de temps de calculet extraction objets vidéo complexes et nous présentons les résultatsobtenus sur des séquences de corpus vidéo couleur\n",
      "extraction de motifs séquentiels dans les flots de données usage du web\n",
      "ces dernières années de nouvelles contraintes sont apparues pour lestechniques de fouille de données ces contraintes sont typiques un nouveaugenre de données  les “data streams” dans un processus de fouille appliquésur un data stream utilisation de la mémoire est limitée de nouveaux élémentssont générés en permanence et doivent être traités le plus rapidement possible,aucun opérateur bloquant ne peut être appliqué sur les données et celles-ci nepeuvent être observées qu'une seule fois a heure actuelle la majorité des travauxrelatifs à extraction de motifs dans les data streams ne concernent pas lesmotifs temporels nous montrons dans cet article que cela est principalement dûau phénomène combinatoire qui est lié à extraction de motifs séquentiels nousproposons alors un algorithme basé sur alignement de séquences pour extraireles motifs séquentiels dans les data streams afin de respecter la contrainte unepasse unique sur les données une heuristique gloutonne est proposée pour segmenterles séquences nous montrons enfin que notre proposition est capabled'extraire des motifs pertinents avec un support très faible\n",
      "extraction de relations dans les documents web\n",
      "nous présentons un système pour inférence de programmes extraction de relations dans les documents web il utilise les vues textuelle et structurelle sur les documents extraction des relations est incrémentale et utilise des méthodes de composition et d'enrichissement nous montrons que notre système est capable extraire des relations pour les organisations existantes dans les documents web (listes  tables tables tournées tables croisées)\n",
      "extraction et identification entités complexes à partir de textes biomédicaux\n",
      "nous présentons ici un système extraction et identification entitésnommées complexes à intention des corpus de spécialité biomédicale nousavons développé une méthode qui repose sur une approche mixte à base ensemblede règles a priori et de dictionnaires contrôlés cet article expose lestechniques que nous avons mises en place pour éviter ou minimiser les problèmesde synonymie de variabilité des termes et pour limiter la présence denoms ambigus nous décrivons intégration de ces méthodes au sein du processusde reconnaissance des entités nommées intérêt de cet outil réside dans lacomplexité et hétérogénéité des entités extraites cette méthode ne se limitepas à la détection des noms des gènes ou des protéines mais s'adapte à autresdescripteurs biomédicaux nous avons expérimenté cette approche en mesurantles performances obtenues sur le corpus de référence genia\n",
      "extraction multilingue de termes à partir de leur structure morphologique\n",
      "nan\n",
      "fabr-cl  méthode de classification croisée de protéines\n",
      "dans cet article nous proposons une méthode de classification croiséepermettant de classer des protéines une part et de classer des descripteurs 3-grammes) selon leurs pertinences par rapport aux groupes de protéines obtenus,d'autres part\n",
      "faire vivre un référentiel métier dans industrie  le système de gestion de connaissances icare\n",
      "la gestion des connaissances enjeu majeur pour l'industrie est entréedans une phase concrète de déploiement la conjonction une maturitédes organisations dans la maîtrise de leur métier la consolidation de méthodeset les outils évolutifs pour faire vivre un patrimoine de connaissances favorisentl'émergence de projets significatifs et leur diffusion opérationnelle au seinde grands groupes industriels icare chez psa peugeot citroën réalisé avecl'environnement ardans knowledge maker en est ici l'exemple\n",
      "fast-mgb  nouvelle base générique minimale de règles associatives\n",
      "le problème de exploitation des règles associatives est devenu primordial,puisque le nombre des règles associatives extraites des jeux de donnéesréelles devient très élevé une solution possible consiste à ne dériver qu'unebase générique de règles associatives cet ensemble de taille réduite permet degénérer toutes les règles associatives via un système axiomatique adéquat danscet article nous proposons une nouvelle approche fast-mgb qui permet dedériver directement à partir du contexte extraction formel une base génériqueminimale de règles associatives\n",
      "finding fragments of orders and total orders from 0-1 data\n",
      "high-dimensional collections of 0-1 data occur in many applications the attributes insuch data sets are typically considered to be unordered however in many cases there is anatural total or partial order underlying the variables of the data set examples of variablesfor which such orders exist include terms in documents and paleontological sites in fossil datacollections we describe methods for finding fragments of total orders from such data basedon finding frequently occurring patterns we also discuss techniques for finding good totalorderings seriation) based on spectral ordering and mcmc methods\n",
      "fouille de données dans les systèmes pair-à-pair pour améliorer la recherche de ressources\n",
      "la quantité de sources information disponible sur internet fait dessystèmes échanges pair-à-pair p2p) un genre nouveau architecture qui offreà une large communauté des applications pour partager des fichiers des calculs,dialoguer ou communiquer en temps réel dans cet article nous proposonsune nouvelle approche pour améliorer la localisation une ressource sur un réseaup2p non structuré en utilisant une nouvelle heuristique nous proposonsd'extraire des motifs qui apparaissent dans un grand nombre de noeuds du réseau.cette connaissance est très utile pour proposer aux utilisateurs des fichierssouvent demandés en requête ou en téléchargement et éviter une trop grandeconsommation de la bande passante\n",
      "fouille de données spatiales approche basée sur la programmation logique inductive\n",
      "ce qui caractérise la fouille de données spatiales est la nécessité de prendre en compte les interactions des objets dans l'espace les méthodes classiques de fouille de données sont mal adaptées pour ce type d'analyse nous proposons dans cet article une approche basée sur la programmation logique inductive elle se base sur deux idées la première consiste à matérialiser ces interactions spatiales dans des tables de distances ramenant ainsi la fouille de données spatiales à la fouille de fonnées multi-tables la seconde transforme les données en logique du premier ordre et applique ensuite la programmation logique inductive cet article présentera cette approche il décrira son application à la classification supervisée par arbre de décision spatial il présentera aussi les expérimentations réalisées et les résultats obtenus sur analyse de la contamination des coquillages dans la lagune de thau\n",
      "gestion de connaissances  compétences et ressources pédagogiques\n",
      "nan\n",
      "graphes de voisinage pour indexation et interrogation images par le contenu\n",
      "la découverte informations cachées dans les bases de données multimédiasest une tâche difficile à cause de leur structure complexe et à la subjectivitéliée à leur interprétation face à cette situation utilisation un indexest primordiale un index multimédia permet de regrouper les données selondes critères de similarité nous proposons dans cet article apporter une améliorationà une approche déjà existante interrogation images par le contenu .nous proposons une méthode efficace pour mettre à jour localement les graphesde voisinage qui constituent notre structure index multimédia cette méthodeest basée sur une manière intelligente de localisation de points dans un espacemultidimensionnel des résultats prometteurs sont obtenus après des expérimentationssur diverses bases de données\n",
      "i-semantec  une plateforme collaborative de capitalisation des connaissances métier en conception de produits industriels\n",
      "nan\n",
      "indexation de vues virtuelles dans un médiateur xml pour le traitement de xquery text\n",
      "intégrer le traitement de requêtes de recherche information dans unmédiateur xml est un problème difficile ceci est notamment dû au fait quecertaines sources de données ne permettent pas de recherche sur mot-clefs etdistance ni de classer les résultats suivant leur pertinence dans cet article nousabordons intégration des fonctionnalités principales du standard xquery textdans xlive un médiateur xml/xquery pour cela nous avons choisid'indexer des vues virtuelles de documents les documents virtuelssélectionnés sont transformés en objets des sources opérateur de sélectiondu médiateur est étendu pour supporter des recherches information sur lesdocuments de la vue la recherche sur mots-clefs et le classement de résultatsont ainsi supportés notre formule de classement de résultats est adaptée auformat de données semi-structurées basé sur le nombre de mots-clefs dans lesdifférents éléments et la distance entre les éléments un résultat\n",
      "interrogation et vérification de documents owl dans le modèle des graphes conceptuels\n",
      "owl est un langage pour la description ontologies sur le web cependant,en tant que langage owl ne fournit aucun moyen pour interpréter lesontologies qu'il décrit et étant orienté machine il reste difficilement compréhensiblepar l'humain on propose une approche de visualisation interrogationet de vérification de documents owl regroupées dans un unique environnementgraphique  le modèle des graphes conceptuels\n",
      "la fouille de graphes dans les bases de données réactionnelles au service de la synthèse en chimie organique\n",
      "la synthèse en chimie organique consiste à concevoir de nouvellesmolécules à partir de réactifs et de réactions les experts de la synthèse s'appuientsur de très grandes bases de données de réactions qu'ils consultent à traversdes procédures interrogation standard un processus de découverte denouvelles réactions leur permettrait de mettre au point de nouveaux procédés desynthèse cet article présente une modélisation des réactions par des graphes etintroduit une méthode de fouille de ces graphes de réaction qui permet de faireémerger des motifs génériques utiles à la prédiction de nouvelles réactions enfinl'article fait le point sur état actuel de ce travail de recherche en présentantle modèle général dans lequel s'intégrera un nouvel algorithme de fouille deréactions chimiques\n",
      "le forage distribué des données  une méthode simple rapide et efficace\n",
      "dans cet article nous nous attaquons au problème du forage de trèsgrandes bases de données distribuées le résultat visé est un modèle qui soit etprédictif et descriptif appelé méta-classificateur pour ce faire nous proposonsde miner à distance chaque base de données indépendamment puis il s'agitde regrouper les modèles produits appelés classificateurs de base) sachant quechaque forage produira un modèle prédictif et descriptif représenté pour nos besoinspar un ensemble de règles de classification afin de guider assemblage del'ensemble final de règles qui sera union des ensembles individuels de règlesun coefficient de confiance est attribué à chaque règle de chaque ensemble cecoefficient calculé par des moyens statistiques représente la confiance que nouspouvons avoir dans chaque règle en fonction de sa couverture et de son taux erreurface à sa capacité être appliquée correctement sur de nouvelles données.nous démontrons dans cet article que grâce à ce coefficient de confiance agrégationpure et simple de tous les classificateurs de base pour obtenir un agrégatde règles produit un méta-classificateur rapide et efficace par rapport aux techniquesexistantes\n",
      "maintaining an online bibliographical database the problem of data quality\n",
      "citeseer and google-scholar are huge digital libraries which provideaccess to computer-)science publications both collections are operated likespecialized search engines they crawl the web with little human interventionand analyse the documents to classify them and to extract some metadata fromthe full texts on the other hand there are traditional bibliographic data baseslike inspec for engineering and pubmed for medicine for the field of computerscience the dblp service evolved from a small specialized bibliographyto a digital library covering most subfields of computer science the collectionsof the second group are maintained with massive human effort on the longterm this investment is only justified if data quality of the manually maintainedcollections remains much higher than that of the search engine style collections.in this paper we discuss management and algorithmic issues of data quality wefocus on the special problem of person names\n",
      "méthode de récolte de traces de navigation sur interface graphique et visualisation de parcours\n",
      "nan\n",
      "modèle conceptuel pour bases de données multidimensionnelles annotées\n",
      "nos travaux visent à proposer une mémoire expertises décisionnellespermettant de conserver et de manipuler non seulement les données décisionnellesmais aussi expertise analytique des décideurs les données décisionnellessont représentées au travers de concepts multidimensionnels etexpertise associée est matérialisée grâce au concept annotation\n",
      "modèle décisionnel basé sur la qualité des données pour sélectionner les règles associations légitimement intéressantes\n",
      "dans cet article nous proposons exploiter des mesures décrivant laqualité des données pour définir la qualité des règles associations résultantd'un processus de fouille nous proposons un modèle décisionnel probabilistebasé sur le coût de la sélection de règles légitimement potentiellement intéressantesou inintéressantes si la qualité des données à origine de leur calcul estbonne moyenne ou douteuse les expériences sur les données de kdd-cup-98 montrent que les 10 meilleures règles sélectionnées après leurs mesuresde support et confiance ne sont intéressantes que dans le cas où la qualité deleurs données est correcte voire améliorée\n",
      "modélisation informationnelle  un cadre méthodologique pour représenter des connaissances évolutives spatialisables\n",
      "pour comprendre et représenter les évolutions du bâti question renouvelée avec le développement des ntic analyste s'appuie sur des connaissances évolutives ayant dans notre champ application - le patrimoine architectural – un caractère spatialisable par attachement à un lieu lambda mais aussi des caractéristiques handicapantes (hétérogénéité incertitudes et contradictions etc) en réponse nous utilisons ce caractère spatialisable pour intégrer les ressources constituant le jeu de connaissances propre à chaque édifice théorie sources documentaires observations cette démarche que nous nommons modélisation informationnelle a pour objectif un gain de compréhension du lieu architectural et des informations qui lui sont associées notre contribution introduit les filiations de cette démarche le cadre méthodologique qui la matérialise et discute de son application au cas concret de la place centrale de cracovie rynek glowny pour en évaluer apport potentiel en matière de gestion et de visualisation de connaissances\n",
      "multi-catégorisation de textes juridiques et retour de pertinence\n",
      "la fouille de données textuelles constitue un champ majeur dutraitement automatique des données une large variété de conférences commetrec lui sont consacrées dans cette étude nous nous intéressons à la fouillede textes juridiques dans objectif est le classement automatique de ces textes.nous utilisons des outils analyses linguistiques extraction de terminologie)dans le but de repérer les concepts présents dans le corpus ces conceptspermettent de construire un espace de représentation de faible dimensionnalité,ce qui nous permet utiliser des algorithmes apprentissage basés sur desmesures de similarité entre individus comme les graphes de voisinage nouscomparons les résultats issus du graphe et de c4.5 avec les svm qui eux sontutilisés sans réduction de la dimensionnalité\n",
      "outil de datamining spatial appliqué à analyse des risques liés au territoire\n",
      "nan\n",
      "prédiction de solubilité de molécules à partir des seules données relationnelles\n",
      "la recherche de médicaments passe par la synthèse de molécules candidatesdont efficacité est ensuite testée ce processus peut être accéléré enidentifiant les molécules non solubles car celles-ci ne peuvent entrer dans lacomposition un médicament et ne devraient donc pas être étudiées des techniquesont été développées pour induire un modèle de prédiction de indice desolubilité utilisant principalement des réseaux de neurones ou des régressionslinéaires multiples la plupart des travaux actuels visent à enrichir les donnéesde caractéristiques supplémentaires sur les molécules dans cet article nous étudionsl'intérêt de la construction automatique attributs basée sur la structureintrinsèquement multi-relationnelle des données les attributs obtenus sont utilisésdans un algorithme arbre de modèles auquel on associe une méthodede bagging les tests réalisés montrent que ces méthodes donnent des résultatscomparables aux meilleures méthodes du domaine qui travaillent sur des attributsconstruits par les experts\n",
      "préparation des données radar pour la reconnaissance/identification de cibles aériennes\n",
      "la problématique générale présentée dans ce papier concerne lessystèmes intelligents dédiés pour aide à la prise de décision dans le domaineradar les premiers travaux ont donc consisté après avoir adapté le processusd'extraction de connaissances à partir de données ecd) au domaine radar àmettre en oeuvre les étapes en amont de la phase de fouille de données nousnous limitons dans ce papier à la phase de préparation des données imagesisar  inverse synthetic aperture radar) nous introduisons ainsi la notion dequalité comme moyen évaluer imperfection dans les données radarsexpérimentales\n",
      "prétraitement de grands ensembles de données pour la fouille visuelle\n",
      "nous présentons une nouvelle approche pour le traitement des ensemblesde données de très grande taille en fouille visuelle de données les limitesde approche visuelle concernant le nombre individus et le nombre dedimensions sont connues de tous pour pouvoir traiter des ensembles de donnéesde grande taille une solution possible est effectuer un prétraitement del'ensemble de données avant appliquer algorithme interactif de fouille visuelle.pour ce faire nous utilisons la théorie du consensus avec une affectationvisuelle des poids) nous évaluons les performances de notre nouvelle approchesur des ensembles de données de uci et du kent ridge bio medicaldataset repository\n",
      "recherche de règles non redondantes par vecteurs de bits dans des grandes bases de motifs \n",
      "nan\n",
      "recherche de sous-structures fréquentes pour intégration de schémas xml\n",
      "la recherche un schéma médiateur à partir un ensemble de schémasxml est une problématique actuelle où les résultats de recherche issusde la fouille de données arborescentes peuvent être adoptés dans ce contexte,plusieurs propositions ont été réalisées mais les méthodes de représentation desarborescences sont souvent trop coûteuses pour permettre un véritable passageà l'échelle dans cet article nous proposons des algorithmes de recherche desous-schémas fréquents basés sur une méthode originale de représentation deschémas xml nous décrivons brièvement la structure adoptée pour ensuitedétailler les algorithmes de recherche de sous-arbres fréquents s'appuyant surune telle structure la représentation proposée et les algorithmes associés ontété évalués sur différentes bases synthétiques de schémas xml montrant ainsil'intérêt de approche proposée\n",
      "recherche en temps réel de préfixes massifs hiérarchiques dans un réseau ip à aide de techniques de stream mining\n",
      "au cours de ces dernières années de nombreuses techniques de streammining ont été proposées afin analyser des flux de données en temps réel.dans cet article nous montrons comment nous avons utilisé des techniques destream mining permettant la recherche objets massifs hiérarchiques hierarchicalheavy hitters dans un flux de données pour identifier en temps réel dans unréseau ip les préfixes dont la contribution au trafic dépasse une certaine proportionde ce trafic pendant un intervalle de temps donné\n",
      "reconnaissance automatique évènements survenant sur patients en réanimation à aide une méthode adaptative extraction en ligne épisodes temporels\n",
      "ce papier présente la version adaptative un algorithmed'extraction épisodes temporels développé précédemment les trois paramè-tres de réglages de algorithme ne sont plus fixes ils sont modifiés en ligne enfonction de la variance estimée du signal que on veut décomposer en épiso-des temporels la version adaptative de algorithme a été utilisée pour recon-naître automatiquement des aspirations trachéales à partir de plusieures varia-bles physiologiques enregistrés sur des patients hospitalisés en réanimation.des résultats préliminaires sont présentés dans ce papier\n",
      "reconnaissance automatique de concepts à partir une ontologie\n",
      "ce papier présente une approche qui s'appuie sur une ontologie pourreconnaître automatiquement des concepts spécifiques à un domaine dans uncorpus en langue naturelle la solution proposée est non-supervisée et peuts'appliquer à tout domaine pour lequel une ontologie a été déjà construite uncorpus du domaine est utilisé dans lequel les concepts seront reconnus dansune première phase des connaissances sont extraites de ce corpus en faisantappel à des fouilles de textes une ontologie du domaine est utilisée pour étiqueterces connaissance le papier donne un aperçu des techniques de fouillesemployées et décrit le processus d ‘étiquetage les résultats d‘une premièreexpérimentation dans le domaine de accidentologie sont aussi présentés\n",
      "règles association avec une prémisse composée  mesure du gain d'information\n",
      "la communauté de fouille de données a développé un grand nombre indices permettantde mesurer la qualité des règles association ra) selon diverses sémantiques (guillet,2004) cependant ces sémantiques qui permettent interpréter les règles simples s'avèrentd'utilisation trop complexe pour un expert dans le cas de règles à prémisse composée notreobjectif est donc de sélectionner les règles à prémisse composée de type ab&#8594;c quiapportent une information supplémentaire à celle des règles simples a&#8594;c et b&#8594;c pourcela nous définissons un indice de gain une règle composée par rapport aux règles simplesdans application présentée nous extrayons des ra de résultats de classifications pouren faciliter analyse  le gain a permis de filtrer des règles interprétation simple\n",
      "représentation expertise psychologique sous la forme de graphes orientés codés en rdf\n",
      "nan\n",
      "représentation des connaissances appliquées à la géotechnique  une approche\n",
      "nan\n",
      "sélection de variables et modélisation expression émotion dans les dialogues homme-machine\n",
      "nan\n",
      "sélection supervisée instances  une approche descriptive\n",
      "la classification suivant le plus proche voisin est une règle simple etperformante sa mise en oeuvre pratique nécessite tant pour des raisons de coûtde calcul que de robustesse de sélectionner les instances à conserver la partitionde voronoi induite par les prototypes constitue la structure sous-jacente àcette règle dans cet article on introduit un critère descriptif évaluation unetelle partition quantifiant le compromis entre nombre de cellules et discriminationde la variable cible entre les cellules une heuristique optimisation estproposée tirant partie des propriétés des partitions de voronoi et du critère laméthode obtenue est comparée avec les standards sur une vingtaine de jeux dedonnées de l'uci notre technique ne souffre aucun défaut de performanceprédictive tout en sélectionnant un minimum d'instances de plus elle ne surapprendpas\n",
      "svm incrémental parallèle et distribué pour le traitement de grandes quantités de données\n",
      "nous présentons un nouvel algorithme de svm support vectormachine ou séparateur à vaste marge linéaire et non-linéaire parallèle etdistribué permettant le traitement de grands ensembles de données dans untemps restreint sur du matériel standard a partir de algorithme de newton-gsvm proposé par mangasarian nous avons construit un algorithmeincrémental parallèle et distribué permettant améliorer les performances entemps exécution et mémoire en s'exécutant sur un groupe d'ordinateurs cenouvel algorithme a la capacité de classifier un million individus en 20dimensions et deux classes en quelques secondes sur un ensemble de dix pc\n",
      "système aide à la décision pour la surveillance de la qualité de air intérieur\n",
      "nan\n",
      "techniques de fouille de données pour la réécriture de requêtes en présence de contraintes de valeurs\n",
      "dans cet article nous montrons comment les techniques de fouilles de données peuvent résoudre efficacement le problème de la réécriture de requêtes en termes de vues en présence de contraintes de valeurs a partir une formalisation du problème de la réécriture dans le cadre de la logique de description aln(ov) nous montrons comment ce problème se rattache à un cadre de découverte de connaissances dans les bases de données exploitation de ce cadre nous permet de bénéficier de solutions algorithmiques existantes pour la résolution du problème de réécriture nous proposons une implémentation de cette approche puis nous l'expérimentons les premiers résultats démontrent intérêt une telle approche en termes de capacité à traiter un grand nombre de sources de données\n",
      "teximus expertise  un logiciel de gestion de connaissances\n",
      "le logiciel teximus expertise est un outil évolué de gestion dynamiquede connaissances basé sur les notions de référentiel sémantique cette suiteintégrée facilite le partage de connaissances et informations dans les entreprises\n",
      "typicalité et contribution des sujets et des variables supplémentaires en analyse statistique implicative\n",
      "analyse statistique implicative traite des tableaux sujets xvariables afin extraire règles et métarègles statistiques entre les variables.l'article interroge les structures obtenues représentées par graphe et hiérarchieorientés afin de dégager la responsabilité des sujets ou des groupes de sujets(variables supplémentaires dans la constitution des chemins du graphe ou desclasses de la hiérarchie on distingue les concepts de typicalité pour signifier laproximité des sujets avec le comportement moyen de la population envers lesrègles statistiques extraites puis de contribution pour quantifier le rôlequ'auraient les sujets par rapport aux règles strictes associées un exemple dedonnées réelles traité à aide du logiciel chic illustre et montre intérêt deces deux concepts\n",
      "un automate pour évaluer la nature des textes\n",
      "on ne peut s'intéresser aux textes sans s'intéresser à leur nature la nature des textes permet de distinguer les textes un point de vue primaire elle est utilisée pour identifier les textes artificiels pour la reconnaissance de la langue afin identifier les spams en ce sens la méthode la plus connue reste encore la méthode de zipf cet article propose une nouvelle méthode basée sur un automate automate construit un signal pour chaque texte automate est présenté en détail et des expérimentations montrent son utilité dans les domaines aussi divers que ceux cités précédemment/\n",
      "un logiciel permettant apprendre des règles et leurs exceptions  area\n",
      "nan\n",
      "un modèle de qualité de information\n",
      "ce travail s'intègre dans la problématique générale de la recherched'information  et plus particulièrement dans la personnalisation et la qualitéd'information dans cet article nous proposons un modèle multidimensionnelde la qualité de information décrivant les différents facteurs de qualité influantsur la personnalisation de information. ce modèle permet de structurerles différents facteurs de qualité de information dans une hiérarchie afind'assister utilisateur dans la construction de son propre profil selon ses besoinset ses exigences en termes de qualité\n",
      "un modèle métier extensible adapté à la gestion de dépêches agences de presse\n",
      "nan\n",
      "une approche distribuée pour extraction de connaissances  application à enrichissement de aspect factuel des bdg\n",
      "les systèmes informations géographiques sig) sont utilisés pouraméliorer efficacité des entreprises et des services publics en associantméthodes optimisation et prise en compte de la dimension géographique.cependant les bases de données géographiques bdg) stockées dans les sigsont restreintes à application pour laquelle elles ont été conçues souvent lesutilisateurs demeurent contraints de existant et se trouvent dans le besoin dedonnées complémentaires pour une prise de décision adéquate d'où idée del'enrichissement de aspect descriptif des bdg existantes pour atteindre cetobjectif nous proposons une approche qui consiste à intégrer un module defouille de données textuelles au sig lui même il s'agit de proposer uneméthode distribuée de résumé de documents multiples à partir de corpus enligne.idée est de faire coopérer un ensemble agents s'entraidant afind'aboutir à un résumé optimal\n",
      "une approche multi-agent adaptative pour la simulation de schémas tactiques\n",
      "ce papier est consacré à la simulation ou à la réalisation automatiquede schémas tactiques par un groupe d´agents footballeurs autonomes son objectifest de montrer ce que peuvent apporter des techniques apprentissagepar renforcement à des agents réactifs conçus pour cette tâche dans un premiertemps nous proposons une plateforme et une architecture agents capabled'effectuer des schémas tactiques dans des cas relativement simples ensuite,nous mettons en oeuvre un algorithme apprentissage par renforcementpour permettre aux agents de faire face à des situations plus complexes enfin,une série expérimentations montrent le gain apporté aux agents réactifs parl'utilisation algorithmes apprentissage.\n",
      "une approche simple inspirée des réseaux sociaux pour la hiérarchisation des systèmes autonomes de internet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "le transit des flux information dans le réseau internet à échellemondiale est régi par des accords commerciaux entre systèmes autonomes accordsqui sont mis en oeuvre via le protocole de routage bgp la négociationde ces accords commerciaux repose implicitement sur une hiérarchie des systèmesautonomes et la position relative de deux systèmes débouche sur un accordde type client/fournisseur un des systèmes le client est nettement mieuxclassé que l'autre le fournisseur et le client paye le fournisseur pour le transitdes flux information) ou sur un accord de type \"peering\" transit gratuit dutrafic entre les deux systèmes) en dépit de son importance il n'existe pas dehiérarchie officielle de internet les clauses commerciales des accords entresystèmes autonomes ne sont pas nécessairement publiques ni de consensus surla façon établir une telle hiérarchie nous proposons une heuristique simpleinspirée de la notion de \"centralité spectrale\" issue de analyse des réseaux sociauxpour analyser la position relative des systèmes autonomes de internet àpartir des informations des seules informations de connectivité entre systèmesautonomes\n",
      "une comparaison de certains indices de pertinence des règles association\n",
      "cet article propose une comparaison graphique de certains indices depertinence pour évaluer intérêt des règles d'association nous nous sommesappuyés sur une étude existante pour sélectionner quelques indices auxquelsnous avons ajouté indice de jaccard et indice accords désaccords iad).ces deux derniers nous semblent plus adaptés pour discriminer les règles intéressantesdans le cas où les items sont des événements peu fréquents une applicationest réalisée sur des données réelles issues du secteur automobile\n",
      "une mesure de proximité et une méthode de regroupement pour aide à acquisition ontologies spécialisées\n",
      "cet article traite du regroupement unités textuelles dans une perspectived'aide à élaboration ontologies spécialisées le travail présenté s'inscritdans le cadre du projet biotim nous nous concentrons ici sur une desétapes de construction semi-automatique une ontologie qui consiste à structurerun ensemble unités textuelles caractéristiques en classes susceptibles dereprésenter les concepts du domaine approche que nous proposons s'appuiesur la dé\u0002nition une nouvelle mesure non-symétrique permettant évaluer laproximité entre lemmes en utilisant leurs contextes apparition dans les documents.en complément de cette mesure nous présentons un algorithme declassi\u0002cation non-supervisée adapté à la problématique et aux données traitées.les premières expérimentations présentées sur les données botaniques laissentpercevoir des résultats pertinents pouvant être utilisés pour assister expert dansla détermination et la structuration des concepts du domaine\n",
      "une nouvelle mesure sémantique pour le calcul de la similarité entre deux concepts une même ontologie\n",
      "les ontologies sont au coeur du processus de gestion des connaissances.différentes mesures sémantiques ont été proposées dans la littératurepour évaluer quantitativement importance de la liaison sémantique entre pairesde concepts cet article propose une synthèse analytique des principales mesuressémantiques basées sur une ontologie modélisée par un graphe et restreinte iciaux liens hiérarchiques is-a après avoir mis en évidence différentes limites desmesures actuelles nous en proposons une nouvelle la pss proportion of sharedspecificity) qui sans corpus externe tient compte de la densité des liens dans legraphe reliant deux concepts\n",
      "utilisation de métadonnées pour aide à interprétation de classes et de partitions\n",
      "les résultats des méthodes de fouille de données sont difficilementinterprétables par un utilisateur n'ayant pas expertise requise dans ce papiernous proposons un outil permettant aux utilisateurs interpréter les résultatsissus des méthodes de classification non supervisée cet outil est basé sur desmétadonnées utilisées pour formaliser le processus d'interprétationautomatique ces métadonnées vont servir à utilisateur pour comprendre dansquelles circonstances les données originales ont été collectées et de quellemanière elles ont été agrégées puis classifiées intérêt de ce travail porte surla souplesse qu'auront les utilisateurs à pouvoir interpréter facilement lesclasses obtenues nous développons notre approche basée sur utilisation desmétadonnées nous traduirons notre méthodologie par un exemple concret\n",
      "utilisation des réseaux bayésiens dans le cadre de extraction de règles association\n",
      "cet article aborde le problème de utilisation un modèle de connaissancedans un contexte de fouille de données approche méthodologique proposéemontre intérêt de la mise en oeuvre de réseaux bayésiens couplée à extractionde règles association dites delta-fortes membre gauche minimal fréquenceminimale et niveau de confiance contrôlé) la découverte de règles potentiellementutiles est alors facilitée par exploitation des connaissances décritespar expert et représentées dans le réseau bayésien cette approche estvalidée sur un cas application concernant la fouille de données interruptionsopérationnelles dans industrie aéronautique\n",
      "vers extraction de motifs rares\n",
      "un certain nombre de travaux en fouille de données se sont intéressés à extraction de motifs et à la génération de règles association à partir de ces motifs cependant ces travaux se sont jusqu'à présent centrés sur la notion de motifs fréquents le premier algorithme à avoir permis extraction de tous les motifs fréquents est apriori mais autres ont été mis au point par la suite certains n'extrayant que des sous-ensembles de ces motifs motifs fermés fréquents motifs fréquents maximaux générateurs minimaux) dans cet article nous nous intéressons aux motifs rares qui peuvent également véhiculer des informations importantes les motifs rares correspondent au complémentaire des motifs fréquents a notre connaissance ces motifs n'ont pas encore été étudiés malgré intérêt que certains domaines pourraient tirer de ce genre de modèle c'est en particulier le cas de la médecine où par exemple il est important pour un praticien de repérer les symptômes non usuels ou les effets indésirables exceptionnels qui peuvent se déclarer chez un patient pour une pathologie ou un traitement donné\n",
      "visualisation en gestion des connaissances développement un nouveau modèle graphique graph'atanor\n",
      "les systèmes de gestion des connaissances servent de support pour lacréation et la diffusion de mémoires entreprises qui permettent de capitaliser,conserver et enrichir les connaissances des experts dans ces systèmes interactionavec les experts est effectuée avec des outils adaptés dans lesquels uneformalisation graphique des connaissances est utilisée cette formalisation estsouvent basée au niveau théorique sur des modèles de graphes mais de façonpratique les représentations visuelles sont souvent des arbres et des limitationsapparaissent par rapport aux représentations basées sur des graphes dans cetarticle nous présentons le modèle utilisé par le serveur de connaissances atanorqui utilise des arbres pour visualiser les connaissances et nous développons unenouvelle approche qui permet de représenter les mêmes connaissances sous laforme de graphes en niveaux une analyse comparative des deux méthodes dansun contexte industriel de maintenance permet de mettre en valeur apport desgraphes dans le processus de visualisation graphique des connaissances\n",
      "visualisation interactive de données avec des méthodes à base de points intérêt\n",
      "nous présentons dans cet article une méthode de visualisation interactivede données numériques ou symboliques permettant à un utilisateur expertdu domaine obtenir des informations et des connaissances pertinentes nousproposons une approche nouvelle en adaptant utilisation des points intérêtsdans un contexte de fouille visuelle de données a partir un ensemble de pointsd'intérêt disposés sur un cercle les données sont visualisées à intérieur de cecercle en fonction de leur similarité à ces points d'intérêt des opérations interactivessont alors définies  sélectionner zoomer changer dynamiquement lespoints d'intérêts nous évaluons les propriétés une telle visualisation sur desdonnées aux caractéristiques connues nous décrivons une application réelle encours dans le domaine de exploration de données issues enquêtes de satisfaction\n",
      "web sémantique pour la mémoire expériences une communauté scientifique  le projet meat\n",
      "cet article décrit le projet meat mémoire expériences pourl'analyse du transcriptome dont le but est assister les biologistes travaillantdans le domaine des puces à adn pour interprétation et la validation de leursrésultats nous proposons une aide méthodologique et logicielle pour construireune mémoire expériences pour ce domaine notre approche basée surles technologies du web sémantique repose sur utilisation des ontologies etdes annotations sémantiques sur des articles scientifiques et autres sourcesde connaissances du domaine notre approche peut être généralisée à autresdomaines requérant des expérimentations et traitant un grand flux de données(protéomique chimie,etc)\n",
      "web usage mining  extraction de périodes denses à partir des logs\n",
      "les techniques de web usage mining existantes sont actuellementbasées sur un découpage des données arbitraire (eg \"un log par mois\" ou guidépar des résultats supposés (eg \"quels sont les comportements des clients pourla période des achats de noël  \") ces approches souffrent des deux problèmessuivants une part elles dépendent de cette organisation arbitraire des donnéesau cours du temps autre part elles ne peuvent pas extraire automatiquementdes \"pics saisonniers\" dans les données stockées nous proposons exploiterles données pour découvrir de manière automatique des périodes \"denses\" decomportements une période sera considérée comme \"dense\" si elle contient aumoins un motif séquentiel fréquent pour ensemble des utilisateurs qui étaientconnectés sur le site à cette période\n",
      "acka  une approche acquisition coopérative de connaissances pour la construction un modèle de simulation multi-agents\n",
      "cet article présente une approche acka an approach for cooperative knowledge acquisition participative et coopérative acquisition de connaissances nécessaires pour la construction un modèle de simulation basé sur des agents elle est basée sur le principe de jeu de rôles dans une réunion d'entreprise nous proposons de construire un modèle multi-acteurs représentant un modèle initial du système multi-agents dans cette étude nous appliquons acka pour construire un modèle multi-acteurs pour la compréhension des processus de décision dans les ?rmes de la ?liere avicole en particulier nous cherchons à comprendre les impacts des comportements individuels sur la gestion de utilisation des matières premières agricoles\n",
      "acquisition et exploitation de connaissances dans un contexte multi-experts pour un système aide à la décision\n",
      "nous présentons une méthodologie d'extraction de gestion et exploitation de connaissances dans un contexte multi-experts elle repose sur trois étapes  extraction des connaissances de chaque expert gestion des connaissances individuelles afin de constituer une base de connaissances commune et exploitation de cette base afin de fournir une aide à la décision aux experts la méthodologie proposée a été mise en œuvre au cameroun avec cinq experts en micro-finance elle a donné des résultats en adéquation avec les pratiques des experts au-delà on envisage de mettre en œuvre un système de capitalisation des connaissances il doit permettre analyser rapidement un plus grand nombre de situations les experts restant en nombre limité et contribuer à un transfert de compétences pour former les décideurs locaux en effet les experts sont en général membres ong et restent rarement plus de deux ans sur place\n",
      "aid  un framework intégré de conception un schéma objet-relationnel\n",
      "devant la prolifération des données complexes qui ne cessent de croître et la diversité des structures qui se multiplient la conception des schémas de base de données en général et des schémas objet-relationnels en particulier est devenue une activité difficile et complexe qui fait appel à des connaissances variées lors de la conception un schéma utilisateur non averti doit connaître la théorie sous-jacente au modèle de données de façon à énoncer son modèle syntaxiquement correct lui permettant de construire un schéma de base de données objet-relationnel répondant à ses besoins plusieurs outils spécialisés dans la conception de schémas de base de données provenant aussi bien de la communauté académique que du monde industriel tels super totem rational/rose etc ont été développés dans des contextes et avec des buts souvent très différents affin de répondre à ce besoin pressant nous avons proposé une solution consistant en élaboration environnements intégrés facilitant la cohabitation de plusieurs modèles et techniques utilisés lors de la conception un schéma de base de données il s'agit offrir une plate-forme logicielle appelée aid aided interface for database design offrant des mécanismes opératoires uniformes représentant un soutien graphique et interactif pour une conception incrémentale basée sur des manipulations directes et systémiques des graphes au travers une palette graphique d'opérateurs innovation aid est son approche systémique qui facilite expression des besoins par le concepteur averti ou non en lui automatisant sa tâche\n",
      "amélioration de la performance de analyse de la sémantique latente pour des corpus de petite taille\n",
      "nan\n",
      "analyse comparative de classifications  apport des règles association floues\n",
      "notre travail s'appuie sur analyse un corpus bibliographique dans le domaine de la géotechnique à aide de cartes réalisées avec la plateforme stanalyst® celui-ci intègre un algorithme de classification automatique non hiérarchique les k-means axiales donnant des résultats dépendant du nombre de classes demandé cette instabilité rend difficile toute comparaison entre classifications et laisse un doute quant au choix du nombre de classes nécessaire pour représenter correctement un domaine nous comparons les résultats de classifications selon 3 protocoles  1) analyse des intitulés des classes  2) relations entre les classes à partir des membres communs  3) règles association floues les graphes obtenus présentant des similitudes remarquables nous privilégions les règles association floues  elles sont extraites automatiquement et se basent sur la description des classes et non des membres ceci nous permet donc analyser des classifications issues de corpus différents\n",
      "analyse de données symboliques et graphe de connaissances un agent\n",
      "dans cet article nous appliquons analyse de données symboliques au graphe de connaissances un agent nous présentons une mesure de similarité entre des données symboliques adaptée à nos graphes de connaissances nous utilisons les pyramides symboliques pour extraire un nouvel objet symbolique le nouvel objet est ensuite réinséré dans le graphe où il peut être utilisé par l'agent faisant ainsi évoluer sa sémantique il peut alors servir individu lors des analyses ultérieures permettant de découvrir de nouveaux concepts prenant en compte évolution de la sémantique\n",
      "analyse géométrique des données pour affinement de la connaissance  cas des données epgy education program for gifted students stanford university\n",
      "nan\n",
      "analyse stochastique de séquences événements discrets pour la découverte de signatures\n",
      "cet article concerne la découverte de signatures ou modèles de chroniques à partir une séquence événements discrets alarmes) générée par un agent cognitif de surveillance monitoring cognitive agent ou mca.considérant un couple (processus mca comme un générateur stochastique événements discrets deux représentations complémentaires permettent de caractériser les propriétés stochastiques et temporelles un tel générateur  une chaîne de markov à temps continu et une superposition de processus de poisson étude de ces deux représentations duales permet de découvrir des \"signatures\" décrivant les relations stochastiques et temporelles entre événements dans une séquence ces signatures peuvent alors être utilisées pour reconnaître des comportements spécifiques comme le montre application de approche à un outil de production industriel piloté par un système sachem le mca développé et utilisé par le groupe arcelor pour aider au pilotage de ses outils de production\n",
      "annotation de textes par extraction informations lexicosyntaxiques  et acquisition de schémas conceptuels de causalité\n",
      "nous présentons la méthode insyse interface syntaxe semantique pour annotation de documents textuels notre objectif est de construire des annotations sémantiques de ces résumés pour interroger le corpus sur la fonction des gènes et leurs relations de causalité avec certaines maladies notre approche est semi-automatique centrée sur 1) extraction informations lexico-syntaxiques à partir de certaines phrases du corpus comportant des lexèmes de causation et 2) élaboration de règles basées sur des grammaires unification permettant acquérir à partir de ces informations des schémas conceptuels instanciés ceux-ci sont traduits en annotations rdf(s sur la base desquelles le corpus de textes peut être interrogé avec le moteur de recherche sémantique corese\n",
      "apprentissage automatique des modèles structurels objets cartographiques\n",
      "pour reconnaitre les objets cartographiques dans les images satellitales on a besoin un modèle objet qu'on recherche nous avons développé un système apprentissage qui construit le modèle structurel objets cartographiques automatiquement a partir des images satellitales segmentées les images contenants les objets sont décomposées en formes primitives et sont transformées en graphes relationnels attribués (args) nous avons généré les modèles objets a partir de ces graphes en utilisant des algorithmes appariement de graphes la qualité un modèle est évaluée par la distance édition des exemples a ce modèle nous sommes parvenus a obtenir des modèles de ponts et de ronds-points qui sont compatibles avec les modèles construits manuellement\n",
      "apprentissage de scénarios à partir de séries temporelles multivariées\n",
      "nan\n",
      "apprentissage de signatures de facteurs de transcription à partir de données expression\n",
      "inférence de signatures de facteurs de transcription à partir des données puces à adn a déjà été étudié dans la communauté bioinformatique la principale difficulté à résoudre est de trouver un ensemble heuristiques pertinentes afin de contrôler la complexité de résolution de ce problème np-difficile nous proposons dans cet article une solution heuristique alternative à celles utilisées dans les approches bayésiennes fondée sur la recherche de motifs fréquents maximaux dans une matrice discrétisée issue des données numériques de puces adn notre méthode est appliquée sur des données de cancer de vessie de institut curie et de hôpital henri mondor de créteil\n",
      "apprentissage de structures de réseaux bayésiens et données incomplètes\n",
      "le formalisme des modèles graphiques connait actuellement un essor dans les domaines du \"machine learning\" en particulier les réseaux bayésiens sont capables effectuer des raisonnements probabilistes à partir de données incomplètes alors que peu de méthodes sont actuellement capables utiliser les bases exemples incomplètes pour leur apprentissage en s'inspirant du principe de ams-em proposé par (friedman 1997 et des travaux de(chow & liu 1968) nous proposons une méthode permettant de faire apprentissage de réseaux bayésiens particuliers de structure arborescente à partir de données incomplètes une étude expérimentale expose ensuite des résultats préliminaires qu'il est possible attendre une telle méthode puis montre le gain potentiel apporté lorsque nous utilisons les arbres obtenus comme initialisation une méthode de recherche gloutonne comme ams-em\n",
      "apprentissage non supervisé de séries temporelles à aide des k-means et une nouvelle méthode agrégation de séries\n",
      "utilisation un algorithme apprentissage non supervisé de type k-means sur un jeu de séries temporelles amène à se poser deux questions  celle du choix une mesure de similarité et celle du choix une méthode effectuant agrégation de plusieurs séries afin en estimer le centre (ie calculer les k moyennes) afin de répondre à la première question nous présentons dans cet article les principales mesures de similarité existantes puis nous expliquons pourquoi une entre elles appelée dynamic time warping nous paraît la plus adaptée à apprentissage non supervisé la deuxième question pose alors problème car nous avons besoin une méthode agrégation respectant les caractéristiques bien particulières du dynamic time warping nous pensons que association de cette mesure de similarité avec agrégation euclidienne peut générer une perte informations importante dans le cadre un apprentissage sur la \"forme\" des séries nous proposons donc une méthode originale agrégation de séries temporelles compatible avec le dynamic time warping qui améliore ainsi les résultats obtenus à aide de algorithme des k-means\n",
      "apprentissage supervisé pour la classification des images basé sur la structure p-tree\n",
      "un problème important de la production automatique de règles de classification concerne la durée de génération de ces règles  en effet les algorithmes mis en œuvre produisent souvent des règles pendant un certain temps assez long nous proposons une nouvelle méthode de classification à partir une base de données images cette méthode se situe à la jonction de deux techniques  algèbre de p-tree et arbre de décision en vue accélérer le processus de classification et de recherche dans de grandes bases d'images la modélisation que nous proposons se base une part sur les descripteurs visuels tels que la couleur la forme et la texture dans le but indexer les images et autre part sur la génération automatique des règles de classification à aide un nouvel algorithme c45(p-tree) pour valider notre méthode nous avons développé un système baptisé c.i.a.d.p-tree qui a été implémenté et confronté à une application réelle dans le domaine du traitement d'images les résultats expérimentaux montrent que cette méthode réduit efficacement le temps de classification\n",
      "arbre de décision sur des données de type intervalle  évaluation et comparaison\n",
      "le critère de découpage binaire de kolmogorov-smirnov nécessite un ordre total des valeurs prises par les variables explicatives nous pouvons ordonner des intervalles fermés bornés de nombres réels de différentes façons notre contribution dans cet article consiste à évaluer et à comparer des arbres de décision obtenus sur des données de type intervalle à aide du critère de découpage binaire de kolmogorov-smirnov étendu à ce type de données mballo et al 2004) pour ce faire nous axons notre attention sur le taux erreur mesuré sur échantillon de test pour estimer ce paramètre nous divisons aléatoirement chaque base de données en deux parties égales en terme effectif à un objet près pour construire deux arbres ces deux arbres sont abord testés par un même échantillon puis par deux échantillons différents\n",
      "caractérisation une région intérêt dans les images\n",
      "une image est un support information qui a montré son efficacité néanmoins une image comporte souvent plusieurs zones arrière plan et une zone intérêt privilégiée la vision humaine permet la segmentation de manière naturelle et intégrant toute la connaissance que le sujet peut avoir de objectif visé par l'image nous proposons ici une méthode de détermination des régions intérêt une image numérique comme zones saillantes les lois de zipf et zipf inverse sont adaptées au traitement des images et permettent évaluer la complexité structurelle une image une comparaison des modèles locaux évalués sur des imagettes permet de mettre en évidence une région de l'image deux méthodes de classification ont été utilisées pour la détermination de la région intérêt  la partition un nuage de points représentant les caractéristiques associées aux imagettes et les réseaux de neurones cette méthode de détection permet obtenir des zones intérêt conformes à la perception humaine on opère une hiérarchisation sur les zones en fonction de la structuration de information élémentaire les pixels\n",
      "chic  traitement de données avec analyse implicative\n",
      "cet article a pour but de montrer les possibilités offertes par le logiciel chic classification hiérarchique implicative et cohésitive pour effectuer certaines analyses de données il est basé sur la théorie de analyse statistique implicative ou asi développée par régis gras et ses collaborateurs le principe premier de l'asi repose sur la problématique une mesure des règles association du type  «si a alors b» dans une population instanciant les variables a et b chic enrichit sa réponse établie sur des bases statistiques en évaluant la responsabilité des sujets dans élection de la règle article présent explique la démarche à suivre pour utiliser le logiciel ainsi que les possibilités offertes par celui-ci\n",
      "classification 2-3 hiéarchique de données du web\n",
      "nan\n",
      "classification un tableau de contingence et modèle probabiliste\n",
      "les modèles de mélange qui supposent que échantillon est formé de sous-populations caractérisées par une distribution de probabilité constitue un support théorique intéressant pour étudier la classification automatique on peut ainsi montrer que algorithme des k-means peut être vu comme une version classifiante de algorithme estimation em dans un cas particulièrement simple de mélange de lois normales lorsque on cherche à classifier les lignes ou les colonnes un tableau de contingence il est possible utiliser une variante de algorithme des k-means appelé mndki2 en s'appuyant sur la notion de profil et sur la distance du khi-2 on obtient ainsi une méthode simple et efficace pouvant s'utiliser conjointement à analyse factorielle des correspondances qui s'appuie sur la même représentation des données malheureusement et contrairement à algorithme des k-means classique les liens qui existent entre les modèles de mélange et la classification ne s'appliquent pas directement à cette situation dans ce travail nous montrons que algorithme mndki2 peut être associé à une approximation près à un modèle de mélange de lois multinomiales\n",
      "classification non supervisée et visualisation 3d de documents\n",
      "le nombre de documents issus une requête sur le web devient de plus en plus important cela nous amène à chercher des solutions pour aider utilisateur qui est confronté à cette masse de données une alternative possible à un affichage linéaire non triée selon un critère consiste à effectuer une classification des résultats c'est dans ce but que on s'intéresse aux cartes auto-organisatrices de kohonen qui sont issues un algorithme de classification non supervisée cependant il faut ajouter des contraintes à cet algorithme afin qu'il soit adapté à la classification des résultats une requête par exemple il doit être déterministe de plus la classification obtenue dépend fortement de la distance utilisée pour comparer deux documents on évalue alors impact de différentes distances ou dissimilarités afin de trouver la plus adaptée à notre problème un compromis doit également être trouvé entre le temps exécution de algorithme et la qualité de la classification obtenue pour cela utilisation un échantillonnage est envisagée enfin ces travaux sont intégrés dans un prototype qui permet de visualiser les résultats en trois dimensions et interagir avec eux\n",
      "classifying xml materialized views for their maintenance on distributed web sources\n",
      "ces dernières années ont mis en évidence la croissance et la diversité des informations électroniques accessibles sur le web c'est ainsi que les systèmes intégration de données tels que des médiateurs ont été conçus pour intégrer ces données distribuées et hétérogènes dans une vue uniforme pour faciliter intégration des données à travers différents systèmes xml a été adopté comme format standard pour échanger des informations xquery est un langage intégration des données à travers différents systèmes xml a été adopté comme format standard pour échanger des informations xquery est un langage interrogation pour xml qui s'est imposé pour les systèmes basés sur xml ainsi xquery est employé sur des systèmes de médiation pour concevoir des vues définies sur plusieurs sources pour optimiser évaluation de requêtes les vues sont matérialisées lors de la mise à jour des sources car dans le contexte de sources web très peu informations sont fournies par les sources les méthodes habituellement proposées ne peuvent pas être appliquées cet article étudie comment mettre à jour des vues matérialisées xml sur des sources web au sein une architecture de médiation\n",
      "combinaison de fonctions de préférence par boosting pour la recherche de passages dans les systèmes de question/réponse\n",
      "nous proposons une méthode apprentissage automatique pour la sélection de passages susceptibles de contenir la réponse à une question dans les systèmes de question-réponse (qr) les systèmes de ri ad hoc ne sont pas adaptés à cette tâche car les passages recherchés ne doivent pas uniquement traiter du même sujet que la question mais en plus contenir sa réponse pour traiter ce problème les systèmes actuels ré-ordonnent les passages renvoyés par un moteur de recherche en considérant des critères sous forme une somme pondérée de fonctions de scores nous proposons apprendre automatiquement les poids de cette combinaison grâce à un algorithme de réordonnancement défini dans le cadre du boosting qui sont habituellement déterminés manuellement en plus du cadre apprentissage proposé originalité de notre approche réside dans la définition des fonctions allouant des scores de pertinence aux passages nous validons notre travail sur la base de questions et de réponses de évaluation trec-11 des systèmes de qr les résultats obtenus montrent une amélioration significative des performances en terme de rappel et de précision par rapport à un moteur de recherche standard et à une méthode apprentissage issue du cadre de la classification\n",
      "de la statistique des données à la statistique des connaissances  avancées récentes en analyse des données symboliques\n",
      "nan\n",
      "élagage et aide à interprétation symbolique et graphique une pyramide\n",
      "le but de ce travail est de faciliter interprétation une classification pyramidale construite sur un tableau de données symboliques alors que dans une hiérarchie binaire le nombre de paliers est égal à n-1 si n est le nombre individus à classer dans le cas une pyramide ce dernier peut atteindre n(n-1)/2 afin de réduire ce nombre on élague la pyramide et on utilise un critère de sélection de paliers basé sur la hauteur de plus on décrit tous les paliers retenus par des variables que on sélectionne également en utilisant \"le degré de généralité\" ainsi que des mesures de dissimilarités de type symbolique-numérique aide à interprétation se sert outils graphiques et interactifs grâce à la bibliothèque opengl enfin une simulation montre comment évoluent ces sélections quand le nombre de classes et de variables croit\n",
      "enrichissement sémantique de documents xml représentant des tableaux\n",
      "ce travail a pour objectif la construction automatique un entrepôt thématique de données à partir de documents de format divers provenant du web exploitation de cet entrepôt est assurée par un moteur interrogation fondé sur une ontologie notre attention porte plus précisément sur les tableaux extraits de ces documents et convertis au format xml aux tags exclusivement syntaxiques cet article présente la transformation de ces tableaux sous forme xml en un formalisme enrichi sémantiquement dont la plupart des tags et des valeurs sont des termes construits à partir de l'ontologie\n",
      "entrepôt de données spatiales basé sur gml  politique  de gestion de cache\n",
      "nan\n",
      "évaluation des algorithmes lem et elem pour données continues\n",
      "très populaire et très efficace pour estimation de paramètres un modèle de mélange algorithme em présente inconvénient majeur de converger parfois lentement son application sur des tableaux de grande taille devient ainsi irréalisable afin de remédier à ce problème plusieurs méthodes ont été proposées nous présentons ici le comportement une méthode connue lem et une variante que nous avons proposée récemment elem celles-ci permettent accélérer la convergence de algorithme, tout en obtenant des résultats similaires à celui-ci dans ce travail nous nous concentrons sur aspect classification et nous illustrons le bon comportement de notre variante sur des données continues simulées et réelles\n",
      "expériences de classification une collection de documents xml de structure homogène\n",
      "cet article présente différentes expériences de classification de documents xml de structure homogène en vue expliquer et de valider une présentation organisationnelle pré-existante le problème concerne le choix des éléments et mots utilisés pour la classification et son impact sur la typologie induite pour cela nous combinons une sélection structurelle basée sur la nature des éléments xml et une sélection linguistique basée sur un typage syntaxique des mots nous illustrons ces principes sur la collection des rapports activité 2003 des équipes de recherche de inria en cherchant des groupements équipes thèmes) à partir du contenu de différentes parties de ces rapports nous comparons nos premiers résultats avec les thèmes de recherche officiels de inria.\n",
      "expérimentations sur un modèle de recherche information utilisant les liens hypertextes des pages web\n",
      "la fonction de correspondance qui permet de sélectionner et de classer les documents par rapport à une requête est un composant essentiel dans tout système de recherche d'information nous proposons de modéliser une fonction de correspondance prenant en compte à la fois le contenu et les liens hypertextes des pages web nous avons expérimenté notre système sur la collection de test trec-9 et nous concluons que pour certains types de requêtes inclure le texte ancre associé aux liens hypertextes des pages dans la fonction de similarité s'avère plus efficace\n",
      "extension de algorithme apriori et des règles association aux cas des données symboliques diagrammes et intervalles\n",
      "nous traitons extension de algorithme apriori et des règles association aux cas des données symboliques diagrammes et intervalles la méthode proposée nous permet de découvrir des règles association au niveau des concepts cette extension implique notamment de nouvelles définitions pour le support et la confiance afin exploiter la structure symbolique des données au fil de article exemple classique du panier de la ménagère est développé ainsi plutôt que extraire des règles entre différents articles appartenant à des mêmes transactions enregistrées dans un magasin comme dans le cas classique nous extrayons des règles association au niveau des clients afin étudier leurs comportements d'achat\n",
      "extension des bases de données inductives pour la découverte de chroniques\n",
      "les bases de données inductives intègrent le processus de fouille de données dans une base de données qui contient à la fois les données et les connaissances induites nous nous proposons étendre les données traitées afin de permettre extraction de motifs temporels fréquents et non fréquents à partir un ensemble de séquences d'évènements les motifs temporels visés sont des chroniques qui permettent exprimer des contraintes numériques sur les délais entre les occurrences d'évènements\n",
      "extraction bayésienne et intégration de patterns représentés suivant les k plus proches voisins pour le go 19x19\n",
      "cet article décrit la génération automatique et utilisation une base de patterns pour le go 19x19 la représentation utilisée est celle des k plus proches voisins les patterns sont engendrés en parcourant des parties de professionnels les probabilités appariement et de jeu des patterns sont également estimées à ce moment là la base créée est intégrée dans un programme existant indigo soit elle est utilisée comme un livre ouvertures en début de partie soit comme une extension des bases pré-existantes du générateur de coups du programme en terme de niveau de jeu le gain résultant est estimé à 15 points en moyenne\n",
      "extraction bilingue de termes médicaux dans un corpus  parallèle anglais/français\n",
      "le catalogue et index des sites médicaux francophones cismef) recense les principales ressources institutionnelles de santé en français la description de ces ressources puis leur accès par les utilisateurs se fait grâce à la terminologie cismef fondée sur le thésaurus américain medical subject headings (mesh) la version française du mesh comprend tous les descripteurs mesh mais de nombreux synonymes américains restent à traduire afin enrichir la terminologie nous proposons ici une méthode de traduction automatique de ces synonymes pour ce faire nous avons constitué deux corpus parallèles anglais/français du domaine médical après alignement semi-automatique des corpus paragraphe à paragraphe nous avons procédé automatiquement à appariement bilingue des termes pour cela le lexique constitué des descripteurs mesh américains et de leur traduction en français a fourni les couples amorces qui ont servi de point de départ à la propagation syntaxique des liens d'appariement 217 synonymes ont pu être traduits avec une précision de 70%\n",
      "extraction de la localisation des termes pour le classement des documents\n",
      "trouver et classer les documents pertinents par rapport à une requête est fondamental dans le domaine de la recherche d'information notre étude repose sur la localisation des termes dans les documents nous posons hypothèse que plus les occurrences des termes une requête se retrouvent proches dans un document alors plus ce dernier doit être positionné en tête de la liste de réponses nous présentons deux variantes de notre modèle à zone d'influence la première est basée sur une notion de proximité floue et la seconde sur une notion de pertinence locale\n",
      "extraction de règles association quantitatives application à des données médicales\n",
      "extraction de règles association est devenue aujourd'hui une tâche populaire en fouille de données cependant algorithme apriori et ses variantes restent dédiés aux bases de données renfermant des informations catégoriques.nous proposons dans cet article quantminer qui est un outil que nous avons développé dans le but extraire des règles association gérant variables catégoriques et numériques outil que nous proposons repose sur un algorithme génétique permettant de découvrir de façon dynamique les intervalles des variables numériques apparaissant dans les règles.nous présentons également une application réelle de notre outil sur des données médicales relatives à la maladie de athérosclérose et donnons des résultats de notre expérience pour la description et la caractérisation de cette maladie\n",
      "extraction de termes centrée autour de expert\n",
      "nous développons un logiciel exit capable aider un expert à extraire des termes qu'il trouve pertinents dans des textes de spécialité tout est mis en place pour faciliter le travail de expert afin qu'il puisse consacrer son temps à la seule reconnaissance des termes pertinents pour cela différentes mesures statistiques et de nombreuses options extraction sont disponibles dans exit afin utiliser au mieux les connaissances de expert, notre approche est semi-automatique de plus expert construit des termes pouvant inclure des termes précédemment extraits ce qui rend itératif et constructif notre processus de formation des termes enfin ergonomie du logiciel a profité des enseignements tirés lors de son utilisation pour une compétition internationale extraction de connaissances\n",
      "extraction des connaissances pour enrichissement des bases  de données géographiques\n",
      "nan\n",
      "fonctions oubli et conservation de détail dans les entrepôts de données\n",
      "nan\n",
      "forage distribué des données  une comparaison entre agrégation échantillons et agrégation de règles\n",
      "pour nous attaquer au problème du forage de très grandes bases de données distribuées nous proposons étudier deux approches la première est de télécharger seulement un échantillon de chaque base de données puis y effectuer le forage la deuxième approche est de miner à distance chaque base de données indépendamment puis de télécharger les modèles résultants sous forme de règles de classification dans un site central où agrégation de ces derniers est réalisée dans cet article nous présentons une vue ensemble des techniques échantillonnage les plus communes nous présentons ensuite cette nouvelle technique de forage distribué des données où la mécanique agrégation est basée sur un coefficient de confiance attribué à chaque règle et sur de très petits échantillons de chaque base de données le coefficient de confiance une règle est calculé par des moyens statistiques en utilisant le théorème limite centrale en conclusion nous présentons une comparaison entre les meilleures techniques échantillonnage que nous avons trouvées dans la littérature et notre approche de forage distribué des données fdd) basée sur agrégation de modèles\n",
      "fouille de données relationnelles dans les sgbd\n",
      "nan\n",
      "fouille de graphes et découverte de règles association  application à analyse images de document\n",
      "cet article présente une méthode permettant la découverte non supervisée de motifs fréquents représentatifs de symboles sur des images de documents les symboles sont considérés comme des entités graphiques porteurs information et les images de document sont représentées par des graphes relationnels attribués dans un premier temps la méthode réalise la découverte de sous-graphes disjoints fréquents et fait correspondre pour chacun eux un symbole différent une recherche des règles association entre ces symboles permet alors accéder à une partie des connaissances du domaine décrit par ces symboles objectif à terme est utiliser les symboles découverts pour la classification ou la recherche images dans un flux hétérogène de document là ou une approche supervisée n'est pas envisageable\n",
      "fouille de textes pour orienter la construction une ressource terminologique\n",
      "la finalité de ce papier est analyser apport de techniques de fouille de données textuelles à une méthodologie de construction ontologie à partir de textes le domaine application de cette expérimentation est celui de accidentologie routière dans ce contexte les résultats des techniques de fouille de données textuelles sont utilisés pour orienter la construction une ressource terminologique à partir de procès-verbaux d'accidents la méthode terminae et outil du même nom offrent le cadre général pour la modélisation de la ressource le papier présente les techniques de fouille employées et intégration des résultats des fouilles dans les différentes étapes du processus de construction de la ressource\n",
      "hiérarchisation des règles association en fouille de textes\n",
      "extraction de règles association est souvent exploitée comme méthode de fouille de données cependant une des limites de cette approche vient du très grand nombre de règles extraites et de la difficulté pour analyste à appréhender la totalité de ces règles nous proposons donc de pallier ce problème en structurant ensemble des règles association en hiérarchies la structuration des règles se fait à deux niveaux un niveau global qui a pour objectif de construire une hiérarchie structurant les règles extraites des données nous définissons donc un premier type de subsomption entre règles issue de la subsomption dans les treillis de galois le second niveau correspond à une analyse locale des règles et génère pour une règle donnée une hiérarchie de généralisation de cette règle qui repose sur des connaissances complémentaires exprimées dans un modèle terminologique ce niveau fait appel à un second type de subsomption inspiré de la subsomption en programmation logique inductive nous définissons ces deux types de subsomptions développons un exemple montrant intérêt de approche pour analyste et étudions les propriétés formelles des hiérarchies ainsi proposées\n",
      "intégration efficace des arbres de décision dans les sgbd  utilisation des index bitmap\n",
      "nous présentons dans cet article une nouvelle approche de fouille qui permet appliquer des algorithmes de construction arbres de décision en répondant à deux objectifs  1) traiter des bases volumineuses 2) en des temps de traitement acceptables le premier objectif est atteint en intégrant ces algorithmes au cœur des sgbd en utilisant uniquement les outils fournis par ces derniers toutefois les temps de traitement demeurent longs en raison des nombreuses lectures de la base nous montrons que grâce aux index bitmap nous réduisons à la fois la taille de la base apprentissage et les temps de traitements pour valider notre approche nous avons implémenté la méthode id3 sous forme une procédure stockée dans le sgbd oracle\n",
      "automate textuel pour la prise en compte de évolution du texte\n",
      "il n'est plus à rappeler que le corpus textuel est tel qu'il est actuellement intraitable à échelle que sa croissance nous confirme obligation utiliser des outils automatique de traitement cet article s'intéresse plus particulièrement à la caractérisation de textes et par là même à celle d'auteurs a heure actuelle toutes les méthodes existant travaillent sur le document fini sans admettre qu'un cheminement existe entre le début du document et sa fin nous proposons une méthode tentant apporter cette notion évolution textuelle en traitant le texte par un automate et évaluation choisie puis nous présenterons des résultats validés par des experts obtenus sur un corpus entretiens sociologiques\n",
      "la démarche ontologique pour la gestion des compétences et des connaissances\n",
      "la gestion des ressources humaines repose une part sur la connaissance des individus et de leurs compétences et autre part sur la connaissance de organisation et de ses métiers c'est par la \"mise en correspondance\" de ces connaissances qu'il est possible améliorer l'emploi de valoriser les connaissances et les compétences individuelles et de mieux gérer organisation. cette mise en correspondance nécessite une représentation explicite des connaissances ce qui permet de répondre à de nouveaux besoins  annuaire de compétences gestion des projets et des retours d'expériences identification des connaissances à risques etc.nous verrons dans le cadre de cet article intérêt de approche ontologique tant un point de vue méthodologique pour la clarification des notions mises en jeu dans le cadre de la gpecc gestion prévisionnelle des emplois des compétences et des connaissances que pour la construction la représentation et la maintenance des référentiels des compétences des connaissances et des métiers elle permet en particulier une gestion de information par la terminologie et le sens métier propre à organisation.\n",
      "la réussite universitaire  prédictions par génération de règles\n",
      "nan\n",
      "les ntic au services de la capitalisation des connaissances\n",
      "nan\n",
      "logiciel aide à étiquetage morpho-syntaxique de  textes de spécialité\n",
      "la compréhension de textes de spécialité nécessite un étiquetage morpho-syntaxique de bonne qualité or lorsque les textes étudiés sont issus de domaines spécifiques et peu usités il est rare de disposer de dictionnaires et autres ressources lexicales fiables le logiciel que nous proposons permet utiliser un étiquetage réalisé par un étiqueteur généraliste puis améliorer cet étiquetage en intégrant des connaissances experts du domaine étudié grâce au logiciel développé il est relativement aisé pour un expert du domaine de détecter des erreurs étiquetage et de mettre en place des règles de ré-étiquetage ces règles peuvent être obtenues de deux manières différentes  1) soit en utilisant un langage de programmation permettant exprimer des règles complexes de ré-étiquetage 2) soit par apprentissage automatique des règles à partir exemples corrigés au moyen une interface dédiée cet apprentissage propose de nouvelles règles à l'expert acquises automatiquement\n",
      "manipulation et fusion de données multidimensionnelles\n",
      "cet article définit une algèbre permettant de manipuler des tables dimensionnelles extraites une base de données multidimensionnelles algèbre intègre un noyau minimum opérateurs unaires permettant effectuer les analyses décisionnelles par combinaison opérateurs. cette algèbre intègre un opérateur binaire permettant la fusion de tables dimensionnelles facilitant les corrélations des sujets analysés\n",
      "méthode de construction ontologie de termes à partir du treillis de iceberg de galois\n",
      "approche présentée dans cet article a pour objectif la construction une ontologie à partir du treillis de iceberg de galois nous entendons par ontologie un ensemble de termes structurés entre eux par un ensemble de liens de divers types dans notre cas d'étude cette ontologie constitue un support de connaissances \"documentaires\" en effet elle peut être utilisée dans diverses applications en recherche information (ri) telles que indexation automatique et expansion de requêtes ainsi qu'en text-mining la méthode de construction que nous proposons est fondée sur analyse formelle de concepts afc) et plus précisément la structure du treillis de iceberg de galois en utilisant cette structure hiérarchique partiellement ordonnée nous présentons une translation directe des relations laticielles vers celles ontologiques nous proposons ainsi enrichir ontologie dérivée par des règles associatives génériques entre termes découvertes dans le cadre un processus de text-mining\n",
      "microarray data mining  recent advances\n",
      "nan\n",
      "mining frequent queries in star schemes\n",
      "extraction de toutes les requêtes fréquentes dans une base de données relationnelle est un problème di±cile même si on ne considère que des requêtes conjonctives nous montrons que ce problème devient possible dans le cas suivant  le schéma de la base est un schéma en étoile et les données satisfont un ensemble de dépendances fonctionnelles et de contraintes référentielles de plus les schémas en étoile sont appropriés pour les entrepôts de données et que les dépendances fonctionnelles et les contraintes référentielles sont les contraintes les plus usuelles dans les bases de données en considérant le modèle des instances faibles nous montrons que les requêtes fréquentes exprimées par sélection-projection peuvent être extraites par des algorithmes de type apriori\n",
      "modélisation objets mobiles dans un entrepôt de données\n",
      "la gestion objets mobiles a connu un regain intérêt ces dernières années particulièrement dans le but de gérer et de prédire la localisation objets mobiles cependant il y a peu de recherches sur exploitation historiques de bases objets mobiles la première étape dans ce processus est la mise en œuvre un entrepôt objets mobiles seulement les modèles entrepôts existants ne permettent pas de traiter directement ce type de données complexes cet article présente une approche originale pour pallier ce problème cette approche offre la puissance de algèbre olap sur toute combinaison de données classiques spatiales et/ou temporelles et mobiles elle a été validée par un prototype et appliquée à analyse de la mobilité urbaine1 les résultats de expérimentation montrent la validité de approche et les tests de performances son efficacité\n",
      "modélisation un agent émotionnel en uml et rdf\n",
      "pouvoir extraire de la connaissance à partir une plate-forme de simulation est aujourd'hui envisageable en conjuguant les avancées obtenues en intelligence artificielle autour des systèmes multi-agents et les méthodes de formalisation et extraction des connaissances c'est donc dans un cadre général de gestion des connaissances que nous proposons de modéliser un agent artificiel doté de connaissances et d'émotions pour cela une expertise psychologique a été recueillie et formalisée de manière à être stockée dans une base de connaissances sous forme de règles et de classes en uml et rdf implémentation du modèle permet entrevoir les perspectives une telle simulation  enrichissement par des données issues de simulations découverte de nouvelles connaissances par application de processus d'ecd\n",
      "modélisation de connaissances pour un système de médiation\n",
      "travaillant sur élaboration une méthodologie de développement de systèmes de médiation intégrés dans des systèmes coopératifs nous avons proposé une architecture à 3 composants  le premier concerne la coopération le second assistance et le troisième est relatif aux connaissances nécessaires aux 2 précédents dans cet article nous présentons plus particulièrement le point de vue des connaissances ces connaissances sont de 2 natures  des connaissances statiques sur le domaine par exemple et des connaissances acquises pendant utilisation coopérative du système notamment la mémoire des activités et les descriptions des actes de résolutions de problèmes pour illustrer cette modélisation de connaissances nous nous intéresserons aux activités coopératives de suivi de gestion et évaluation de projets d'étudiants assistées par outil ipédagogique\n",
      "modélisation de la cognition sociale – propositions autour de  utilisation de schémas cognitifs\n",
      "nan\n",
      "modélisation des individus et de leurs relations pour aide à intégration des individus dans organisation\n",
      "objectif de ce papier est de présenter une contribution à la modélisation des individus et de leurs relations pour permettre aide à intégration des acteurs dans une organisation nous étudions en particulier le cas du remplacement un acteur « turn-over ») dans ce cadre nous proposons un modèle regroupant un ensemble de données relatives à un individu aux relations que celui-ci entretient avec les autres acteurs et à son espace informationnel étude porte sur la mise en oeuvre de mécanismes aide fournissant à un acteur les moyens de son intégration  la mise à disposition une image des espaces informationnels et relationnels de son prédécesseur ainsi que la mise en relation de acteur avec les autres acteurs de l'organisation cette étude est menée en partenariat avec des experts en grh\n",
      "modélisation des interactions entre individus avec agentuml\n",
      "pour faciliter étude de certains phénomènes des outils de simulation ont été créés dans de nombreux domaines étude du comportement humain à jusque là échappé à cette tendance aujourd'hui les systèmes multi-agents couplés aux avancées des sciences humaines fournissent les bases nécessaires à élaboration de ce type d'outil cet article s'inscrit ainsi dans cette dynamique avec objectif de développer un outil de simulation du comportement individus traumatisés crâniens sur une chaîne de production cet outil doit permettre la collecte de la connaissance relative au système étudié et fournir une aide à la décision pour les responsables de l'entreprise cet article propose une modélisation des interactions entre individus dans le formalisme agentuml une implémentation du modèle au sein un outil de simulation fonctionnel et les résultats obtenus seront également présentés a terme le but est la production de données de simulation exploitables par des techniques d'ecd\n",
      "motifs séquentiels flous  un peu beaucoup passionément\n",
      "la plupart des bases de données issues du monde réel sont constituées de données numériques et historiées données de capteurs données scientifiques données démographiques) dans ce cadre les algorithmes extraction de motifs séquentiels s'ils sont adaptés au caractère temporel des données ne permettent pas le traitement de données numériques es données sont alors pré-traitées pour les transformer en données binaire ce qui entraîne une perte d'information des algorithmes ont donc été proposés pour traiter les données numériques sous forme intervalles et intervalles flous notamment en ce qui concerne la recherche de motifs séquentiels fondée sur des intervalles flous les deux méthodes de la littérature ne sont pas satisfaisantes car incomplètes soit dans le traitement des séquences soit dans le calcul du support dans cet article nous proposons donc trois méthodes extraction de motifs séquentiels flous {speedyfuzzy minifuzzy et totallyfuzzy et en détaillons les algorithmes sous-jacents en soulignant les différents niveaux de fuzzification ces algorithmes sont implémentés et évalués à travers différentes expérimentations menées sur des jeux de tests synthétiques\n",
      "notion de sémantiques bien-formées pour les règles\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "la notion de règles entre attributs est très générale allant des règles association en fouille de données aux dépendances fonctionnelles df) en bases de données malgré cette diversité la syntaxe des règles est toujours la même seule leur sémantique diffère pour une sémantique donnée en fonction des propriétés induites des techniques algorithmiques sont mises en oeuvre pour découvrir les règles à partir des données a partir un ensemble de règles il est aussi utile en pratique de raisonner sur ces règles comme cela est le cas par exemple avec les axiomes armstrong pour les dépendances fonctionnelles dans cet article nous proposons un cadre qui permet de s'assurer qu'une sémantique donnée pour les règles est bien-formée ie les axiomes armstrong sont justes et complets pour cette sémantique les propositions faites dans ce papier proviennent du contexte applicatif de analyse de données de biopuces a partir de plusieurs sémantiques pour les données expression de gènes nous montrons comment ces sémantiques s'intègrent dans le cadre présenté\n",
      "outil de classification et de visualisation de grands volumes de données mixtes\n",
      "nous avons conçu un outil de classification de données original que nous détaillons dans le présent article cet outil comporte un module de création de résumés et un module d'affichage le module de visualisation permet une lecture aisée des résumés grâce à une interface graphique évoluée permettant la présentation et exploration des résumés sous forme une hiérarchie de profils ou un tableau de profils chaque profil donne de manière claire les informations relatives au résumé de données correspondant la lecture de la hiérarchie et du tableau est aussi grandement facilitée par les choix un ordre optimal pour la présentation des variables et des résumés\n",
      "prise en compte des « points de vue » pour annotation un processus extraction de connaissances à partir de données\n",
      "dans cet article on propose une nouvelle approche qui rend explicite la notion de point de vue dans une analyse multivues issue un processus extraction de connaissances à partir de données (ecd) par point de vue nous entendons la vision particulière un analyste lors de son processus ecd vision référant à un corps de connaissances qui lui est spécifique on cherche une part à faciliter la réutilisabilité et adaptabilité du processus et autre part à garder une trace des points de vues sous-jacents aux analyses faites le processus ecd sera vu comme un processus de génération et de transformation de vues qui seront annotées par des métadonnées pour garder la sémantique de la connaissance extraite un positionnement de notre approche vis-à-vis des travaux méthodologiques du processus ecd sera donné des éléments de modélisation du processus ecd basé sur les points de vue seront décrits au niveau ontologique enfin on illustrera notre approche sur analyse des usages un site web à partir des fichiers log selon le point de vue fiabilité\n",
      "problématiques de gestion de connaissances dans le cadre de enseignement à distance sur l'internet\n",
      "le développement des réseaux à haut-débit et de internet fournit un nouveau support à enseignement à distance aujourd'hui de nombreux acteurs dans le domaine de enseignement ont mis en place des dispositifs de formation en ligne ceux-ci se composent généralement une sélection de matériaux organisés et présentés de manière à suivre un programme pédagogique particulier de mécanismes de communication entre apprenants et enseignants et outils de suivi des apprenants les plates-formes enseignement à distance devenant de plus en plus génériques des nouveaux modèles ont été définis standardisés ou normalis és permettant la formalisation de méta-données pédagogiques ou tentant évaluer les connaissances acquises par les apprenants en nous appuyant sur ces modèles nous proposons de construire une base de connaissances associant notamment les termes des domaines enseignés en relations à sémantique pédagogique exploitation de cette base de connaissances fournit un premier niveau aide à ingénierie pédagogique en particulier lorsque le volume de contenus en ligne est important des inférences mettant en jeu ces connaissances permettent alors un meilleur suivi du dispositif enseignement.\n",
      "processus de traitement de données radar pour la reconnaissance/identification de cibles aériennes\n",
      "nan\n",
      "raisonnement en gestion des compétences\n",
      "nous nous intéressons au raisonnement sur les compétences des ressources humaines pour simplifier leur gestion dans cet article nous proposons une méthode de raisonnement pour aide à identification des compétences un individu un processus de knowledge-mining défini par analogie avec extraction de règles association en data-mining est proposé afin induire une base de règles à partir une base de connaissances sur le domaine de plus un prototype a été développé pour expérimenter notre approche sur un exemple académique\n",
      "rasma  une approche multi-agent pour amélioration de algorithme des règles associations spatiales\n",
      "nan\n",
      "réécriture de requêtes multimédias  approche basée sur usage une ontologie\n",
      "nous proposons dans cet article une stratégie de réécriture de requêtes sur des données multimédias décrites moyennant le standard mpeg-7 ce standard se base sur xml schéma qui permet de décrire la structure des données cependant aucune sémantique n'est assignée à cette structure nous proposons étendre ce standard une ontologie permettant exprimer les connaissances du domaine ainsi ontologie sera utilisée durant indexation des données multimédias et la réécriture de requêtes le but de la réécriture de requêtes est de transformer une requête initiale en une ou plusieurs requêtes équivalentes ou sémantiquement proches compte tenu des connaissances représentées dans ontologie.\n",
      "règles de propagation pour la création ontologies annotation de ressources\n",
      "annotation se distingue de indexation automatique par utilisation une ou plusieurs ontologies qui définissent un domaine global de référence permettant de cadrer et de normaliser les annotations effectuées par ailleurs une ressource annotée doit être non pas par une liste de mots clefs mais bien par une ou plusieurs ontologies malheureusement il est peu réaliste de penser que les centaines de millions de ressources mises à disposition sur le web puissent être annotées par leurs auteurs pour résoudre ce problème notre démarche consiste à indexer les documents en se basant sur ontologie globale et ensuite propager les annotations en utilisant des documents déjà annotés pour annoter autres documents référencés par ceux-ci la propagation des annotations suit des règles que nous proposons dans cet article illustration est effectuée sur un corpus de livres dont le thème relève de l'informatique\n",
      "réponses coopératives dans interrogation de documents rdf\n",
      "le développement du web sémantique a conduit à élaboration de standards pour la représentation des connaissances sur le web rdf comme un de ces standards est devenu une recommandation du w3c même s'il a été conçu pour être interprétable par homme et la machine encodage xml triplets graphes étiquetés) rdf n'a pas été fourni avec des services interrogation et de raisonnement la plupart des travaux concernant interrogation de documents rdf se sont concentrés sur usage de techniques issues de la programmation logique et sur des extensions de sql nous portons un nouveau regard sur les techniques interrogation et de raisonnement sur les documents rdf et nous montrons que la sémantique des termes osf order sorted features est compatible avec la représentation isomorphique triplets) des propositions rdf cette transformation permet ordonnancement des ressources en ontologies et à travers ceci des meilleurs mécanismes de réponses par approximation et recouvrement aux interrogations de documents rdf\n",
      "restructuration automatique de documents dans les corpus semi-structurés hétérogènes\n",
      "interrogation de grandes bases de documents semi-structurés type xml est un problème ouvert important en effet pour interroger un document dont le schéma est nouveau un système doit pouvoir soit adapter la requête posée au document soit adapter le document pour pouvoir lui appliquer la requête nous nous positionnons ici dans le cadre de la restructuration de documents qui consiste à transformer des documents semi-structurés issus de diverses sources dans un schéma de médiation connu nous proposons un cadre statistique général à la problématique de la restructuration de documents et détaillons une instance un modèle stochastique de documents structurés appliquée à cette problématique nous détaillons enfin un ensemble expériences effectuées sur les documents du corpus inex afin de mesurer la capacité de notre modèle\n",
      "sélection de modèles par des méthodes à noyaux pour la classification de données séquentielles\n",
      "ce travail concerne le développement de méthodes de classification discriminantes pour des données séquentielles quelques techniques ont été proposées pour étendre aux séquences les méthodes discriminantes comme les machines à vecteurs supports par nature plus adaptées aux données en dimension fixe elles permettent de classifier des séquences complètes mais pas de réaliser la segmentation qui consiste à reconnaître la séquence d'unités phonèmes ou lettres par exemple correspondant à un signal en utilisant une correspondance donnée / modèle nous transformons le problème de apprentissage des modèles à partir de données par un problème de sélection de modèles qui peut être attaqué via des méthodes du type machines à vecteurs supports nous proposons et évaluons divers noyaux pour cela et fournissons des résultats expérimentaux pour deux problèmes de classification\n",
      "semi-supervised incremental clustering of categorical data\n",
      "le clustering semi-supervisé combine apprentissage supervisé et non-supervisé pour produire meilleurs clusterings dans la phase initiale supervisée de algorithme un échantillon apprentissage est produit par sélection aléatoire on suppose que les exemples de échantillon apprentissage sont étiquetés par un attribut de classe puis un algorithme incrémentiel développé pour les données catégoriques est utilisé pour produire un ensemble de clusters pur tels que les exemple de chaque cluster ont la même étiquette) qui servent de \"seeding clusters\" pour la deuxième phase non-supervisée de algorithme dans cette phase algorithme incrémentiel est appliqué aux données non étiquetées la qualité du clustering est évaluée par index de gini moyen des clusters les expériences démontrent que des très bons clusterings peuvent être obtenus avec des petits échantillons apprentissage.\n",
      "seqtree un outil de fouille de données séquentielles par visualisation\n",
      "dans cet article nous présentons un outil de visualisation de séquences modélisées par des arbres de suffixes probabilistes prediction suffix trees - pst) ce type arbre permet de représenter une chaîne de markov ordre variable dans différentes application il s'est avéré plus efficace qu'une chaîne de markov ordre fixe avec un coût calculatoire moindre pour ces raisons il nous a paru intéressant exploiter le caractère arborescent de ce mode de représentation non seulement un point de vue algorithmique mais aussi un point de vue visuel\n",
      "ssc  statistical subspace clustering\n",
      "cet article se place dans le cadre du subspace clustering dont la problématique est double  identifier simultanément les clusters et le sous-espace spécifique dans lequel chacun est défini et caractériser chaque cluster par un nombre minimal de dimensions permettant ainsi une présentation des résultats compréhensible par un expert du domaine d'application les méthodes proposées jusqu'à présent pour cette tâche ont le défaut de se restreindre à un cadre numérique objectif de cet article est de proposer un algorithme de subspace clustering capable de traiter des données décrites à la fois par des attributs continus et des attributs catégoriels nous présentons une méthode basée sur algorithme classique em mais opérant sur un modelé simplifié des données et suivi une technique originale de sélection attributs pour ne garder que les dimensions pertinentes de chaque cluster les expérimentations présentées ensuite menées sur des bases de données aussi bien artificielles que réelles montrent que notre algorithme présente des résultats robustes en termes de qualité de la classification et de compréhensibilité des clusters obtenus\n",
      "svm et visualisation pour la fouille de grands ensembles de données\n",
      "nous présentons un algorithme de svm et des méthodes graphiques pour le traitement de grands ensembles de données pour pouvoir traiter de tels ensembles de données nous utilisons une représentation des données de plus haut niveau sous forme symbolique) algorithme de séparateur à vaste marge svm) est adapté pour pouvoir traiter ce nouveau type de données nous construisons un nouveau noyau rbf radial basis function que algorithme utilise à la fois pour la classification la régression et la détection individus atypiques dans des données de type intervalle nous utilisons ensuite des méthodes de visualisation interactive elles aussi adaptées au cas des variables de type intervalle pour expliquer les résultats obtenus par les svm la méthode est évaluée sur des ensembles de données symboliques existant ou créés artificiellement\n",
      "tableau de bits indexé tbi)  pour la recherche de séquences fréquentes\n",
      "nan\n",
      "tanagra  un logiciel gratuit pour enseignement et la recherche\n",
      "tanagra est un logiciel « open source » librement accessible sur le web il tente de concilier deux types d'utilisation une part en proposant une interface suffisamment conviviale il est accessible aux utilisateurs non spécialistes qui veulent effectuer des études sur des données réelles autre part en définissant une architecture simplifiée à l'extrême les efforts de développement portent sur l'essentiel à savoir la mise au point et intégration algorithmes de fouille de données les chercheurs peuvent ainsi mener des expérimentations sur les méthodes dans cet article nous présentons les principales fonctionnalités du logiciel en essayant de le positionner sur échiquier des très) nombreux logiciels diffusés actuellement\n",
      "tendances dans les expressions de gènes   application à analyse du transcriptome  de plasmodium falciparum\n",
      "étude de expression des gènes est depuis quelques années révolutionnée par les puces à adn les méthodes habituellement mises en oeuvre pour analyser ces données s'appuient sur des algorithmes de partitionnement comme les clustering hiérarchiques et sur une hypothèse communément admise qui associe à un ensemble de profils expression similaires une fonction identique cette analyse étudie ensemble des gènes sans distinction approche que nous proposons deux catégories de gènes  connus ou putatifs pour chaque gène n'ayant pas information rattachée nous étudions son voisinage afin y trouver des motifs fréquents (itemsets) ensuite analyse est guidée par interprétation biologique afin de faire émerger des propriétés intéressantes un premier jeu de test sur plasmodium falciparum agent de la malaria nous a permis de mettre en évidence en nous intéressant aux items relatifs à la glycolyse un transporteur de nucléosides qui intervient au niveau énergétique dans la phase ring précoce) du parasite\n",
      "un automate pour la génération complète ou partielle des concepts du treillis de galois\n",
      "cet article se situe dans le domaine de analyse formelle de concepts et du treillis de concepts treillis de galois lequel est un cadre théorique intéressant pour le regroupement conceptuel des données et la génération des règles d'association puisque la prospection de données data mining est utilisée comme support à la prise de décision par des analystes rarement intéressés par la liste exhaustive souvent très longue des concepts et des règles élaboration une solution approximative sera dans la plupart des cas un compromis satisfaisant et relativement moins coûteux qu'une solution exhaustive dans cet article on propose une approche appelée ciga closed itemset generation using an automata de génération partielle ou complète de concepts par la construction et le parcours un automate à états finis la génération des concepts permet identification des \"itemsets\" fermés fréquents étape cruciale pour extraction des règles d'association\n",
      "un critère évaluation pour la sélection de variables\n",
      "cet article aborde le problème de la sélection de variables dans le cadre de la classification supervisée les méthodes de sélection reposent sur un algorithme de recherche et un critère évaluation pour mesurer la pertinence des sous-ensembles potentiels de variables nous présentons un nouveau critère évaluation fondé sur une mesure d'ambigüité cette mesure est fondée sur une combinaison étiquettes représentant le degré de spécificité ou appartenance aux classes en présence les tests menés sur de nombreux jeux de données réels et artificiels montrent que notre méthode est capable de sélectionner les variables pertinentes et augmenter dans la plupart des cas les taux de bon classement\n",
      "un système aide à la navigation dans des hypermédias\n",
      "avec le développement internet et applications hypermédias la construction et exploitation de profils ou modèles des utilisateurs deviennent capitaux dans de nombreux domaines pouvoir cibler un utilisateur un hypermédia ou un site web afin de lui proposer ce qu'il attend devient essentiel par exemple lorsque on veut lui présenter les produits qu'il est le plus susceptible d'acheter ou bien plus généralement à chaque fois que on veut éviter de noyer utilisateur dans un flot d'informations nous présentons un système aide à la navigation intégrant un système de modélisation du comportement de navigation et un stratège qui met en œuvre en fonction du comportement détecté une aide visant à recommander des liens particuliers\n",
      "une approche filtre pour la sélection de variables en apprentissage non supervisé\n",
      "la sélection de variable sv) constitue une technique efficace pour réduire la dimension des espaces apprentissage et s'avère être une méthode essentielle pour le pré-traitement de données afin de supprimer les variables bruitées et/ou inutiles peu de méthodes de sv ont été proposées dans le cadre de apprentissage non supervisé et la plupart entre elles sont des méthodes dites \"enveloppes\" nécessitant utilisation un algorithme apprentissage pour évaluer les sous ensembles de variables or approche \"enveloppe\" est largement mal adaptée à une utilisation lors de cas \"réels\" en effet une part ces méthodes ne sont pas indépendantes vis à vis des algorithmes apprentissage non supervisé qui nécessitent le plus souvent de fixer un certain nombre de paramètres  mais surtout il n'existe pas de critères bien adaptés à évaluation de la qualité apprentissage non supervisé dans des sous espaces différents nous proposons et évaluons dans ce papier une méthode \"filtre\" et donc indépendante des algorithmes apprentissage non supervisé cette méthode s'appuie sur deux indices permettant évaluer adéquation entre deux ensembles de variables entre deux sous espaces)\n",
      "une méthode évaluation de la pertinence des pages web dans websum\n",
      "dans cet article nous présentons une méthode évaluation de la pertinence des pages web retournées par un moteur de recherche\n",
      "usage non classificatoire arbres de classification  enseignements une analyse de la participation féminine à emploi en suisse\n",
      "cet article présente une application en grandeur réelle des arbres de classification dans un contexte non classificatoire les arbres générés visent à mettre en lumière les différences régionales dans la façon dont les femmes décident de leur participation au marché du travail accent est donc mis sur la capacité descriptive plutôt que prédictive des arbres application porte sur des données relatives à la participation féminine au marché du travail issues du recensement suisse de la population de an 2000 ce vaste ensemble de données a été analysé en deux phases un premier arbre exploratoire a mis en évidence la nécessité de procéder à des études séparées pour les non mères les mères mariées ou veuves et les mères célibataires ou divorcées nous nous limitons ici aux résultats de ce dernier groupe pour lequel nous avons généré un arbre séparé pour chacune des trois régions linguistiques principales les arbres obtenus font apparaître des différences culturelles fondamentales entre régions du point de vue méthodologique la principale difficulté de cet usage non classificatoire des arbres concerne leur validation puisque le taux erreur de classification généralement retenu perd tout son sens dans ce contexte nous commentons cet aspect et illustrons usage alternatives plus pertinentes et facilement calculables\n",
      "utilisation des technologies xml pour la formalisation de ontologie de modèles e-business\n",
      "notre travail de recherche consiste à représenter ontologie des modèles e-business e-bmo par le langage bm²l spécifié sur la base un méta-modèle xml bm²l est comparé à autres langages de définition ontologie à savoir rdf(s) daml + oil et owl et ce selon un framework établis sur les spécificités de cette ontologie aussi introduisons nous une application web e-bmh pour la conception et exploitation des modèles e-business conformément à ontologie\n",
      "validation statistique des cartes de kohonen en apprentissage supervisé\n",
      "en apprentissage supervisé la prédiction de la classe est le but ultime plus largement on attend une bonne méthodologie apprentissage qu'elle permette une représentation des données susceptible de faciliter la navigation de utilisateur dans la base exemples et aider au choix des exemples et des variables pertinents tout en assurant une pré-diction de qualité dont on comprenne les ressorts différents travaux ont montré aptitude des graphes de voisinage issus des prédicteurs à fonder une telle méthodologie ainsi le graphe des voisins relatifs de toussaint cependant la complexité de leur construction en o(n3) reste élevée dans le cas de données volumineuses nous proposons de substituer aux graphes de voisinage les cartes de kohonen construites sur les prédicteurs après un bref rappel du principe des cartes de kohonen en apprentissage non supervisé nous montrons comment celles-ci peuvent fonder une stratégie apprentissage optimisée nous proposons ensuite évaluer la qualité de cette stratégie par une statistique originale qui est étroitement corrélée au taux erreur en généralisation différentes expérimentations montrent la faisabilité de cette approche on dispose alors un critère fiable pour sélectionner les individus et les attributs pertinents\n",
      "visualisation de la perception un site web par ses utilisateurs\n",
      "nous proposons dans cet article une méthode de visualisation de activité des utilisateurs un site web qui permet évaluer qualitativement adéquation entre son architecture logique et la perception de celle-ci par les internautes nous travaillons sur les parcours des internautes sur le site étudié après reconstruction de ceux-ci grâce aux fichiers logs des serveurs concernés nous utilisons la structure logique des sites étudiés pour simplifier la représentation des parcours en ne tenant pas compte de ordre de visite des catégories sémantiques du site les parcours simplifiés sont utilisés pour calculer une dissimilarité entre les catégories sémantiques qui sont ensuite représentées dans un plan par multi dimensional scaling nous complétons cette visualisation ensemble par une représentation de arbre couvrant minimal des catégories sémantiques qui permet de mieux appréhender certaines interactions nous illustrons intérêt de la méthode en appliquant au site de l'inria\n",
      "« la connaissance de la connaissance »  une réflexion sur la triangulation des analyses textuelles à partir un corpus spécialisé en gouvernance entreprise\n",
      "suite à la survenue récente de scandales financiers la synthèse des idées mobilisables en gouvernance entreprise semble désormais essentielle si on veut sécuriser les investisseurs dans cette perspective le présent projet de recherche consiste à mettre en œuvre un panel outils analyse de données textuelles (alceste syntex tropes-zoom/decision explorer wordmapper weblex afin évaluer les moyens dont peut disposer un analyste désireux extraire des connaissances contenues dans un ensemble articles académiques la qualité de représentation du corpus dans sa globalité est tout abord testée étude est ensuite centrée sur le concept même de connaissance mobilisé dans la théorie de la gouvernance des entreprises la convergence et la complémentarité des approches méthodologiques sont alors explicitées il en est de même pour ce qui concerne la capacité extraction une connaissance pertinente à partir des textes étudiés\n",
      "a galois connecion semantics-based approach for deriving generic bases of association rules\n",
      "augmentation vertigineuse de la taille des données textuelles ou transactionnelles est un défi constant pour la \"scalabilité\" des techniques extraction des connaissances dans ce papier on présente une approche pour la dérivation des bases génériques de règles associatives les principales caract éristiques de cette approches sont les suivantes une part introduction une structure de données appelée \"trie-itemset\" pour le stockage de la relation en entrée autre part on utilise une méthode \"diviser pour régner\" pour réduire le coût de construction de structures partiellement ordonnées à partir desquelles les bases génériques de règles sont directement extraites\n",
      "a metric approach to supervised discretization\n",
      "nous présentons une nouvelle approche à la discrétisation supervisée des attributs continues qui se sert de espace métrique des partitions un ensemble fini nous discutons deux nouvelles idées fondamentales  une généralisation des techniques de discrétisation de fayyad-irani basée sur une distance sur des partitions dérivée de entropie généralisée de daroczy et un nouveau critère géométrique pour arrêter algorithme de discrétisation les arbres de décision résultants sont plus petits ont moins de feuilles et montrent des niveaux plus élevés exactitude établis par la validation croisée stratifiée\n",
      "a robust method for partitioning the values of categorical attributes\n",
      "dans le domaine de apprentissage supervisé les méthodes de groupage des modalités un attribut symbolique permettent de construire un nouvel attribut synthétique conservant au maximum la valeur informationnelle de attribut initial et diminuant le nombre de modalités nous proposons ici une généralisation de algorithme de discrétisation khiops pour le problème du groupage des modalités algorithme proposé permet de contrôler a priori le risque de sur-apprentissage et améliorer significativement la robustesse des groupages produits cette caractéristique de robustesse a été obtenue en étudiant la statistique des variations du critère du khi2 lors de regroupements de lignes un tableau de contingence et en modélisant le comportement statistique de algorithme khiops des expérimentations intensives ont permis de valider cette approche et ont montré que la méthode de groupage khiops aboutit à des groupages performants à la fois en terme de qualité prédictive et de faible nombre de groupes\n",
      "accélération de em pour données qualitatives  études comparative de différentes versions\n",
      "algorithme em est très populaire et très efficace pour estimation de paramètres un modèle de mélange inconvénient majeur de cet algorithme est la lenteur de sa convergence son application sur des tableaux de grande taille pourrait ainsi prendre énormément de temps afin de remédier à ce problème nous étudions ici le comportement de plusieurs variantes connus de em ainsi qu'une nouvelle méthode celles-ci permettent accélérer la convergence de algorithme, tout en obtenant des résultats similaires à celui-ci dans ce travail nous nous concentrons sur aspect classification nous réalisons une étude comparative entre les différentes variantes sur des données simulées et réelles et proposons une stratégie utilisation de notre méthode qui s'avère très efficace\n",
      "acquisition de données vs gestion de connaissances  patrimoniales  le cas des vestiges du théâtre antique arles\n",
      "qu'y a t'il de commun aujourd'hui entre acquisition de données 3d la gestion informations patrimoniales ou encore la modélisation tridimensionnelle en temps réel  bien peu force est de le constater si ce n'est que édifice patrimonial sert là souvent de terrain d'expérimentation pourtant il ne saurait être réduit à ce seul statut  il est objet de connaissances dont étude doit bénéficier de différents jeux de technologies notre proposition expérimentée sur des vestiges du théâtre antique d'arles place cet édifice au centre un dispositif visant à intégrer au sein un système informations architecturales 3d en devenir les résultats de différentes phases de son étude un jeu de connaissances formalisé sur édifice sert de dénominateur commun depuis acquisition de données 3d jusqu'à la représentation dans une maquette temps réel pour la toile cette maquette devient outil de navigation dans le jeu informations et de savoirs qui caractérise édifice.\n",
      "analyse information relationnelle par des graphes interactifs de grandes tailles\n",
      "la découverte de connaissances à partir importantes masses de données hétérogènes débouche le plus souvent sur analyse relationnelle la recherche informations stratégiques s'appuie en effet sur les liens fonctionnels et sémantiques entre documents acteurs terminologie et concepts un domaine sans oublier le paramètre temps de nombreuses méthodes sont proposées pour identifier analyser et visualiser les mécanismes mis à jour  analyse relationnelle classifications supervisées et non supervisées analyse factorielle analyse sémantique cartes dendogrammes … mais ces approches demandent souvent une expertise non négligeable pour être comprises et ne s'adressent donc pas aux non initiés par contre la vue un graphe mettant en relation une ou deux classes éléments interdépendants est directement assimilable par tout le monde nous proposons donc un ensemble de visualisations interactives de graphes dont la manipulation doit permettre une découverte de connaissances intuitive et basée sur un langage graphique naturel nous illustrons notre propos de nombreux exemples tirés de cas réels analyses stratégiques qui ont permis évaluer cette approche sur un panel très large de données\n",
      "annotation automatique de documents xml\n",
      "nous proposons dans cet article un mécanisme automatique annotation de documents ce mécanisme s'appuie sur une opération de composition permettant de créer de nouveaux documents à partir de documents existants et sur un algorithme permettant inférer annotation un document composé à partir annotation de ses parties notre modèle est illustré par une étude de cas consacrée à la mise en commun de documents pédagogiques au format xml dans un environnement coopératif enseignement à distance nous décrivons un prototype permettant annoter ces documents et engendrer une description rdf contenant les annotations\n",
      "apprentissage des réseaux bayésiens avec des graphes chaînés maximaux\n",
      "nan\n",
      "apprentissage et optimisation conjoints  extraction de connaissances pertinentes sur les systèmes de production\n",
      "nan\n",
      "apprentissage incrémental des profils dans un système de filtrage information\n",
      "cet article présente une méthode apprentissage des profils dans les systèmes de filtrage d'information le processus apprentissage est effectué une manière incrémentale au fur et à mesure que les informations sont filtrées et jugées par l'utilisateur des expérimentations effectuées sur une collection de test de référence trec montrent que la méthode permet effectivement amélioration des profils\n",
      "approche binaire pour la génération de fortes règles association\n",
      "dans ce papier nous proposons une nouvelle méthode extraction des règles association dans des bases de données relationnelles basée sur la technologie des arbres de peano (ptree) la structure de données utilisée pour représenter la base de données est un ensemble de ptrees de base représentant chacun un vecteur binaire et tous ces ptrees sont stockés dans des fichiers binaires nous montrons que la structure ptree combinée avec la technique de réduction appelée élagage par support minimum produisent des règles association fortes et réduisent considérablement le temps de construction de l'association en effet notre approche présente avantage de ne pas effectuer des parcours coûteux de la base de données cette approche a été testée à travers un prototype que nous avons implémenté les résultats expérimentaux montrent que les règles association fortes sont générées dans un temps minimum comparativement à autres travaux\n",
      "approche innovante pour la recherche et extractin coopérative et dynamique informations sur internet\n",
      "il existe de nombreuses techniques qui permettent de classifier les documents textuels en fonction de intérêt un utilisateur (knn svm ) malheureusement intégration de ces méthodes dans les plates-formes de textmining est souvent très statique au cours du temps le but de cet article est de présenter une plate-forme de webmining dans laquelle les données hétérogènes sont représentées uniformément selon un formalisme xml/tei et où utilisateur peut interagir sur les processus de récupération et analyse de ces données pour cela les modules de traitements sont représentés par des agents fonctionnant sur la plate-forme madkit et apprentissage se fait par une méthode dérivée de vsm et tfidf utilisant un principe de listes noires pondérées permettant la reconnaissance de documents indésirables la dynamique de la plate-forme repose principalement sur la possibilité ajouter à la volée des agents de traitement et de pouvoir modifier ordre et les paramètres analyse des documents\n",
      "beluga  un outil pour analyse dynamique des connaissances de la littérature scientifique un domaine première application au cas des maladies à prions\n",
      "un projet ciblé sur étude du domaine des maladies à prions à permis de formaliser une méthodologie commune sociologique et informatique de compréhension de sa dynamique par analyse thématique nous avons créé une plate forme indexation de notices bibliographiques dont le but est extraire des associations évoluant à travers des intervalles de temps beluga propose une chaîne de traitement basée sur indexation des documents en unités de base  références auteurs termes simples et composés organismes outil est fondé sur une double approche apprentissage et de visualisation qui automatise les processus extraction de groupes auteurs et de termes et permet à utilisateur de revenir aux données documentaires sources analyse diachronique de corpus de documents électroniques nous permet analyser comment la terminologie est structurée en thématiques émergentes\n",
      "booloader  un chargeur efficace dédié aux bases de transactions denses\n",
      "nous nous intéressons à la représentation et au chargement de bases de transactions en mémoire pour cela nous proposons utiliser un format condensé fondé sur les diagrammes de décision binaires et nous présentons un algorithme que nous avons implanté en un système baptisé booloader pour charger des bases de transactions nous donnons également des résultats expérimentaux de notre système sur des bases éparses et denses\n",
      "caractérisation de signatures complexes dans des familles de protéines distantes\n",
      "identification de signatures de protéines est un problème majeur pour la découverte de nouveaux membres dans des familles de protéines connues le concept de signature qui permet de caractériser ces familles est généralement basé sur la définition de motifs communs il s'avère que les familles de protéines connues le concept de signature qui permet de caractériser ces familles est généralement basé sur la définition de motifs communs il s'avère que les familles distantes sont trop hétérogènes pour qu'on puisse identifier les régions conservées à partir des algorithmes classiques de la bioinformatique nous proposons une approche génétique pour la découverte de motifs hiérarchiques algorithme suit une démarche descendante en s'appuyant dans une première phase sur les classes physico-chimiques des acides aminés les signatures sont ensuite définies par des séquences des motifs ainsi obtenus elles sont extraites au moyen un algorithme de découverte itemsets séquentiels où les motifs jouent le rôle d'items une dernière étape consiste à fouiller dans cette base itemsets pour n'en retenir qu'un ensemble réduit de signatures plusieurs stratégies sont proposés pour déterminer un ensemble optimal de signatures qui respecte des contraintes de complétude de cardinalité et de spécificité nous appliquons notre démarche sur la famille des cytokines analyse de la base de protéines scop a montré que les groupes de signatures que nous avons extrait cible spécifiquement cette famille d'intérêt\n",
      "caractérisation globale de exécution de jobs\n",
      "la caractérisation globale de exécution de jobs passe par exploitation de mesures recueillies sur les machines en production afin de répondre à la problématique il est nécessaire de tenir compte des différents types de données ainsi que de la dualité de la caractérisation  statique et dynamique une solution technique répondant aux contraintes est proposée elle repose sur utilisation de svm afin de détecter des phases et à un niveau supérieur à un réseau bayésien afin automatiser analyse de modèles de markov enrichis ceux-ci sont introduits comme la base formelle et synthétique de description du comportement du job aussi bien sur un système batch que parallèle enfin les résultats obtenus à aide un prototype sont discutés\n",
      "cartographie sémantique des connaissances à la carte\n",
      "nan\n",
      "classer pour découvrir  une nouvelle méthode analyse du comportement de tous les utilisateurs un site web\n",
      "analyse du comportement des utilisateurs un site web est un domaine riche et complexe le grand nombre de méthodes extraction de connaissances appliquées aux logs web ainsi que la diversité du type de ces méthodes en est une preuve cependant compte tenu de cette complexité nous posons dans cet article la question suivante  est-il possible de combiner des méthodes existantes pour proposer une analyse qui tire profit des résultats de plusieurs spécialités et extraire par exemple des comportements fréquents minoritaires ?notre étude à donc porté sur une nouvelle approche hybride issue de la classification neuronale et de la recherche de motifs séquentiels visant à classer les navigations des utilisateurs un site à aide de leurs résumés sémantiques puis pour chaque classe de navigations en extraire les comportements fréquents notre objectif est 1 de pallier les limites de extraction de motifs fréquents par rapport à la quantité de données à traiter et aussi par rapport à la qualité des résultats et 2 de pallier les limites une première méthode analyse du comportement appelée \"diviser pour découvrir\" que nous avons proposé en 2003 nous avons mené des expérimentations sur les logs http des sites inria les résultats obtenus confirment le bien fondé de notre approche vis à vis de état de l'art\n",
      "classification automatique images\n",
      "nan\n",
      "construction de variables et arbre de décision\n",
      "nan\n",
      "contrôle du risque multiple pour la sélection de règles association significatives\n",
      "les algorithmes extraction de règles association parcourent efficacement le treillis des itemsets pour constituer une base de règles admissibles à des seuils de support et de confiance mais donnent une multitude de règles peu exploitables nous suggérons épurer de telles bases en éliminant les règles non statistiquement significatives la multitude de tests pratiqués conduit mécaniquement à multiplier les règles sélectionnées à tort après avoir présenté des procédures issues de la biostatistique qui contrôlent non pas le risque mais le nombre de fausses découvertes nous proposons bs_df un algorithme original fondé sur le bootstrat qui sélectionne les règles significatives en contrôlant le nombre de fausses découvertes des expérimentations montrer efficacité de ces procédures\n",
      "découverte de régularités pour intégration de  données semi structurées\n",
      "cet article présente utilisation une technique de fouille de données pour aider à la spécification de vues sur des sources xml notre langage de vues permet intégrer des données xml provenant de sources hétérogènes cependant la définition de motifs sur les sources permettant de spécifier les données à extraire est souvent difficile car la structure des données n'est pas toujours connue nous proposons extraire les structures fréquentes dans les données des sources pour spécifier des motifs pertinents à utiliser dans la spécification des vues\n",
      "détection par svm - application à la détection de churn en téléphonie mobile prépayée\n",
      "nan\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "etiq un étiqueteur inductif convivail pour les corpus de spécialité\n",
      "nan\n",
      "étude de textes par leur image\n",
      "nous proposons une méthode automatique de comparaison de textes reposant sur une technique de transformation un texte en une image de taille donnée et analyse à aide des outils de la géométrie fractale nous présentons une application à étude un corpus de 90 textes longs\n",
      "étude expérimentale de mesures de qualité de règles association\n",
      "la validation des connaissances extraites un processus ecd par un expert métier nécessite de filtrer ces connaissances pour ce faire de nombreuses mesures ont été proposées chacune répondant à des besoins spécifiques ces mesures présentent des caractéristiques variées et parfois contradictoires qu'il convient alors d'examiner arguant du fait que la sélection des bonnes connaissances passe aussi par utilisation un ensemble de mesures adaptées au contexte nous présentons dans cet article une étude expérimentale de différentes mesures cette étude est mise en regard une étude formelle synthétisant les qualités des mesures\n",
      "exit  extraction itérative de la terminologie\n",
      "nan\n",
      "extraction de connaissances grâce à un outil de text-mining application à la veille informationnelle dans le cadre policier\n",
      "nan\n",
      "extraction de processus fonctionnels en génétique des  microbes à partir de résumés medline\n",
      "après ère du décodage des génomes les biologistes sont de plus en plus confrontés à intégration de myriades de connaissances parcellaires stockées majoritairement sous forme textuelle nous montrons à travers un exemple concret que la conjonction de deux chaînes de traitement faisant appel de façon modérée à expertise humaine offre au biologiste une aide utile pour parcourir cette littérature à partir une structuration sans a priori de son corpus  il s'agit ici de résumés medline indexés par les gènes et protéines qu'ils citent et que algorithme structure sans superviseur en principales voies métaboliques et de régulation présentes dans le corpus choisi 1 une chaîne indexation par les noms de gènes et protéines inclut un expert pour valider 2 un environnement interactif de clustering thématique attribue des valeurs graduées de centralité dans chaque thème aux résumés comme aux noms comme à toute autre variable illustrative autres termes bio. mesh …)\n",
      "extraction of text summary using latent semantic indexing and information retrieval technique  comparison of four strategies\n",
      "in this paper we present four generic text summarization techniques each technique extracts a text summary by ranking and extracting sentences from an original document the first method summarizer 1 uses standard information retrieval ir) methods to rank sentences the second method summarizer 2 uses the latent semantic analysis lsa) technique to identify semantically important sentences for summary creations the third method summarizer 3 uses a combination of the latent semantic analysis technique reduction and relevance measure the fourth method simply uses the tf*idf term frequency * inverse document frequency weighting scheme evaluations of the four methods are conducted using document understanding conferences duc) datasets from nist we have compared the summary of each method with the manual summaries summarizer 4 with its lowest overhead has comparable performance to summarizer 1 analysis shows that a combination of lsa technique and the relevance measure summarizer 3 has the best performance on an average\n",
      "fonctionnement un système informatique aide à la décision siad)\n",
      "cet article présente le fonctionnement un siad  alimentation traitement des données qui l'alimentent production des résultats outils de consultation mis à la disposition des utilisateurs exploitation éditoriale\n",
      "fonctions oubli dans les entrpôts de données\n",
      "les entrepôts de données stockent des quantités de données de plus en plus massives en particulier du fait de la constitution d'historiques nous proposons ici une solution pour éviter la saturation des entrepôts de données nous définissons un langage de spécifications de fonctions oubli des données les plus anciennes permettant de déterminer ce qui doit être présent dans entrepôt de données à chaque instant ces spécifications de fonctions oubli se traduisent par des opérations de résumé par agrégation et par des opérations de suppression des données anciennes réalisées de façon mécanique à chaque pas de mise à jour la communication présente tout abord une description syntaxique du langage de spécifications des fonctions oubli. les contraintes à vérifier pour assurer la cohérence du langage sont ensuite décrites enfin nous proposons des structures de données adaptées au stockage des données nécessaires à la gestion des fonctions oubli.\n",
      "fouille dans la structure de documents xml\n",
      "la prolifération des documents xml appelle des techniques appropriées pour extraire et exploiter information contenue dans ces documents on distingue deux approches de fouille  xml content mining portant sur le contenu et xml structure lining qui a trait à la structure des documents combiner ces deux approches est très intéressant les informations contenues dans la structure orientent la fouille sur le contenu nous présentons la première étape de cette démarche  une nouvelle méthode extraction des règles association à partir de la structure des documents xml qui permet de gérer les aspects hiérarchiques de ces documents tout en améliorant les mécanismes extraction grâce à la création une structure spéciale représentant la hiérarchie des balises rencontrées\n",
      "fouille de grands ensembles de données avec un boosting proximal svm\n",
      "les svm support vector machines ont montré leur efficacité dans plusieurs domaines d'application apprentissage des svm se ramène à résoudre un programme quadratique dont la mise en oeuvre est en général coûteuse en temps une reformulation plus récente des svm proximal svm) proposée par fung et mangasarian ne nécessite que la résolution un système linéaire cet algorithme de psvm est plus efficace et permet de traiter des données dont le nombre individus est très important 109) et le nombre attributs plus restreint (104) nous proposons utiliser la formule de sherman-morrison-woodbury pour adapter le psvm à la fouille ensembles de données dont le nombre attributs est très important et le nombre individus plus restreint sur un matériel standard puis nous présentons un algorithme de boosting de psvm pour classifier des données de très grandes tailles en nombre individus et attributs. nous évaluons les performances du nouvel algorithme sur les ensembles de données de l'uci twonorm ringnorm reuters-21578 et ndc\n",
      "gestion de données hétérogènes dans un entrepôt de données\n",
      "nan\n",
      "gvsr  un annuaire de logiciels de manipulation et édition de graphes\n",
      "nan\n",
      "how well go lattice algorithms on currently used machine leaning testbeds \n",
      "many research papers in classification or association rules increase the interest of concept lattices structures for data mining dm) and machine learning (ml) to increase the efficiency of concept lattice-bases algorithms in ml it is necessary to make us of an efficient algorithms to build concept lattices in fact more than ten algorithms for generating concept lattices were published as real data sets for data mining are very large concept lattice structure suffers form its complexity issues on such data the efficiency and performance of concept lattices algorithms are very different from one to another so we need to compare the existing lattice algorithms with large data we implemented the four first algorithms in java environment and compared these algorithms on about 30 datasets of the uci repository that are well established to be used to compare ml algorithms preliminary results give preference to ganter's algorithm and then to bordat's algorithm which do not fil well with the recommendations of kuznetsov and obiedkov furthermore we analyzed the duality of lattice-based algorithms\n",
      "identification de blocs homogènes sur des données continues\n",
      "contrairement aux méthodes usuelles de classification ne cherchant généralement qu'une seule partition soit des instances soit des attributs les méthodes de classification croisée et de classification directe fournissent des blocs de données liant des instances à des attributs les premières consistent à chercher simultanément une partition en lignes et une partition en colonnes les secondes elles s'appliquent directement sur les données et permettent obtenir des blocs de données homogènes de toutes tailles ainsi que des hiérarchies de classes en lignes et en colonnes combinant les avantages des deux méthodes nous présentons ici une méthodologie permettant de travailler sur de grandes bases de données\n",
      "induction extensionnelle  définition et application à acquisition de concepts à partir de textes\n",
      "lorsque des outils inductifs sont inclus dans un système acquisition des connaissances on dit que on construit un système apprenti c'est dans le but de soulager la charge de travail de expert du domaine que cette forme apprentissage comporte des outils inductifs la difficulté tient en ce que énumération des connaissances expertes produit des données peu bruitées mais très incomplètes que les itérations successives induction vont compléter toutefois en y ajoutant de grandes quantités de bruit il en résulte qu'on doit utiliser des procédures inductives spéciales adaptées à apprentissage par croissance de noyaux de connaissance supervisée en particulier pour résoudre le problème difficile de la reconnaissance de concepts dans les textes nous avons défini une forme apprentissage qui intègre apprentissage à partir instances et les systèmes apprentis que nous nommons \"induction extensionnelle\" un oxymoron qui souligne que malgré absence de création un modèle explicite une induction prend effectivement place\n",
      "intégration efficace de méthodes  de fouille de données dans les sgbd\n",
      "cet article présente une nouvelle approche permettant appliquer des algorithmes de fouille en particulier apprentissage supervisé à de grandes bases de données et en des temps de traitement acceptables cet objectif est atteint en intégrant ces algorithmes dans un sgbd ainsi nous ne sommes limités que par la taille du disque et plus par celle de la mémoire cependant les entrées-sorties nécessaires pour accéder à la base engendrent des temps de traitement longs nous proposons donc dans cet article une méthode originale pour réduire la taille de la base apprentissage en construisant sa table de contingence les algorithmes apprentissage sont alors adaptés pour s'appliquer à la table de contingence afin de valider notre approche nous avons implémenté la méthode de construction arbre de décision id3 et montré que utilisation de la table de contingence permet obtenir des temps de traitements équivalents à ceux des logiciels classiques\n",
      "interrogation de sources biomédicales  prise en compte des préférences de utilisateur\n",
      "nous nous plaçons dans le cadre un projet de constitution une plate-forme intégrative de données biomédicales pour étude génomique des cancers la plate-forme comporte entre autres un certain nombre de scénarios analyse qui sont proposés à utilisateur a chaque étape un scénario qu'il a choisi de réaliser pour les besoins de son étude utilisateur peut être amené à poser une requête nécessitant accéder à différentes sources et il doit alors choisir les sources pertinentes nous proposons un guide à utilisateur sous forme un algorithme de sélection de sources adapté à sa requête et à ses préférences pour cela nous explorons quelques spécificités des banques de données biomédicales et définissons différents critères de préférence utiles pour les biologistes nous illustrons notre démarche avec un exemple de requête biomédicale\n",
      "le e-lien une solution pour extraction et le partage de connaissances structurées dans les documents hypertextuels\n",
      "nan\n",
      "les règles association comme outil de catégorisation textuelle\n",
      "nan\n",
      "maintenance de bases de connaissances terminologiques\n",
      "acquisition des connaissances terminologiques de entreprise se fait souvent à partir des textes qu'elle utilise dans le cadre de ce travail la base de connaissances terminologiques repose sur la modélisation des concepts-métier sous la forme une ontologie le problème de la maintenance de cette base et de cette ontologie doit alors être traité.dans cet article après avoir donné une définition une base de connaissances terminologiques bct) et des problèmes de diachronie nous présentons notre modèle et notre méthode acquisition des connaissances terminologiques de entreprise. nous exposons alors notre proposition pour maintenir au cours du temps la base de connaissances terminologiques ainsi construite.nous illustrons ce travail sur une base de connaissance terminologique sur le cinéma animation en décrivant le problème de la maintenance dans une reconstitution historique de différents états de cette base lors de apparition des techniques numériques animation.\n",
      "manipulation de représentations de cubes de données\n",
      "nan\n",
      "mediating the semantic web\n",
      "cet article développe une extension une architecture de médiation pour intégrer le web sémantique plus précisément xlive est un médiateur tout xml développé à prism il permet exécuter des xquery sur des sources de données hétérogènes après une rapide présentation de xlive et du web sémantique une architecture à trois niveaux ontologies et de schémas est introduite pour connecter des adaptateurs pour le web sémantique cette architecture vise à intégrer des sources de type web service information conformément à une ontologie globale de référence elle conduit à étendre xlive avec le support de vues un outil de conception de vues et de mappings et des adaptateurs pour les web services\n",
      "mesurer la qualité des règles et de leurs contraposées avec le taux informationnel tic\n",
      "la validation des connaissances est une des étapes les plus problématiques un processus de découverte de règles d'association pour que le décideur expert des données puisse trouver des connaissances intéressantes dans les grandes quantités de règles produites par les algorithmes de fouille de données il est nécessaire de mesurer la qualité des règles nous insérant dans le cadre de analyse statistique implicative nous proposons dans cet article évaluer les règles en considérant leur contenu informationnel à travers un nouvel indice de qualité fondé sur entropie de shannon  tic taux informationnel modulé par la contraposée) cet indice a avantage être bien adapté à la sémantique des règles puisque une part il respecte leur caractère asymétrique et autre part il tire profit de leurs contraposées par ailleurs c'est à notre connaissance la seule mesure de qualité de règles qui intègre à la fois indépendance et déséquilibre c'est-à-dire qui permette de rejeter simultanément les règles entre variables corrélées négativement et les règles qui possèdent plus de contre-exemples que d'exemples des comparaisons de tic avec la j-mesure information mutuelle indice de gini et la confiance sont réalisées sur des simulations numériques\n",
      "mesurer les usages internet\n",
      "nous rendons compte une démarche mise en place pour construire une représentation fine des usages internet et de leur évolution en procédant à du traitement secondaire de données de trafic provenant de panels représentatifs d'internautes après avoir présenté les caractéristiques des cohortes étudiées et les différents modes enrichissement des données de trafic mis en place nous présentons quelques résultats construits à partir de ces données enrichies et en particulier une segmentation des internautes construite sur la base de entrelacement des pratiques de communication et de navigation\n",
      "mise en oeuvre des méthodes de fouille de données spatiales alternatives et performances\n",
      "la fouille de données spatiales nécessite analyse des interactions dans l'espace ces interactions peuvent être matérialisées dans des tables de distances ramenant ainsi la fouille de données spatiales à analyse multitables or les méthodes de fouilles de données traditionnelles considèrent une seule table en entrée où chaque tuple est une observation à analyser de simples jointures entre ces tables ne résoud pas le problème et fausse les résultats en raison du comptage multiple des observations nous proposons trois alternatives de fouille de données multi-tables dans le cadre de la fouille des données spatiales la première consiste à interroger à la volée les différentes tables et modifie en dur les algorithmes existants la seconde est une optimisation de la première qui pré -calcule les jointures et adapte les algorithmes existants la troisième réorganise les données dans une table unique en complétant - et non en joignant- la table analyse par les données présentes dans les autres tables ensuite applique un algorithme standard sans modification cet article présente ces trois alternatives il décrit leur implémentation pour la classification supervisée et compare leur performance\n",
      "modèle de gestion intégrée des compétences et connaissances\n",
      "la compétence et la connaissance sont deux concepts qui nous semblent fortement conjoints cependant ils sont rarement étudiés et gérés ensemble nous cherchons donc à identifier les liens et frontières qui peuvent exister entre eux ceci a pour objectif de développer un modèle de représentation et de gestion intégré aux connaissances et aux compétences dans cet article est tout abord présentée une synthèse sur les concepts de compétence et de connaissance ensuite les modèles et outils de gestion de ces concepts sont exposés puis le modèle ckim competency and knowledge integrated model développé est défini les utilités de ce modèle et son exploitation sont discutées en quatrième partie la dernière partie représente un prototype implantation du modèle ckim réalisé sur le serveur de connaissances athanor\n",
      "modèle topologique pour interrogation des bases images\n",
      "nous proposons dans cet article un modèle topologique de représentation de bases d'images chaque image est représentée à aide un vecteur de caractéristiques dans r^p et figure comme noeud dans un graphe de voisinage exploration du graphe correspond à la navigation dans la base de données les voisins un noeud représentent des images similaires afin de pouvoir traiter des requêtes nous définissons un modèle topologique image requête est représentée par un vecteur de caractéristiques dans r^p et insérée dans le graphe en mettant à jour localement les relations de voisinage ce travail se positionne dans le domaine de la fouille de données complexes\n",
      "modélisation dynamique et temporelle de utilisateur pour un filtrage personnalisé de documents textuels\n",
      "apprentissage efficace du profil utilisateur est un challenge car il évolue sans cesse dans cet article nous proposons une nouvelle approche pour apprentissage du profil long-terme de utilisateur pour le filtrage de documents textuels dans ce cadre les documents consultés sont classés de manière dynamique et nous analysons la répartition dans le temps des classes de documents afin de déterminer le mieux possible les classes intérêts de utilisateur. étude empirique confirme la pertinence de notre approche pour une meilleure personnalisation de documents\n",
      "musette  a framework for knowledge capture from experience\n",
      "nous présentons dans cet article une nouvelle approche de modélisation de expérience utilisation un système informatique avec pour objectif de réutiliser cette expérience en contexte pour assister utilisateur à effectuer sa tâche quatre scénarios illustrent cette approche\n",
      "opac  opérateur analyse en ligne basé sur une technique de fouilles de données\n",
      "analyse en ligne olap on-line analysis processing et la fouille de données data mining sont deux champs de recherche qui ont connu depuis quelques années des évolutions parallèles et indépendantes de récentes études ont montré importance et intérêt de association entre ces deux domaines scientifiques a heure actuelle on assiste à accroissement du besoin une analyse en ligne plus élaborée nous pensons que le couplage entre olap et la fouille de données pourra apporter des réponses à ce besoin dans cet article nous proposons adopter ce couplage en vue de créer un nouvel opérateur baptisé opac opérateur agrégation par classification) analyse en ligne des données multidimensionnelles opac consiste particulièrement en agrégation sémantique des modalités une dimension un cube de données en se basant sur la technique de la classification ascendante hiérarchique\n",
      "optimisation des requêtes temporelles sur le web\n",
      "nan\n",
      "outil de représentation des évolutions de communautés intérêts\n",
      "cet article présente un système de visualisation permettant observation des comportements collectifs implicites il s'agit de reconnaître et de représenter des communautés à partir des connexions internet des utilisateurs  les utilisateurs sont répartis en communautés en fonction des similarités entre des listes de termes établies sur analyse des documents consultés par chacun d'eux étude est rendue dynamique par la comparaison des communautés reconnues sur des périodes de temps connexes outil décrit ci après offre deux représentations différentes de ces communautés  une vision des liaisons thématiques entre les utilisateurs sur chaque période étudiée et une vue comparative des communautés reconnues sur toute la durée de étude.\n",
      "poboc  un algorithme de \n",
      "nous décrivons algorithme poboc pole-based overlapping clustering qui génère un ensemble de clusters non-disjoints ou \"softclusters\" présentés sous forme une hiérarchie de concepts à partir de la seule matrice de similarités sur les données considérées nous évaluons approche sur deux situations apprentissage  la classification par apprentissage de règles et organisation de données plus complexes et peu structurées telles que les données textuellesla validation des méthodes de clustering est une étape difficile résolue le plus souvent par une évaluation d'experts les deux applications proposées permettent de valider la méthode organisation selon deux points de vue  une part quantitativement en évaluant influence de la méthode pour la classification autre part en permettant une analyse \"humaine\" du résultat dans le cas des données textuelles nous mettons en évidence intérêt de poboc comparativement à autres approches apprentissage non-supervisé\n",
      "positionnement multidimensionnel et partitionnement pour la visualisation de données multivariées\n",
      "nan\n",
      "qualité et datawarehouse dans le milieu hospitalier\n",
      "nan\n",
      "recherche ciblée de documents sur le web\n",
      "les langages de requêtes mots-clés pour le web manquent souvent de précision lorsqu'il s'agit de rechercher des documents particuliers difficilement caractérisables par de simples mots-clés exemple  des cours java ou des photos de formule 1) nous proposons un langage multi-critères de type attribut-valeur pour augmenter la précision de la recherche de documents sur le web.nous avons expérimentalement montré le gain de précision de la recherche de documents basé sur ce langage\n",
      "recherche dans de grandes bases images fixes  une nouvelle approche guidée par les règles association\n",
      "une base images fixes peut être décrite de plusieurs façons notamment par des descripteurs visuels globaux de couleur de texture ou de forme les requêtes les plus fréquentes impliquent et combinent les résultats de plusieurs types de descripteurs  par exemple \"retrouver toutes les images ayant une couleur et une texture semblables à celles une image requête donnée\" pour retrouver plus efficacement et plus rapidement une image dans une grande base nous exploitons des combinaisons appropriées de descripteurs et étudions intérêt des règles association entre clusters de descripteurs pour accélérer le temps de réponse à des requêtes sur de grandes bases images fixes\n",
      "recherche de règles association hiérarchiques par une approche anthropocentrée\n",
      "extraction de connaissances dans la bases de données est devenue pour les banques une alternative au problème lié à la quantité de données qui sont stockées et qui ne cessent d'augmenter ceci aboutit à un paradoxe puisqu'il faut mieux cibler la clientèle susceptible être intéressée par une offre en utilisant des méthodes qui ne permettent plus de traiter le nombre croissant enregistrements des bases de données nos travaux se situent dans la continuité une étude que nous avons réalisée sur la recherche de règles association appliquée au marketing bancaire en effet des premiers résultats encourageants nous ont conduit à approfondir nos travaux vers une recherche de règles association hiérarchiques utilisant non plus une approche automatique mais une approche anthropocentrée il s'agit une approche dans laquelle expert fait partie intégrante du processus en jouant le rôle heuristique évolutive cet article présente les résultats de notre démarche de recherche\n",
      "réduction un jeu de règles association par des méta-règles issues de la logique du \n",
      "nan\n",
      "réduction du coût évaluation une règle relationnelle\n",
      "de nombreuses tâches en fouille de données visent à extraire des connaissances exprimées sous la forme un ensemble de règles les algorithmes dédiés à ces tâches engendrent des règles dont adéquation aux données doit être évaluée on se place dans le cadre où cette évaluation est réalisée directement en lançant des requêtes de dénombrement sur la base de données et où cette base est relationnelle les requêtes comptent les données qui s'apparient avec la règle calcul qui peut être extrêmement coûteux dans cet article nous étudions impact une approche échantillonnage visant à réduire le coût de évaluation des règles relationnelles en tenant compte des spécificités structurelles des requêtes induites\n",
      "règles identification et méthodes de visualisation objets architecturaux\n",
      "dans étude du patrimoine bâti la gestion informations pose aujourd'hui des problèmes interfaçage non triviaux notamment par la masse la diversité la complexité et le caractère hétérogène des contenus la représentation tridimensionnelle du tissu urbain à différentes échelles de la ville au corpus architectural) parce qu'elle localise spatialement information à délivrer et attache à la morphologie de l'édifice apparaît comme une des réponses possibles cette réponse semble par ailleurs bien adaptée aux problématiques spécifiques de analyse architecturale du patrimoine que sont par exemple la restitution édifices disparus et les notions incertitude qui s'y attachent ou le réemploi éléments de corpus pourtant la représentation tridimensionnelle dans notre champ application est aujourd'hui loin de remplir ce rôle notre contribution vise à discuter quelques uns des pré-requis qui nous semblent s'imposer à la lumière de nos expériences pour faire de la maquette 3d un outil investigation des connaissances sur l'édifice\n",
      "régression linéaire symbolique avec variables taxonomiques\n",
      "le présent papier concerne extension des méthodes classiques de régression linéaire aux cas des données symboliques et fait suite à de précédents travaux de billard et diday sur la régression linéaire avec variables intervalles et histogrammes dans ce papier nous présentons des méthodes de régression avec variables taxonomiques les variables taxonomiques sont des variables organisées en arbre exprimant plusieurs niveaux de généralité les villes sont regroupées en régions qui sont elles-mêmes regroupées en pays) la méthode proposée sera testée sur données simulées finalement nous observerons que ces méthodes nous permettent utiliser la régression linéaire pour étudier des concepts et pour réduire le nombre de données afin améliorer les résultats obtenus par rapport à une régression classique\n",
      "relations entre gènes impliqués dans les cancers de la thyroïde\n",
      "des relations entre gènes et protéines impliqués dans les cancers de la thyroïde ont été mises en évidence par analyse un important corpus de résumés de  la base de données bibliographique medline une approche pluridisciplinaire (biologistes cliniciens linguistes et chercheurs en sciences de l'information a permis indexation automatique et analyse de ce corpus indexation contrôlée structurée en classes sémantiques à partir de vastes ressources hétérogènes les bases biomédicales et génétiques umls et locuslink) prend en compte la spécificité des termes  nomenclatures biochimiques acronymes de gènes aberrations chromosomiques ou encore variantes linguistiques de termes les deux méthodes de classification complémentaires appliquées révèlent un réseau lexical dense de gènes concurrents autour de trois principales pathologies de la thyroïde  les cancers médullaires papillaires et des dysfonctionnements du système immunitaire les développements apportés aux outils de visualisation interactifs du serveur visa de inist facilitent lecture et navigation au sein des documents\n",
      "représentation condensée de motifs émergents\n",
      "les motifs émergents sont des associations de caractéristiques fortement présentes dans une classe et rares dans les autres ils font ressortir les distinctions entre classes et se révèlent particulièrement efficaces pour construire des classifieurs et apporter une aide au diagnostic à cause de la forte combinatoire du problème la recherche et la représentation des motifs émergents restent des tâches complexes pour de grandes bases de données nous proposons ici une représentation condensée exacte des motifs émergents (i.e. les motifs et leurs taux de croissance sont directement obtenus depuis la représentation condensée) idée principale est de s'appuyer sur les récents résultats relatifs aux représentations condensées de motifs fermés fréquents à partir de cette représentation nous donnons aussi une méthode aisée à mettre en oeuvre pour obtenir les motifs émergents ayant les meilleurs taux de croissance ces motifs appelés motifs émergents forts ont été exploités avec succès dans une collaboration avec la société philips\n",
      "représentation de graphes par acp granulaire\n",
      "extraction information de grands graphes repose le plus souvent sur leur représentation dans des espaces de dimension réduite et on utilise généralement des méthodes factorielles appliquées à des mesures de dissimilarités calculées à partir des matrices associée du graphe ou analyse spectrale de leur laplacien discret efficaces pour dégager les structures globales ces représentations sont parfois peu exploitables dès lors que on s'intéresse à une perspective du graphe à partir de certains sommets privilégiés or information recherchée a souvent un caractère \"local\" pour représenter le graphe du point de vue un ou plusieurs sommets sélectionnés nous proposons une méthode analyse en composantes principales \"granulaire\" consistant à appliquer une acp \"filtrée\" à un tableau de proximités la visualisation un graphe de dictionnaire dont la mesure de proximité est obtenue à partir un algorithme original illustre notre propos\n",
      "résumé de cubes de données multidimensionnelles à aide de règles floues\n",
      "dans le contexte des entrepôts de données et des magasins de données multidimensionnelles les outils olap fournissent des moyens aux utilisateurs de naviguer dans leur données afin y découvrir des informations pertinentes cependant les données à traiter sons souvent très volumineuses et ne permettent pas une exploration systématique et exhaustive il s'agit donc de développer des traitements automatisés facilitant la visualisation et la navigation dans les données dans cet article nous étudions une méthode originale permettant de construire et identifier de manière automatique et efficace des blocs de données similaires présents dans les cubes de données pouvant être exprimés sous la forme de règles cette méthode est fondée sur utilisation combinée un algorithme par niveaux de type apriori et de la théorie des sous-ensembles flous cette théorie nous permet en effet de pallier les problèmes posés par le fait que les blocs de données calculés par notre algorithme peuvent se recouvrir\n",
      "sélection attributs et classification objets complexes\n",
      "nan\n",
      "sélection rapide en apprentissage supervisé\n",
      "la sélection de variables sdv) permet de réduire espace de représentation des données ce processus est de plus en plus critique en raison de augmentation de la taille des bases de données traditionnellement les méthodes de sdv nécessitent plusieurs accès au jeu de données ce qui peut représenter une part relativement importante du temps exécution de ces algorithmes nous proposons une nouvelle méthode efficiente et rapide ne nécessitant qu'un unique accès aux données) cette méthode utilise les algorithmes génétiques ainsi que des mesures de validité de classification non supervisée (cns)\n",
      "sous-ensembles flous définis sur une ontologie\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "les sous-ensembles flous peuvent être utilisés pour représenter des valeurs imprécises comme un intervalle aux limites mal définies ils peuvent également servir à expression de préférences dans les critères de sélection de requêtes en bases de données en représentation des connaissances utilisation de hiérarchies de types est largement répandue afin de modéliser les relations existant entre les types objets un domaine donné nous nous intéressons aux sous-ensembles flous dont le domaine de définition est une hiérarchie éléments partiellement ordonnés par la relation \"sorte de\" que nous appelons ontologie nous introduisons la notion de sous-ensemble flou défini sur une partie de l'ontologie puis sa forme développée définie sur ensemble de l'ontologie que nous appelons extension du sous-ensemble flou des classes équivalence de sous-ensembles flous définis sur une ontologie peuvent être caractérisées par un représentant unique que nous appelons sous-ensemble flou minimal nous concluons par un exemple application dans un système information relatif à la prévention du risque micro-biologique en sécurité alimentaire\n",
      "uitliation de connaissances pour aide à la recherche documentaire fondée sur le contenu\n",
      "nan\n",
      "un algorithme de génération des itemsets fermés pour la fouille de données\n",
      "le traitement de grand volume de données est un problème pour extraction de connaissances la fouille de données nécessite des méthodes de résolution efficaces le treillis de concepts treillis de galois est un outil utile pour analyse de données des travaux en classification et sur les règles association ont permis accroître son intérêt plusieurs algorithmes de génération on été proposés parmi lesquels nextclosure est un des meilleurs pour traiter des données de grande taille.mais la complexité de nextclosure reste malgré tout très élevé aussi nous proposons un nouvel algorithme efficace nommé scalingnextclosure et basé sur une méthode de partitionnement de données pour générer de manière indépendante les itemsets fermés de chaque partition les résultats expérimentaux montrer que cette technique de partitionnement améliore efficacement nextclosure\n",
      "une approche probabiliste pour le classement objets incomplets dans un arbre de décision\n",
      "nan\n",
      "une étude algorithmes de classification supervisée basée sur les treillis de galois\n",
      "nan\n",
      "une méthode pour appropriation de savoir-faire capitalisé avec mask\n",
      "la gestion explicite des savoirs et savoir-faire occupe une place de plus en plus importante dans les organisations la construction de mémoires entreprise dans un but de préservation et de partage est devenu une pratique assez courante cependant on oublie trop suivent que efficacité de ces activités est étroitement liée aux capacités appropriation et apprentissage des acteurs de organisation.dans cet article nous proposons des démarches générales accompagnement permettant de faciliter le processus appropriation des mémoires entreprise construits avec la méthode mask en exploitant des techniques ingénierie pédagogique\n",
      "utilisation des graphes de proximité dans le cadre de apprentissage basé sur les voisins\n",
      "la classification suivant les plus proches voisins est une règle simple et attractive basée sur une définition paramétrique du voisinage les graphes des proximité quand à eux induisent des notions plus souples de voisinage il s'agit ici effectuer la substitution.les variantes obtenues peu testées dans la bibliographie ont été soumises à une expérimentation intensive sur bases de données de uci et de france télécom on a ainsi considéré divers types de prétraitement des données et plusieurs catégories de graphes de plus on a caractérisé les effets du \"piège de la dimension\" sur le comportement théorique de tous les graphes présentés une quantification empirique du phénomène ayant été réalisée.il ressort de notre étude que utilisation du voisinage de gabriel provoque une amélioration en moyenne et que le prétraitement basé sur la statistique de rang est le plus adéquate quoiqu'il arrive des précautions doivent être prises en grande dimension\n",
      "validation de graphes conceptuels\n",
      "les travaux menés en validation des connaissances visent à améliorer la qualité des bases de connaissances le modèle des graphes conceptuels est un modèle de représentation des connaissances de la famille des réseaux sémantiques fondé sur la théorie des graphes et sur la logique du premier ordre nous proposons une solution pour valider sémantiquement une base de connaissances composée de graphes conceptuels la validation sémantique une base de connaissance consiste à confronter ses connaissances à des contraintes certifiées fiables nous proposons utiliser des contraintes descriptives exprimées sous forme de graphes conceptuels qui permettent de poser des conditions sur la représentation de certaines connaissance dans la base ces contraintes introduisent une notion de cardinalités et sont soit minimales soit maximales elles permettent respectivement exprimer \"si a alors au moins ou au plus n fois b\" la satisfaction de ces contraintes par une base de connaissances repose sur utilisation de opération de base du modèle des graphes conceptuels  la projection\n",
      "veille technologique assistée par la fouille de textes\n",
      "le domaine de la veille technologique vise à récolter traiter et analyser des informations scientifiques et techniques utiles aux acteurs économiques dans cet article nous proposons utiliser des techniques de fouille de textes pour automatiser le processus de traitement des données issues de bases de textes scientifiques toutefois la veille introduit une difficulté inhabituelle par rapport aux domaines application classiques des techniques de fouille de textes puisqu'au lieu de rechercher de la connaissances fréquente cachée dans les données il faut rechercher de la connaissance inattendue les mesures usuelles extraction de la connaissance à partir de textes doivent de ce fait être revues pour ce faire nous avons développé le système unexpectedminer dans lequel de nouvelles mesures permettent estimer le caractère inattendu un document notre système est évalué sur une base articles dans le domaine de apprentissage automatique\n",
      "vers un entrepôt de données pour la gestion des risques naturels\n",
      "les entrepôts de données sont un des plus importants développements dans le domaine des systèmes d'informations ils permettent intégrer des données de plusieurs sources souvent très volumineux distribuées et hétérogènes dans cet article nous examinons la possibilité utiliser la technique entrepôt de données dans la gestion des risques naturels nous présentons un modèle conceptuel pour entrepôt proposé avec la présence de formats et types variés de données tel que des données géographiques et multimédia nous proposons également des opérations olap pour la navigation des informations stockées dans le cube de données\n"
     ]
    }
   ],
   "source": [
    "# On effectue ces opérations sur tous les titres et tous les abstracts.\n",
    "resume = [tokenize(doc[\"title\"][0].lower())]\n",
    "abstract = [tokenize(str(doc[\"abstract\"][0].lower()))]\n",
    "for line in range(len(doc)): # title et abstract ont la même taille.\n",
    "    if line != 0:\n",
    "        resume.append(tokenize(doc[\"title\"][line].lower()))\n",
    "        abstract.append(tokenize(str(doc[\"abstract\"][line]).lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le traitment des données est acceptable. Un meilleur traitement pourrait être envisagé.\n",
    "\n",
    "Par exemple, le traitement des entités nommées peut être intéressant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut obtenir un seul jeu de données avec les titres et les abstracts pour pouvoir les traiter et éliminer les cas qui n'ont pas d'abstract."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est possible que seul le titre suffise. Il est possible que la racinisation ne soit pas nécéssaire."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On relie titre et abstract, sauf pour ceux qui ne disposent pas du second.\n",
    "for line in range(len(abstract)):\n",
    "    if abstract[line] != \"nan\":\n",
    "        for element in abstract[line]:\n",
    "            resume[line].append(element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tf-Idf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention : l'étape suivante est longue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Calcul du temps passé\n",
    "dico_time = time.process_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80576"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On se crée un dictionnaire avec tous les mots rencontrés.\n",
    "# dico constitue le vocabulaire connu du système.\n",
    "dico = []\n",
    "for line in resume:\n",
    "    for word in line:\n",
    "        dico.append(word)\n",
    "for word in dico:\n",
    "    if dico.count(word) != 1:\n",
    "        dico.remove\n",
    "len(dico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 767.73 secondes ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- %s secondes ---\" % round(time.process_time() - dico_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12159"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dico = list(set(dico))\n",
    "len(dico)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['décrira',\n",
       " 'est-il',\n",
       " \"qu'une\",\n",
       " 'ensemblistes',\n",
       " 'one',\n",
       " 'fouiller',\n",
       " 'leacock',\n",
       " 'similairement',\n",
       " 'd´agents',\n",
       " 'criminality',\n",
       " 'principles',\n",
       " 'pré-calcul',\n",
       " 'nearness',\n",
       " 'borner',\n",
       " 'beconsidered',\n",
       " 'unordered',\n",
       " 'révélant',\n",
       " 'portant',\n",
       " 'fonctiondes',\n",
       " 'réflexion',\n",
       " 'implantation',\n",
       " 'panoplie',\n",
       " 'telecom',\n",
       " 'governments',\n",
       " 'apporte',\n",
       " 'effects',\n",
       " 'empêche',\n",
       " \"n'avons\",\n",
       " 'grouper',\n",
       " 'flexible',\n",
       " 'ungraphe',\n",
       " 'aln',\n",
       " 'compromis',\n",
       " 'strategies',\n",
       " 'grandeconsommation',\n",
       " 'porteur',\n",
       " 'cbor',\n",
       " 'cabine',\n",
       " 'pgr',\n",
       " \"d'implantation\",\n",
       " 'échange',\n",
       " 'malédiction',\n",
       " 'probability',\n",
       " 'graduelles',\n",
       " 'banalisé',\n",
       " 'dimère',\n",
       " \"d'enquête\",\n",
       " 'nousmontrons',\n",
       " 'rétrospective',\n",
       " 'preliminary',\n",
       " 'reconciliates',\n",
       " 'blockchain',\n",
       " 'téléologique',\n",
       " 'biais',\n",
       " 'juriste',\n",
       " 'seinde',\n",
       " 'representation',\n",
       " 'knn',\n",
       " 'généralité',\n",
       " \"processusd'extraction\",\n",
       " 'victoire',\n",
       " 'eddectivement',\n",
       " 'supérieur',\n",
       " 'borgne',\n",
       " 'empirique',\n",
       " 'incorporation',\n",
       " 'établies',\n",
       " 'eclat',\n",
       " 'adaptateur',\n",
       " 'lbsn',\n",
       " 'krex',\n",
       " 'représentons',\n",
       " 'pairesde',\n",
       " 'contain',\n",
       " 'devraient',\n",
       " 'spatialisées',\n",
       " 'producteurs/consommateursquestions/réponses',\n",
       " 'caractérisée',\n",
       " 'migrateur',\n",
       " 'spontanés',\n",
       " 'recommandation',\n",
       " 'raisonneur',\n",
       " 'vector',\n",
       " 'informationserronées',\n",
       " 'android',\n",
       " 'favorisent',\n",
       " 'approchediachronique',\n",
       " 'vote',\n",
       " 'typique',\n",
       " 'latents',\n",
       " 'écosystème',\n",
       " 'pmms',\n",
       " 'generator',\n",
       " 'plupartdes',\n",
       " 'stimulated',\n",
       " 'exceptionnelles',\n",
       " 'multi-couche',\n",
       " 'mtr',\n",
       " 'proximity',\n",
       " 'first',\n",
       " 'orientées',\n",
       " 'surprisingness',\n",
       " 'noirhomme-fraiture',\n",
       " 'done',\n",
       " 'animale',\n",
       " 't-box',\n",
       " 'vt100',\n",
       " 'minimalité',\n",
       " 'arranged',\n",
       " 'composite',\n",
       " 'reconstituer',\n",
       " 'sageo',\n",
       " 'souplesse',\n",
       " 'montronsessentiellement',\n",
       " 'corrigée',\n",
       " 'validés',\n",
       " 'coûtde',\n",
       " 'gender',\n",
       " 'retenu',\n",
       " 'matlas',\n",
       " 'modèlesdont',\n",
       " 'could',\n",
       " 'adéquates',\n",
       " 'polynomial-delay',\n",
       " 'concentrer',\n",
       " 'appeler',\n",
       " 'mésoscopique',\n",
       " 'avecune',\n",
       " 'preference',\n",
       " 'collective',\n",
       " 'dominance',\n",
       " 'distributedweb',\n",
       " 'rélative',\n",
       " 'ciblant',\n",
       " 'attaque',\n",
       " 'vie',\n",
       " 'spatio-temporal',\n",
       " 'undoubtedly',\n",
       " 'rgcca',\n",
       " 'imdb',\n",
       " 'labellisation',\n",
       " 'nécessairement',\n",
       " 'télévision',\n",
       " 'crucial',\n",
       " 'sous-parties',\n",
       " 'offert',\n",
       " '24',\n",
       " 'obtenus',\n",
       " 'génomique',\n",
       " 'reproductibilité',\n",
       " 'réceptrices',\n",
       " 'saline',\n",
       " 'évitant',\n",
       " 'deuxméthodes',\n",
       " 'synchroniques',\n",
       " \"qu'ils\",\n",
       " 'ligneillustre',\n",
       " 'énorme',\n",
       " 'visually',\n",
       " '94',\n",
       " 'spécialisés',\n",
       " 'mené',\n",
       " 'photographs',\n",
       " 'évoluer',\n",
       " 'entité',\n",
       " 'xmlde',\n",
       " 'littéral',\n",
       " 'opère',\n",
       " 'cartocel',\n",
       " \"puisqu'il\",\n",
       " 'plante',\n",
       " 'neighbours',\n",
       " 'prémisse',\n",
       " 'summarize',\n",
       " 'transformant',\n",
       " 'radar',\n",
       " 'inconvénient',\n",
       " 'failure',\n",
       " 'multi-threading',\n",
       " 'becoming',\n",
       " 'discriminationde',\n",
       " 'compact',\n",
       " 'porphyry',\n",
       " 'hypergraphe',\n",
       " \"d'animaux\",\n",
       " 'khiops',\n",
       " 'toxicophores',\n",
       " 'automaton',\n",
       " 'hétérogène',\n",
       " 'inferred',\n",
       " \"d'appartenance\",\n",
       " 'classification',\n",
       " 'ajouter',\n",
       " 'inspirée',\n",
       " 'reliés',\n",
       " 'respecting',\n",
       " 'personnalisés',\n",
       " 'sparql-generate',\n",
       " 'pilotés',\n",
       " 'signe',\n",
       " 'existent',\n",
       " 'classées',\n",
       " 'sélection',\n",
       " 'accessibilité',\n",
       " 'lestechniques',\n",
       " 'politiques',\n",
       " 'alimentation',\n",
       " 'structurels',\n",
       " 'heritage',\n",
       " 'correlated',\n",
       " 'relationnelles',\n",
       " 'parasite',\n",
       " '|',\n",
       " 'minimizes',\n",
       " 'fenêtrage',\n",
       " 'curie',\n",
       " 'lesattaques',\n",
       " 'rbf-gene',\n",
       " 'durée',\n",
       " 'post-traitement',\n",
       " 'parameter-free',\n",
       " 'bref',\n",
       " 'agence',\n",
       " 'intensions',\n",
       " 'sits',\n",
       " 'réel',\n",
       " '1983',\n",
       " 'individus.ces',\n",
       " \"l'oral\",\n",
       " 'suffers',\n",
       " 'relationnelet',\n",
       " 'queles',\n",
       " 'dictionnaire',\n",
       " 'cherche',\n",
       " 'surinvestissement',\n",
       " 'patron',\n",
       " 'notion',\n",
       " 'inférer',\n",
       " 'majeure',\n",
       " 'lorsdes',\n",
       " 'methontology',\n",
       " 'créer',\n",
       " 'séparé',\n",
       " 'recommander',\n",
       " 'language',\n",
       " 'addition',\n",
       " 'coopérative',\n",
       " 'jointly',\n",
       " 'toone',\n",
       " 'sqe',\n",
       " 'puissance',\n",
       " 'arbitrairement',\n",
       " 'gratuit',\n",
       " 'heatmap',\n",
       " 'navigation',\n",
       " 'accessibility',\n",
       " 'duc',\n",
       " 'noeud',\n",
       " 'above',\n",
       " 'story',\n",
       " 'recorded',\n",
       " 'artifice',\n",
       " 'isolated',\n",
       " 'churn',\n",
       " 'valider',\n",
       " \"d'internet\",\n",
       " '0-1',\n",
       " 'commonly',\n",
       " 'connue',\n",
       " 'focused',\n",
       " 'eventually',\n",
       " 'simultanéité',\n",
       " 'patent',\n",
       " 'infirmier',\n",
       " 'équiper',\n",
       " 'pourlesquelles',\n",
       " 'practically',\n",
       " 'traitement',\n",
       " 'contribue',\n",
       " '000',\n",
       " 'dépasse',\n",
       " 'expressivité',\n",
       " 'cascade',\n",
       " 'créés',\n",
       " 'simplifiés',\n",
       " 'useful',\n",
       " 'actionsà',\n",
       " 'centaine',\n",
       " 'evolve',\n",
       " 'ssc',\n",
       " 'connected',\n",
       " 'agroalimentaire',\n",
       " 'adaptative',\n",
       " 'contrary',\n",
       " 'map',\n",
       " 'vallée',\n",
       " 'sécurisé.notre',\n",
       " 'semi-interactif',\n",
       " 'risque',\n",
       " 'auxquelsnous',\n",
       " 'laticielles',\n",
       " 'mapping',\n",
       " 'surles',\n",
       " 'sontmis',\n",
       " 'enidentifiant',\n",
       " 'composer',\n",
       " 'augmenter',\n",
       " 'temporelleoutskewer',\n",
       " 'marin',\n",
       " 'applied',\n",
       " 'side',\n",
       " 'manual',\n",
       " 'lesconnaissances',\n",
       " 'viennent',\n",
       " 'ibtracs',\n",
       " 'prévention',\n",
       " 'musicaux',\n",
       " 'exécutés',\n",
       " 'brut',\n",
       " 'logically',\n",
       " 'characterization',\n",
       " 'contrainte',\n",
       " 'microbiologie',\n",
       " 'résume',\n",
       " 'fendu',\n",
       " \"s'inscrivent\",\n",
       " 'extension',\n",
       " 'approximer',\n",
       " 'dernier',\n",
       " 'manufacturés',\n",
       " 'attendions',\n",
       " 'vérifier',\n",
       " 'confondue',\n",
       " 'denombreuses',\n",
       " 'anomaly',\n",
       " 'respect',\n",
       " 'étiquetées',\n",
       " 'interdépendants',\n",
       " 'efficace',\n",
       " 'vrac',\n",
       " 'inductives',\n",
       " 'ignorées',\n",
       " 'utilisé',\n",
       " 'mobility',\n",
       " 'social',\n",
       " 'bruit',\n",
       " 'ald',\n",
       " 'evolving',\n",
       " 'luxembourg',\n",
       " 'exploration-exploitation',\n",
       " 'friedman',\n",
       " 'étape',\n",
       " 'appliqué',\n",
       " 'servi',\n",
       " 'défitechnologique',\n",
       " 'empruntés',\n",
       " 'recently',\n",
       " 'bâtir',\n",
       " 'antenne',\n",
       " 'caractérisé',\n",
       " 'portefeuille',\n",
       " 'demande',\n",
       " 'maintenue',\n",
       " 'equally',\n",
       " 'répartis',\n",
       " 'grandissant',\n",
       " 'bande',\n",
       " 'associée',\n",
       " 'increases',\n",
       " 'participation',\n",
       " 'scalabilité',\n",
       " 'surmonter',\n",
       " \"puisqu'elle\",\n",
       " 'utilisésdans',\n",
       " 'résponses',\n",
       " 'élaborons',\n",
       " 'échouent',\n",
       " 'simplification',\n",
       " 'mécanisme',\n",
       " '+|v',\n",
       " 'conjonctive',\n",
       " 'rôle',\n",
       " 'whose',\n",
       " 'recherchant',\n",
       " 'reçu',\n",
       " 'port',\n",
       " 'mln',\n",
       " \"d'optimalité\",\n",
       " 'quantifions',\n",
       " 'paramètre',\n",
       " 'démantique',\n",
       " 'unereprésentation',\n",
       " 'initiate',\n",
       " 'interprétables',\n",
       " 'théâtre',\n",
       " 'visdans',\n",
       " 'épistémiques',\n",
       " 'details',\n",
       " 'externe',\n",
       " 'replicated',\n",
       " \"d'atomes\",\n",
       " 'desuivi',\n",
       " 'inventing',\n",
       " 'matérialisées',\n",
       " 'method',\n",
       " 'holistic',\n",
       " 'artificielle',\n",
       " 'comportementsdes',\n",
       " 'linear',\n",
       " 'femme',\n",
       " 'capacité',\n",
       " 'dis-',\n",
       " 'common',\n",
       " 'molecular',\n",
       " \"priorilorsqu'elles\",\n",
       " 'schémasxml',\n",
       " 'smarter',\n",
       " 'problème.dans',\n",
       " 'fonctionnelle',\n",
       " 'probabiliste',\n",
       " 'retreival',\n",
       " 'peculiarity',\n",
       " 'akin',\n",
       " \"découverted'associations\",\n",
       " 'intérêt',\n",
       " \"l'approche\",\n",
       " 'redirection',\n",
       " 'contextualisation',\n",
       " 'matière',\n",
       " 'thematically',\n",
       " 'intended',\n",
       " 'multinomiales',\n",
       " 'dediagnostic',\n",
       " 'sert',\n",
       " 'laclassification',\n",
       " 'mean',\n",
       " 'transaction',\n",
       " 'préservées',\n",
       " 'cenouvel',\n",
       " 'implémente',\n",
       " 'capables',\n",
       " 'has',\n",
       " 'analogique',\n",
       " 'agrègent',\n",
       " 'indicateur',\n",
       " 'multi-agents',\n",
       " 'conventionnel',\n",
       " 'denotre',\n",
       " 'raison',\n",
       " 'explosion',\n",
       " 'régularité',\n",
       " 'generally',\n",
       " 'least-squares',\n",
       " 'objects',\n",
       " '2,4',\n",
       " 'adn',\n",
       " 'climatiques',\n",
       " 'convexité',\n",
       " 'estimées',\n",
       " 'exceed',\n",
       " 'multidimensionnalité',\n",
       " 'visités',\n",
       " 'fayyad-irani',\n",
       " 'hints',\n",
       " 'étendent',\n",
       " 'hoc',\n",
       " 'algorithmede',\n",
       " 'relationnelle',\n",
       " 'fréquemment',\n",
       " 'fragmentation',\n",
       " 'articulé',\n",
       " 'insuch',\n",
       " '{',\n",
       " 'contextual',\n",
       " 'classify',\n",
       " 'correcting',\n",
       " 'inférés',\n",
       " 'séparabilité',\n",
       " 'concernés',\n",
       " 'excessivement',\n",
       " \"d'expérience\",\n",
       " 'draix',\n",
       " 'gigantesque',\n",
       " 'cliniciens',\n",
       " 'décrits',\n",
       " 'computers',\n",
       " 'récolter',\n",
       " 'syr',\n",
       " 'vue',\n",
       " 'flickr',\n",
       " \"l'apprenant\",\n",
       " 'rigoureuses',\n",
       " 'existes',\n",
       " 'disséminés',\n",
       " 'quiutilise',\n",
       " 'state-of-the-artmethods',\n",
       " 'quasiéquivalencesà',\n",
       " 'offices',\n",
       " 'régression',\n",
       " 'réseau.cette',\n",
       " '109',\n",
       " 'découvrir',\n",
       " 'requête',\n",
       " 'sélectionnant',\n",
       " 'allows',\n",
       " 'compléter',\n",
       " \"l'enquête\",\n",
       " 'prononciation',\n",
       " 'nousprésentons',\n",
       " \"s'appuyer\",\n",
       " 'assignent',\n",
       " 'merger',\n",
       " 'sensor',\n",
       " 'redonner',\n",
       " 'exposons',\n",
       " 'opposition',\n",
       " 'morik2008',\n",
       " 'adopted',\n",
       " 'approchestochastique',\n",
       " 'répétées',\n",
       " 'estimons',\n",
       " 'crowdsourcing',\n",
       " 'certainly',\n",
       " 'satellitales',\n",
       " 'intégralité',\n",
       " 'sensory',\n",
       " 'spatiotemporeldéfini',\n",
       " 'default',\n",
       " 'production',\n",
       " 'afc',\n",
       " 'begun',\n",
       " 'automate',\n",
       " 'récupération',\n",
       " 'impact',\n",
       " 'restreindre',\n",
       " 'stocke',\n",
       " 'characters',\n",
       " 'métro',\n",
       " 'induction',\n",
       " 'weka',\n",
       " \"l'essentiel\",\n",
       " 'jouent',\n",
       " 'socio-semantic',\n",
       " 'habituel',\n",
       " 'occurrence',\n",
       " 'cycliques',\n",
       " 'hydroécologiques',\n",
       " 'inter-domaines',\n",
       " 'intégrative',\n",
       " 'victime',\n",
       " 'expressif',\n",
       " 'aménagement',\n",
       " 'continuously',\n",
       " 'rewriting',\n",
       " 'ob-tenir',\n",
       " 'private',\n",
       " 'combinant',\n",
       " 'desirable',\n",
       " 'simulés',\n",
       " 'automatique',\n",
       " 'aminés',\n",
       " 'massivement',\n",
       " 'program',\n",
       " 'ii',\n",
       " 'formidable',\n",
       " 'shapes',\n",
       " 'andboat',\n",
       " 'explored',\n",
       " 'desdescriptions',\n",
       " 'bernoulli',\n",
       " 'agréger',\n",
       " 'intégré',\n",
       " '}',\n",
       " 'sport',\n",
       " 'each',\n",
       " '12.5',\n",
       " 'phits',\n",
       " 'hog',\n",
       " 'principe',\n",
       " 'actionnable',\n",
       " 'électrique',\n",
       " 'spécialisation/généralisation',\n",
       " '512',\n",
       " '70000',\n",
       " 'smiley',\n",
       " 'découvert',\n",
       " 'potentiellementutiles',\n",
       " 'polar',\n",
       " 'present',\n",
       " 'factorisation',\n",
       " 'discriminante',\n",
       " 'dédiées',\n",
       " 'lexico-scientométrique',\n",
       " 'attachement',\n",
       " 'terrier',\n",
       " '244',\n",
       " \"d'opérateurs\",\n",
       " 'biomimétique',\n",
       " 'yahoo',\n",
       " 'library',\n",
       " 'transformé',\n",
       " 'rim',\n",
       " 'triadiques',\n",
       " \"l'agronomie\",\n",
       " 'riche',\n",
       " 'équipé',\n",
       " 'obtention',\n",
       " 'écriture',\n",
       " 'classic',\n",
       " 'pcfg',\n",
       " 'dysfonctionnement',\n",
       " 'évolué',\n",
       " 'theirnumber',\n",
       " 'personnelles',\n",
       " 'totallyfuzzy',\n",
       " 'clarté',\n",
       " 'exceptional',\n",
       " 'expiration',\n",
       " 'computerscience',\n",
       " 'fidèle',\n",
       " 'focus',\n",
       " 'protéine-arn',\n",
       " 'syllabique',\n",
       " 'accident',\n",
       " \"proposonsd'extraire\",\n",
       " 'noir',\n",
       " 'sous-graphes',\n",
       " 'wikis',\n",
       " 'dimensionnement',\n",
       " 'batch',\n",
       " 'euclidiennes',\n",
       " 'folksonomie',\n",
       " 'consistent',\n",
       " '19x19',\n",
       " 'nouvelleapproche',\n",
       " 'pervade',\n",
       " 'reçoive',\n",
       " 'scénario',\n",
       " 'unable',\n",
       " 'pondéré',\n",
       " 'known',\n",
       " 'compétition',\n",
       " 'conceptuelle',\n",
       " 'nodaux',\n",
       " 'prune',\n",
       " 'directional',\n",
       " 'desméthodes',\n",
       " 'planète',\n",
       " 'use',\n",
       " 'primitif',\n",
       " 'guidée',\n",
       " 'garder',\n",
       " 'durance',\n",
       " 'multi-tables',\n",
       " 'danger',\n",
       " 'biological',\n",
       " 'reuters-21578',\n",
       " 'probables',\n",
       " 'attribué',\n",
       " 'wide-spread',\n",
       " 'émergents',\n",
       " 'measuring',\n",
       " 'privacy-aware',\n",
       " \"s'interroger\",\n",
       " 'ensembliste',\n",
       " 'désaccord',\n",
       " 'vient',\n",
       " 'généralisation',\n",
       " 'few',\n",
       " 'decontenu',\n",
       " 'nautilus',\n",
       " 'obtenir',\n",
       " 'obsolescence',\n",
       " 'newest',\n",
       " 'océan',\n",
       " 'codage',\n",
       " 'bioprocédés',\n",
       " 'ranking',\n",
       " 'intrinsèquement',\n",
       " 'suicidant',\n",
       " 'pertinante',\n",
       " 'adjacence',\n",
       " \"d'outlier\",\n",
       " 'générique',\n",
       " 'accèssimple',\n",
       " 'boisé',\n",
       " \"faciliterl'analyse\",\n",
       " 'constituer',\n",
       " 'lod',\n",
       " 'ratp',\n",
       " 'quelques-unes',\n",
       " \"egc'2007\",\n",
       " 'cellulaires',\n",
       " 'protection',\n",
       " 'donnent',\n",
       " 'written',\n",
       " 'debases',\n",
       " 'personnage',\n",
       " 'support',\n",
       " 'cameroun',\n",
       " 'pièce',\n",
       " 'outskewer',\n",
       " 'intensive',\n",
       " 'technologiques',\n",
       " 'commentée',\n",
       " 'offrant',\n",
       " 'freely',\n",
       " 'illustrate',\n",
       " 'ranking-loss',\n",
       " 'mab+lp',\n",
       " 'interactively',\n",
       " 'lake',\n",
       " 'liées',\n",
       " 'presque',\n",
       " 'estimateur',\n",
       " 'revenir',\n",
       " 'chat',\n",
       " 'k-faiblement',\n",
       " 'trajectories',\n",
       " 'rapc',\n",
       " 'transformée',\n",
       " 'intertwinning',\n",
       " 'confusion',\n",
       " 'constatons',\n",
       " 'teximus',\n",
       " 'robustes',\n",
       " 'inclure',\n",
       " 'introduces',\n",
       " \"s'agit\",\n",
       " 'control',\n",
       " 'infer',\n",
       " 'avonspu',\n",
       " 'peering',\n",
       " 'holds',\n",
       " 'instead',\n",
       " 'dendrogrammes',\n",
       " 'audio',\n",
       " 'bipartis',\n",
       " 'biologiques',\n",
       " \"d'améliorer\",\n",
       " 'acronyme',\n",
       " 'cosinus',\n",
       " 'activitéde',\n",
       " 'optimisant',\n",
       " 'tableau',\n",
       " 'travail',\n",
       " 'icm',\n",
       " 'segmenter',\n",
       " 'interventionand',\n",
       " 'éristique',\n",
       " 'element',\n",
       " 'ids',\n",
       " \"l'information\",\n",
       " 'bonsrésultats',\n",
       " 'robotisé',\n",
       " 'whenever',\n",
       " 'defence',\n",
       " 'performants',\n",
       " 'pressant',\n",
       " 'provider',\n",
       " 'bn',\n",
       " \"d'afc\",\n",
       " 'construisant',\n",
       " 'considérablement',\n",
       " 'estcapable',\n",
       " 'computing',\n",
       " 'déplacementvoire',\n",
       " 'hernie',\n",
       " 'tdag',\n",
       " 'ciblés',\n",
       " 'opportun',\n",
       " 'alignement',\n",
       " 'distributional',\n",
       " 'desanalyses',\n",
       " 'spécifiant',\n",
       " 'guidépar',\n",
       " \"problèmes'étend\",\n",
       " 'enfonction',\n",
       " 'génétique',\n",
       " 'aisée',\n",
       " 'dfc',\n",
       " 'discover',\n",
       " 'locuteur',\n",
       " 'connaissant',\n",
       " 'decides',\n",
       " 'processus',\n",
       " 'disposons',\n",
       " 'surface',\n",
       " 'svms',\n",
       " 'semi-conducteur',\n",
       " 'obtenussont',\n",
       " 'réduites',\n",
       " 'préfèrent',\n",
       " 'naviguer',\n",
       " 'originelles',\n",
       " 'nextclosure',\n",
       " 'artificielles',\n",
       " 'effectuons',\n",
       " 'p-boxes',\n",
       " 'souventen',\n",
       " 'pre-serve',\n",
       " 'areemerging',\n",
       " 'critèresdéfinis',\n",
       " 'broadcasters',\n",
       " 'routier',\n",
       " 'montagne',\n",
       " 'nomenclature',\n",
       " 'réussir',\n",
       " 'couleur',\n",
       " 'appréhender',\n",
       " 'maîtriser',\n",
       " 'généralisable',\n",
       " 'put',\n",
       " 'uncover',\n",
       " 'disparité',\n",
       " '69,9',\n",
       " \"s'intersecter\",\n",
       " 'relate',\n",
       " 'biotechnologie',\n",
       " 'traduire',\n",
       " 'ipédagogique',\n",
       " 'scénarii',\n",
       " 'caractéristique',\n",
       " 'analysait',\n",
       " 'given',\n",
       " 'mouvance',\n",
       " 'graduel',\n",
       " 'blob',\n",
       " 'traitant',\n",
       " 'survenus',\n",
       " 'acteur',\n",
       " 'foundations',\n",
       " 'dérivable',\n",
       " 'déduisons',\n",
       " 'pulvériser',\n",
       " 'arrive',\n",
       " 'chimiste',\n",
       " 'clusters',\n",
       " 'conference',\n",
       " 'sms',\n",
       " 'detail',\n",
       " 'nombreusessémantiques',\n",
       " 'utilisation',\n",
       " \"d'ontologies\",\n",
       " 'ignorent',\n",
       " 'post-génomique',\n",
       " 'nistér-stewénius',\n",
       " 'longue',\n",
       " 'non-contradiction',\n",
       " 'modeleur',\n",
       " 'facebook',\n",
       " 'simultaneous',\n",
       " \"s'appuie\",\n",
       " '2018',\n",
       " 'fairemontre',\n",
       " 'médiocre',\n",
       " \"egc'2017\",\n",
       " 'tf-idf',\n",
       " 'bayésien',\n",
       " \"détectiond'individus\",\n",
       " 'bi-bootstrap',\n",
       " 'performant',\n",
       " 'joining',\n",
       " 'philosophie',\n",
       " 'sécurisée',\n",
       " 'paire',\n",
       " 'boîte',\n",
       " 'approchede',\n",
       " 'gestuelle',\n",
       " 'destructuration',\n",
       " 'clivage',\n",
       " 'ciblage',\n",
       " 'ex',\n",
       " 'nasopharynx',\n",
       " \"lorsqu'une\",\n",
       " 'figurant',\n",
       " 'rapide',\n",
       " 'lumière',\n",
       " 'playgrounds',\n",
       " 'vis-à-vis',\n",
       " 'justifions',\n",
       " 'categories',\n",
       " 'compresser',\n",
       " 'falsified',\n",
       " 'biomédicale',\n",
       " 'routine',\n",
       " 'structurelles',\n",
       " 'ipums',\n",
       " 'avéré',\n",
       " 'envi-ronnements',\n",
       " 'méta-modèle',\n",
       " 'explicatives',\n",
       " 'transductive',\n",
       " 'accéder',\n",
       " 'cybercriminels',\n",
       " 'exploiterla',\n",
       " 'méthodologique',\n",
       " \"s'appuientsur\",\n",
       " 'proportion',\n",
       " 'straightforwardly',\n",
       " 'n-1',\n",
       " 'correspondant',\n",
       " 'rapidité',\n",
       " 'composition',\n",
       " 'arcelor',\n",
       " 'invention',\n",
       " 'univariée',\n",
       " 'multimedia',\n",
       " 'programmerdes',\n",
       " 'déclarées',\n",
       " 'gérer',\n",
       " 'générale',\n",
       " 'récursives',\n",
       " 'partitionnée',\n",
       " 'résultante',\n",
       " 're-ordering',\n",
       " 'pathologie',\n",
       " 'personnalité',\n",
       " 'espérant',\n",
       " 'champagne',\n",
       " 'conviviale',\n",
       " 'semi-automatiquement',\n",
       " 'analyseur',\n",
       " 'graal',\n",
       " 'go',\n",
       " 'meta-learning',\n",
       " 'ensembleplus',\n",
       " 'zoomer',\n",
       " 'initiative',\n",
       " 'voiture',\n",
       " 'apprenant',\n",
       " 'abordées',\n",
       " 'intéressé',\n",
       " 'propre',\n",
       " 'lèvre',\n",
       " 'totalement',\n",
       " 'syntaxe',\n",
       " 'exclusively',\n",
       " 'virtuels',\n",
       " \"n'offrent\",\n",
       " 'cliquer',\n",
       " 'illustré',\n",
       " 'classique',\n",
       " 'populaires',\n",
       " 'dehiérarchie',\n",
       " 'perspective',\n",
       " 'nature',\n",
       " 'information.we',\n",
       " 'renforcementpour',\n",
       " 'traduit',\n",
       " 'segmentation/indexation',\n",
       " 'sherman-morrison-woodbury',\n",
       " 'négatif',\n",
       " 'expliquons',\n",
       " 'contraints',\n",
       " '×',\n",
       " 'age',\n",
       " '230',\n",
       " 'tellesque',\n",
       " 'suzuki',\n",
       " 'possibleles',\n",
       " 'pénalisé',\n",
       " 'ralentissement',\n",
       " 'permettent',\n",
       " 'virtuel',\n",
       " \"deft'06\",\n",
       " 'compréhension',\n",
       " 'acceptant',\n",
       " 'false',\n",
       " 'italy',\n",
       " 'désambiguïser',\n",
       " 'multi-échelles',\n",
       " 'modern',\n",
       " 'évaluationsdu',\n",
       " 'capitalisé',\n",
       " 'four',\n",
       " 'surprise',\n",
       " 'continu',\n",
       " 'osf',\n",
       " 'comblerce',\n",
       " 'outperforms',\n",
       " 'neretenir',\n",
       " 'stationnaire',\n",
       " 'associated',\n",
       " 'inspiré',\n",
       " 'csps',\n",
       " 'perdre',\n",
       " 'réserve',\n",
       " 'personnalisant',\n",
       " 'leréseau',\n",
       " 'press',\n",
       " 'spécification',\n",
       " 'façcon',\n",
       " 'computation',\n",
       " 'euroscientia',\n",
       " 'robot.notre',\n",
       " 'consécutive',\n",
       " 'reliées',\n",
       " 'skylines',\n",
       " 'prior',\n",
       " ...]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dico"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On pourra réduire le vocabulaire en retirant des mots avec peu ou pas d'importance.\n",
    "\n",
    "Ce nettoyage se passe dans le fichier `clean.txt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul du Tf.\n",
    "tf = []\n",
    "for x in range(len(resume)):\n",
    "    tf.append([])\n",
    "    for y in range(len(resume[x])):\n",
    "        tf[x].append(resume[x].count(resume[x][y]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention : l'étape suivante est longue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_time = time.process_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul du Df.\n",
    "df = {}\n",
    "for x in range(len(dico)):\n",
    "    sum = 0\n",
    "    for y in range(len(resume)):\n",
    "        if dico[x] in resume[y]:\n",
    "            sum = sum + 1\n",
    "    df[dico[x]] = sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 119.78 secondes ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- %s secondes ---\" % round(time.process_time() - df_time, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcul de l'Idf\n",
    "idf = {}\n",
    "for word in df:\n",
    "    idf[word] = np.log10(len(resume)/df[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1269"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calcul du Tf-Idf.\n",
    "tfidf = []\n",
    "for x in range(len(resume)):\n",
    "    tfidf.append({})\n",
    "    for y in range(len(resume[x])):\n",
    "        tfidf[x][resume[x][y]] = tf[x][y] * idf[resume[x][y]]\n",
    "len(tfidf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il faut trier les Tf-Idf par valeur et récupérer les 5 meilleurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "best = []\n",
    "for item in tfidf:\n",
    "    topic = \"\"\n",
    "    trier = sorted(item.items(), key=lambda kv: kv[1])\n",
    "    trier.reverse()\n",
    "    if len(item)<5:\n",
    "        top = len(item)\n",
    "    else:\n",
    "        top = 5\n",
    "    for i in range(top):\n",
    "        topic = topic + trier[i][0] + \" \"\n",
    "    best.append(topic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['idéo2017 citoyen politiques tweets candidat ',\n",
       " 'co-clustering modl gagne taille passent ',\n",
       " 'recommandation mobilité factorisation inféré visités ',\n",
       " 'arabe sentiment commentaire marocain dialectal ',\n",
       " 'cds résumé texte composant rôle ',\n",
       " 'big scénario ontologique data contexte ',\n",
       " 'tentative décès supervisées patient risque ',\n",
       " 'formés majoritaire vote global obtenus ',\n",
       " 'label heureux musique morceau co-occurrence ',\n",
       " 'a/b test procédure stratification contextuel ',\n",
       " 'dynamics power understanding big human ',\n",
       " 'incrémentales clustering som collaboratif auto-organisatrices ',\n",
       " 'mot-clé catégorisation scientifique relation sémantique ',\n",
       " 'cardiaque similarité quasi-arithmétiques moyenne type ',\n",
       " 'community complex network in structure ',\n",
       " 'importance noeud réseau influe différencie ',\n",
       " 'similarité campagne sémantique vectorielles complémentarité ',\n",
       " \"temps-réel l'architecture oblige pertinence démontrées \",\n",
       " 'complétude prescriptives contrainte commande vérifier ',\n",
       " 'treillis distributif fca concept médians ',\n",
       " 'gpo séquence ordonnés graduel partiellement ',\n",
       " 'dbpédia catégorie apparentés lacune encode ',\n",
       " 'spark temps-réel singularité basés combinaison ',\n",
       " 'motif échantillonnage norme aléatoire séquentiels ',\n",
       " 'multi-couches edoi itérative exploration graphe ',\n",
       " 'elaboration poster géotechnique élaboration domaine ',\n",
       " 'lien wikipédia inter-langues édition erronés ',\n",
       " 'métier antécédent recrutent e-recrutement sociaux ',\n",
       " 'thématique scientifique sémantique corpus fusionnons ',\n",
       " 'influenceurs centralité comparative détection évaluation ',\n",
       " 'météorologiques journée série mesurées ville ',\n",
       " 'chemin information propagation trajectoire retrouver ',\n",
       " 'défaillance événement compteur cause sous-tendant ',\n",
       " 'temporalité graduel fermé fréquents contrainte ',\n",
       " 'événement négatif motif ntgsp électricité ',\n",
       " 'document interrogation schéma orientés dictionnaire ',\n",
       " 'apprenant leader ” profil recommandation ',\n",
       " 'recommandation hôtel contextuelles distinctives volatilité ',\n",
       " 'ontobiotope vie microbiens agriculture microorganismes ',\n",
       " 'long-range influence network social in ',\n",
       " 'bord personnalité média communauté suivi ',\n",
       " 'nnms mean-shift k-plus scalable voisin ',\n",
       " 'enquête ordinale méta-analyse ordre opinion ',\n",
       " 'approximatifs déséquilibré incrémental décision mai2p ',\n",
       " 'microblogs localisation extraire apprentissage méthode ',\n",
       " 'métadonnées vault physique data stockage ',\n",
       " 'blockchain notarisation nfb protocole document ',\n",
       " 'approximatifs massives théorie caractéristique sélection ',\n",
       " 'palm espace treillis parallèle branche ',\n",
       " 'peerus scientifique review expert entrainé ',\n",
       " 'capacitaire planning perforecast tôt service ',\n",
       " 'solaire rayonnement prédiction météorologiques régression ',\n",
       " 'inventaire thr satellitaires image terrain ',\n",
       " 'performante indexation prise compte document ',\n",
       " 'financement campagne participatif prédiction succès ',\n",
       " \"qu'est-ce apprentissage machine évolué question \",\n",
       " 'personnalisées réponse mot-clé fournir raison ',\n",
       " 'manuscrit chancellerie registre historique transcription ',\n",
       " 'linéaire shift distribution variées dataset ',\n",
       " 'assainissement date conduite pose reconstitution ',\n",
       " 'âgées chute information bayésien risque ',\n",
       " 'artificielle voir dela vision savoir ',\n",
       " 'neuro-imagerie réaliste observation réutilisation cliniciens ',\n",
       " 'verbalisation fait temporel formalisation indice ',\n",
       " 'bayésien modèle co-clustering mixtes critère ',\n",
       " 'recommandation actualité personnalisées hybride échelle ',\n",
       " 'carreau désagrégation population échelle bâtiment ',\n",
       " \"unitex/gramlab grammaire s'étend marne-la-vallée paris-est \",\n",
       " 'universal-endpoint.com plateforme web sparql accès ',\n",
       " 'thématique nouveauté émergentes observer scénario ',\n",
       " 'cardiovasculaires maladie facteur risque interaction ',\n",
       " 'textuel lignée su influencers hybrid ',\n",
       " 'image satellitaires série temporelles évolution ',\n",
       " 'sujet biclustering investigation variante textuels ',\n",
       " 'table anonymiser publiées coclustering individu ',\n",
       " 'binarisées variable coclustering individu party ',\n",
       " \"item zone intermédiaire valider l'application \",\n",
       " 'structuration multi-instance apprendre cadre apprentissage ',\n",
       " 'crf définissent séquentielles appel relation ',\n",
       " 'flux parallélisme opérateur traitement degré ',\n",
       " 'méta evaluation cadre analyse donnée ',\n",
       " 'cah formule ascendant part noyau ',\n",
       " 'sous-parties discriminantes aléatoire 3d objet ',\n",
       " 'label relation apprendre cycliques ignorer ',\n",
       " 'dt-cwt transform isar phase reconnaissance ',\n",
       " 'co-clustering mixtes table continues bloc ',\n",
       " 'treillis similarité concept mesure basées ',\n",
       " 'a/b généraliste conception test évaluation ',\n",
       " 'carlo monte bs sd exploration ',\n",
       " 'city captured cities and how ',\n",
       " 'précision/rappel cost-sensitive ville maladie défi ',\n",
       " 'motif utilisateur préféré implicitement transaction ',\n",
       " 'multi-modales faux sociaux détection réseau ',\n",
       " 'ratings user-user recommendation filtering enhanced ',\n",
       " 'communauté stabilité représentatives période temporelle ',\n",
       " 'normalité singularité expression langage sélection ',\n",
       " \"paysage l'image enjeu d'automatisation alliant \",\n",
       " 'chronique discriminantes motif discriminant séquence ',\n",
       " 'entité relation événement lexico-sémantique rachetées ',\n",
       " 'récurrentes attribué graphe noeud figés ',\n",
       " 'chaînage entretien supervisés histoire science ',\n",
       " '3d géométriques propriété raisonnement assemblage ',\n",
       " 'archive télévisés face2graph 15 visage ',\n",
       " 'témoignage saisi rédigeant saisie préserver ',\n",
       " 'format rdf sparql non-rdf ii ',\n",
       " 'waves dénotée potable souterrain décuplées ',\n",
       " 'rdf format web service objet ',\n",
       " 'k-sc centroïd k-spectral translation barycentre ',\n",
       " 'apps android through text features ',\n",
       " 'could for knowledge learning ontology ',\n",
       " \"confiance source navire information d'information \",\n",
       " 'treillis correspondance resp attribut objet ',\n",
       " \"0-subsomption pli d'acteurs subsomption passent \",\n",
       " 'nosql colonne orienté entrepôt hbase ',\n",
       " 'pharmacovigilance web social fondé sémantique ',\n",
       " 'porgy pilotés rewriting port modelling ',\n",
       " 'uni-label numéro multi-label défi tâche ',\n",
       " 'financement campagne montant participatif levé ',\n",
       " 'ferré voyageur prévision bayésiens flux ',\n",
       " 'client profil aide prototype crm ',\n",
       " 'recommandation folksonomie filtrage préférence basées ',\n",
       " 'décision justice crf hmm loi ',\n",
       " \"ciblée d'images descripteur visuel sélection \",\n",
       " 'multi-label prétraiter label sélection learning ',\n",
       " 'subspace flux clustering identifer visualisant ',\n",
       " 'réseau lien conceptuels clusters évolution ',\n",
       " ': personnalisés rdf/sparql type xsd ',\n",
       " 'télédétection référence réalignement préentons venant ',\n",
       " 'prédictives k-moyennes modifiée compromis mesurer ',\n",
       " 'graphe communautaire générateur réseau dynamique ',\n",
       " 'lbsn recommandation poisson factorisation point ',\n",
       " 'graduel motif groupement variation temporel ',\n",
       " 'neutralité personnalité innovante diffusion compréhension ',\n",
       " 'règle association étape savoir fouille ',\n",
       " 'société calcul numériques vie conférence ',\n",
       " 'crowdsourcing expertise participant campagne degré ',\n",
       " 'rôle communautaires centralité communauté mesure ',\n",
       " 'opinion twitter plateforme recommandation réel ',\n",
       " 'veille re-watch thème document web ',\n",
       " 'transformé échantillonnage flux donnée ',\n",
       " 'nell instance français anglais chaîne ',\n",
       " 'label multilabel ... court interactif ',\n",
       " 'user engagement query system search ',\n",
       " 'passage question reclassement sqr retourner ',\n",
       " 'mappings soc évolution adaptation biomédical ',\n",
       " 'users survey published have that ',\n",
       " 'variable coviz partitionnées khiops catégorielles ',\n",
       " 'publication egc conférence 3 méthodologie ',\n",
       " 'édition thématique publication conférence intéressé ',\n",
       " 'electricity long provider period weron ',\n",
       " 'each neighborhood-based we stream group ',\n",
       " 'model training tree method stream ',\n",
       " 'agroalimentaire – acteur décision découlant ',\n",
       " 'to more learning and in ',\n",
       " 'désambiguïsation catégorisation renseignent individu wikipédia ',\n",
       " 'clustering préférence distance attribut utilisateur ',\n",
       " 'semi-interactif purement visuel extrait accepté ',\n",
       " 'discovery metabolomic biomarkers métabolomiques methodologies ',\n",
       " 'suicide parole drift monitorées mental ',\n",
       " \"image l'arbre exploration visuel construite \",\n",
       " '2-colorability verification chromatic hypergraphs hypergraph ',\n",
       " 'label doublon trace binaires treillis ',\n",
       " 'anomalie trace unitaire motif collection ',\n",
       " 'thématique egc mot partageant topic ',\n",
       " 'collaboration egc tendance réseau conceptuelles ',\n",
       " 'fpof motif fréquents aberrantes approchée ',\n",
       " 'message system and ais falsified ',\n",
       " 'multidimensionnel hiérarchique hiérarchie factuelles rolap ',\n",
       " 'groupe centralité collaboration scientifique thématique ',\n",
       " '2016 egc logique défi exploration ',\n",
       " 'c-sparql flux rdf extension sparql ',\n",
       " 'affixe chimiques nommées entité reconnaissance ',\n",
       " 'clé liage publiées candidat peut-être ',\n",
       " 'commentaire commentsminer extraction publiquement 84 ',\n",
       " 'pervasifs treillis formelle smartphone généraliser ',\n",
       " 'factory network social overloading sub-typing ',\n",
       " 'in data and category fairness-aware ',\n",
       " 'multistratégie fodomust plateforme interface dynamictimewarping ',\n",
       " 'encodage programmation asp motif séquentiels ',\n",
       " \"duplicats redondantes d'argumentation transparent figurant \",\n",
       " 'contrainte clustering intervention satellite expert ',\n",
       " 'semantic relation in rich divided ',\n",
       " 'sms sentiment new social explosive ',\n",
       " \"recommandation géographique lbsn ' factorisation \",\n",
       " 'multi-tables variable khiops million table ',\n",
       " 'po-motifs sous-séquences arc relationnelle concept ',\n",
       " 'summaries real time seems chorems ',\n",
       " 'mi pattern to sequential patient ',\n",
       " 'révolution captation implique assurance opportunité ',\n",
       " 'and project learning mtr sop ',\n",
       " 'protocole stockage libre cohérence cassandra ',\n",
       " 'hospitalier myocarde infarctus pmsi soumis ',\n",
       " 'their semantics may to data ',\n",
       " \"clowdflows relationnelles fouille d'exécuter venons \",\n",
       " 'forum réputation vote santé message ',\n",
       " 'latent spectral arbitraire espace transformation ',\n",
       " 'recommandation tags persorec ressource utilisateur ',\n",
       " 'plongement isométrique métrique lch chodorow ',\n",
       " 'plateforme collaboratives qualité noeud relation ',\n",
       " 'hyperplan parallèle point phase commune ',\n",
       " 'logistique régression image parallèle échelle ',\n",
       " 'skyline retournés pertinencede relaxéest preferred ',\n",
       " 'requête négatif discriminantes exploration exemple ',\n",
       " \"géographiques spatiales régissant pourextraire d'associations \",\n",
       " 'sarem architecture architectureextraction softwarearchitecture specifies ',\n",
       " 'comportementale action client segmentation adapteroffre ',\n",
       " 'sous-ensemble topologique sélection discrimination induite ',\n",
       " 'slider to reasoner more knowledge ',\n",
       " 'haie markov hilbert courbe densité ',\n",
       " 'moving objects done trip andboat ',\n",
       " 'tom topic and modeling browsing ',\n",
       " 'mettons egc auteur thématique décrivonsune ',\n",
       " 'constraint programming generic solvers settings ',\n",
       " 'transmute trace interactif motif àse ',\n",
       " 'cosc télédétection collaboratif segmentation thématique ',\n",
       " 'egc 2016 thème défi lorsdes ',\n",
       " 'desdonnées propriété textuelles représentationsphonologiques unicode ',\n",
       " 'lexico-scientométrique egc 2016 défi websous-tendu ',\n",
       " 'défaillance détection système robotisé estimées ',\n",
       " 'lod définition étiquetage raisonnement ontologie ',\n",
       " 'data both and knowledge t-box ',\n",
       " 'agrégation réduction dimensionnalité qualitatives préférence ',\n",
       " 'phrase sémantique similarité noyau indicateur ',\n",
       " 'mobilité trace contextualisés individu motif ',\n",
       " 'k-moyennes centre vise standard kmoyennes ',\n",
       " 'viewpoints overlaps bi-cluster variant extract ',\n",
       " 'carte & cd7online utilisateur cartographie ',\n",
       " '/owl schema rdf to data ',\n",
       " 'segmentation annotation maillage objet étape ',\n",
       " 'paramètre recherche modifiant rungeneration terrier ',\n",
       " 'opinion veille amiei analyse plateforme ',\n",
       " 'blog olap projet billet véhiculées ',\n",
       " 'cybercriminalité mondiale déplaisant interconnecter mondialisation ',\n",
       " 'bilingual multilingual document and collection ',\n",
       " 'advantage to consider window sequential ',\n",
       " 'data will explore and decision ',\n",
       " 'big data research is it ',\n",
       " 'and to — help cities ',\n",
       " 'proximité topologique mesure discriminante discrimination ',\n",
       " 'secm évidentielle croyance fonction schéma ',\n",
       " 'multi-label centrée économique raisonnement indexation ',\n",
       " 'g-stream flux clustering topologique gas ',\n",
       " 'base – rdf jour maintien ',\n",
       " 'critère ecart partition modularité graphe ',\n",
       " 'précision-rappel compromis performance espace fonction ',\n",
       " 'skyline calcul espace candidat réduction ',\n",
       " \"d113 flux '' `` centrerons \",\n",
       " 'analogiques proportion analogique paire n-uplets ',\n",
       " 'reformulation plagiat mot détection détecter ',\n",
       " 'détection auteur plagiat passage regroupement ',\n",
       " 'risque chimique alimentaire confondue substance ',\n",
       " 'stream shedding load data processing ',\n",
       " 'collection function is proportional parameter ',\n",
       " 'pondérés chemin condensé complète motiver ',\n",
       " 'mf recommandation technique item relation ',\n",
       " \"mahout écosystème forest random d'amélioration \",\n",
       " 'manquant hydrologie gapit hydrométriques station ',\n",
       " 'knowledge uncertainty representation an we ',\n",
       " 'mappings adaptation ontologie heuristique guidant ',\n",
       " 'auteur pan-clef rédigé authentification rédigés ',\n",
       " 'recommandation utilisateur sociale préférence recevant ',\n",
       " 'amazighe langue repérage nommées entité ',\n",
       " 'and property re real-estate 2.0 ',\n",
       " 'fusion éventuel driven linked conflit ',\n",
       " 'multidimensional big data and business ',\n",
       " 'network actor centrality measures in ',\n",
       " 'texte intersection copier/coller maximale alternative ',\n",
       " 'classe new mixture labeling examining ',\n",
       " 'biclustering mapreduce dataset cores scalable ',\n",
       " 'vocabulary tweet tweets this to ',\n",
       " \"croisé collaboratif item filtrage qu'elle \",\n",
       " '2d outil projection interactif clustering ',\n",
       " 'complexité motif axiome évaluation lié ',\n",
       " 'rankmerging lien réseau classement épars ',\n",
       " 'cpt prédiction compact ppm all-k-order ',\n",
       " 'attribut règle liaison nombre regroupement ',\n",
       " 'élastique noyau régularisation kdtw non-linéaire ',\n",
       " 'évidentielles skyline évidentiel pareto incertaines ',\n",
       " 'method and with knowledge campaign ',\n",
       " 'tweets in creation to users ',\n",
       " 'ultramétricité dissimilaritées regroupement genere pam ',\n",
       " 'composante parcimonieuse em principal probabiliste ',\n",
       " 'icm compacité énergie haute convergence ',\n",
       " 'interrogation graphe conceptuels optionnel langage ',\n",
       " 'multiplexe réseau graine communauté centrée ',\n",
       " 'propagation comparer modèle réécriture plate-forme ',\n",
       " 'changement composé formalisation ontologiques élémentaires ',\n",
       " 'sémiotique style indicateur expliquerons neuronal ',\n",
       " 'p-etl massives etl * parallèle ',\n",
       " 'site social microblogging conversational interaction ',\n",
       " \"stemma codicum triplet obtained 's \",\n",
       " 'sous-groupes molécule olfactives neuroscientifiques olfaction ',\n",
       " 'hotspots hotspot photographie relation fendu ',\n",
       " 'xplor xewgraph everywhere intelligence and ',\n",
       " '1d-sax sax série symbolisation temporelles ',\n",
       " 'agrégation représentation respectent rapportées attendues ',\n",
       " 'alignement ontologie liées nalt agrovoc ',\n",
       " \"document administratif numérisation homme l'opposé \",\n",
       " 'mapreduce musée accessibilité paradigme parallèle ',\n",
       " 'protéine-arn rosettadock fonction score interaction ',\n",
       " 'naïf bayésien log-vraisemblance obtenue variable ',\n",
       " 'phrase dépendance traduisant grammaire syntaxique ',\n",
       " 'fusion typés ontologie formelle grammaire ',\n",
       " 'sommet multi-résolution temps qualitatif catégoriels ',\n",
       " 'drift drifted dataset new sudden ',\n",
       " 'data datasets and broad to ',\n",
       " 'descripteur action st-surf reconnaissance spatio-temporels ',\n",
       " 'solaire rayonnement prédiction photovoltaïque flux ',\n",
       " 'bayes naïf classifieur flux variable ',\n",
       " 'programme clustering détournant attributs-valeurs proscrit ',\n",
       " 'séquence évènements grille dimension temporel ',\n",
       " 'fcl intersection conceptuels communauté lien ',\n",
       " 'conduisent syndrome alarmiste déclaration revanche ',\n",
       " 'cluster affectation borne bernstein accélération ',\n",
       " 'caractérisation chemin voisinage capte linéarisation ',\n",
       " 'compréhension cuisine recette site déduite ',\n",
       " 'cube olap orientée architecture calcul ',\n",
       " \"préférence profil contextuelles l'utilisateur effectuer \",\n",
       " 'eclipse logiciel passé leçon fondation ',\n",
       " 'volume diminue pondération descripteur influençant ',\n",
       " 'humanité matériau numériques tenterons numérique ',\n",
       " 'opinion détectant tweet polarité subjectivité ',\n",
       " 'changement qualitatives détection flot cdcstream ',\n",
       " 'domicile dépendantes mouvement capteur risque ',\n",
       " 'géographiques bdg openjump gdb remplissage ',\n",
       " 'communauté prédiction réseau interaction prédit ',\n",
       " 'pertinence recommandation confusion intrinsèquement natif ',\n",
       " 'chanson parole collection explorer interface ',\n",
       " 'étiquetage thème pixel couverte classification ',\n",
       " 'orientés attribués graphe isomorphisme combinatoire ',\n",
       " \"d'épisodes déposés quotidiennement prédiction épisode \",\n",
       " \"minimaux motif d'ensembles minimisable profondeur \",\n",
       " 'kd-ariane déploiement programmation ariane visuel ',\n",
       " 'stratégie agent trace élicitation méthodologie ',\n",
       " 'recouvrement k-moyennes contrôler groupe généralisation ',\n",
       " 'textuel extrait prend ontologie génération ',\n",
       " 'attribués sommet hiérarchie dynamique motif ',\n",
       " 'densité groupe non-disjoints classification observation ',\n",
       " 'rôle communautaire réseau connectivité twitter ',\n",
       " 'média noeud social non-incrémentales social-attribute ',\n",
       " 'spatial modeling raster ocelet format ',\n",
       " 'géographique référentiel localisation thématique interconnexion ',\n",
       " 'événement flot exploratoire méritant outskewer ',\n",
       " 'nommées expansion entité requête recherche ',\n",
       " 'discours subjectivité médical médecin médicaux ',\n",
       " 'incertitude théorie due incertain distribution ',\n",
       " 'traverse minimales local-generator hypergraphe régner ',\n",
       " 'locuteur articulateurs articulatoire 3-way tableau ',\n",
       " 'crowd for mining human that ',\n",
       " 'caméra opérateur trajectoire sélectionner jour ',\n",
       " 'motif récursifs transactionnelles séquentielles résumé ',\n",
       " 'noyau recouvrante ensembliste formulons autoriser ',\n",
       " 'bi-partitionnement pondération bloc variable topologique ',\n",
       " 'manquant analogique proportion valeur booléens ',\n",
       " 'forum patient sentiment santé surprise ',\n",
       " 'across profiles snss individual that ',\n",
       " \"reconstruction évènements chronologie enquêteur d'enquête \",\n",
       " 'traduction rim rail inter-langues exprimées ',\n",
       " 'samples models population learned training ',\n",
       " 'exception skyline point anomalie requête ',\n",
       " 'multi-label ebr ecc chains ml-knn ',\n",
       " 'knn voisin catégorisation texte proche ',\n",
       " 'élastique axe réduction matière complexité ',\n",
       " 'multicritères cruciales conflit collaborative décideur ',\n",
       " 'symétrie motif ensemblistes toivonen mannila ',\n",
       " 'smarter computers dream texts large ',\n",
       " 'twitter message stream can system ',\n",
       " 'consensus partition adjoint fondons pex ',\n",
       " 'topologiques statis carte partition compromis ',\n",
       " 'déclaratif ppc contrainte hétérogène croisant ',\n",
       " \"mobile m-learning apprentissage l'apprenant prochaine \",\n",
       " 'spectral amènent notoirement satisfaisante fonctionne ',\n",
       " 'genre auteur hybride profil statisque ',\n",
       " 'nodaux communauté réseau noeud attribut ',\n",
       " 'populaires thématique twitter message délivrée ',\n",
       " 'métrique étiquetage sélection variable textuelles ',\n",
       " 'image ontologiques terme tomodensitométriques annotation ',\n",
       " 'textual data representation parameterized offering ',\n",
       " 'simplifiés arbre décision obtenir puissante ',\n",
       " 'modularité newman 2mod-louvain inertie vectorielles ',\n",
       " 'prosopographiques visualisation portail accès interface ',\n",
       " 'ralentissement quantitative bibliographique découverte motif ',\n",
       " '3d stéréoscopique stéréoscopie exploratoire perspective ',\n",
       " 'environments pos tagging collaboration literary ',\n",
       " 'catégorisation texte proche coût k-voisins ',\n",
       " 'cabine simulation conception confort passager ',\n",
       " 'ex réseau concept afc analyse ',\n",
       " 'réclamation allocataire typologie caf nationale ',\n",
       " 'arc concept relationnelle relationnelles treillis ',\n",
       " 'orientée objet haute région résolution ',\n",
       " 'protéiques epsilon aligneurs multi-étiquettes alignement ',\n",
       " \"routier trajectoire réseau contrainte l'empruntent \",\n",
       " 'gisement historien historique entrer projet ',\n",
       " 'série temporelles descripteur coclustering objets-attributs ',\n",
       " 'skypatterns soft-skypatterns toxicophores chémoinformatique manqué ',\n",
       " 'plagiarism methods academic by to ',\n",
       " 'traverse minimales hypergraphe hypergraphes imt-extractor ',\n",
       " 'tendance enseigne commercial clientèle précoce ',\n",
       " 'ontologie page noyau jardinage fiche ',\n",
       " 'antenne appel habitant france mobile ',\n",
       " 'évolutifs oubli floue incrémental récursifs ',\n",
       " 'n-aires ontologie dédiée relation représentation ',\n",
       " 'aboutissons acycliques pondérés acyclique extrayant ',\n",
       " 'attribués arbre fréquents motif sous-arbres ',\n",
       " 'betti génératif simplicial complexe csg ',\n",
       " 'syntagme nominaux sri indexation filtrage ',\n",
       " \"positives négative règle rapn s'est \",\n",
       " \"changement étiqueté fenêtre détection n'avoir \",\n",
       " 'tags pourcent relation taux annotateur ',\n",
       " 'protéine espèce trimères coli escherichia ',\n",
       " 'régulation inférence réseau sortie interaction ',\n",
       " 'twitter capitaliste sociaux réseau graphe ',\n",
       " 'ri sociale social information modèle ',\n",
       " 'text groups representation terms sequence ',\n",
       " 'bitm bi-partitionnement topologique carte bi-clusters ',\n",
       " 'alignement choquet intégrale paramétrage ontologie ',\n",
       " 'classe itératif supervisée centre définie ',\n",
       " 'ré-écriture requête adressées intégration phase ',\n",
       " 'mot-clé document similitude présentant similaires ',\n",
       " 'formateur nucléaire pleine simulateur activité ',\n",
       " 'variable hiérarchiques mesurer sélection supervisé ',\n",
       " 'snow subspace copac dénommé sous-espaces ',\n",
       " 'factorisation note recommandation prédiction matrice ',\n",
       " 'text2geo étang géospatiales thau information ',\n",
       " 'réseau attribut entité relation étendant ',\n",
       " 'in and work dimensional narrative ',\n",
       " 'secondaire table variable itemsets construites ',\n",
       " 'ràpc ontologie composition recherche information ',\n",
       " 'ppc contrainte programmation maximal clusters ',\n",
       " 'généralisée vraisemblance thématique press associated ',\n",
       " 'videos tags video we system ',\n",
       " 'carte cognitive valident critère influence ',\n",
       " 'évolutif statique suivi clustering ordinaire ',\n",
       " 'pco strate multicouche spécialisation mixte ',\n",
       " 'variable construction table constructibles choisissant ',\n",
       " 'séquence similarité mesure efficacement extrême ',\n",
       " 'parallélisation cpu gpu radiale configuration ',\n",
       " 'antipatterns antipattern bad ontology queries ',\n",
       " 'rs-ndf bootstrap nouveauté filtre ndf ',\n",
       " 'ald poids multimédia discriminante modalité ',\n",
       " 'event biological and which statements ',\n",
       " 'tca triadiques biclusters biclustering numériques ',\n",
       " 'intervalle généralisation concept probabilités/fréquences ordinales ',\n",
       " 'vigilance eeg électrode 58 cart ',\n",
       " 'modularité maximisation spectrale catégorielles algèbrique ',\n",
       " 'probabiliste instant formulation séquentielles génération ',\n",
       " 'probabiliste catégorielles som auto-organisatrice topologique ',\n",
       " 'procédure chirurgicales chirurgien dtw surgical ',\n",
       " 'courbe clusters paramétrique fonctionnelles hiérarchique ',\n",
       " 'multi-niveaux topologique hiérarchique clustering graphe ',\n",
       " 'caractéristique classificateur sélection combinaison sélectionné ',\n",
       " 'croyance classification théorie supervisée classificateur ',\n",
       " 'relationship attribute community detection network ',\n",
       " 'maritime surveillance navire accident prévision ',\n",
       " 'gof groupe outliers outlier factor ',\n",
       " 'sous-population expert éprouvée idéal converger ',\n",
       " 'recommender distributed was encouraged research ',\n",
       " 'cooper-herskovitz simplified criterion assumptions bayesian ',\n",
       " 'diamètre graphe erreur rapide commentés ',\n",
       " 'taxonomie lexique hypothèse approximer defaits ',\n",
       " 'opinion internaute critère expression mot ',\n",
       " 'propriété co-variations sommet réseau motif ',\n",
       " 'df approximatives modifiée exactes relation ',\n",
       " 'régularité fréquents lien noeud flmin ',\n",
       " 'intervalle séquence temporelles incertitude fréquentes ',\n",
       " 'sous-parties généraliste ontologie enrichir focalisées ',\n",
       " 'concept vidéo définir indexer segment ',\n",
       " 'incrémentale itemsets fréquentes flux séquence ',\n",
       " 'robot hog corps svm détection ',\n",
       " 'boycott caractérisation identification type analyse ',\n",
       " 'client probabilité produit typologie campagne ',\n",
       " 'dépendance règle multivaluées définies analogie ',\n",
       " 'négative règle méta-règles positives part ',\n",
       " 'genetic biological utilizing guarantee high-throughput ',\n",
       " \"supervision domotiques interaction l'utilisateur capteur \",\n",
       " 'analysis multi-block canonical regularized correlation ',\n",
       " 'secondaire variable discrétisation table attribut ',\n",
       " 'légende corese concevoir raisonner cartographie ',\n",
       " 'agrégée élément bayésiens xml inex2009 ',\n",
       " 'spatial learning relational and already ',\n",
       " 'réorganisation olap cube hiérarchique bond ',\n",
       " 'impact service recommandation découverte web ',\n",
       " 'ricsh fragment corpus thématique contextuelle ',\n",
       " 'jointe grille modèle consistance distribution ',\n",
       " 'visual and analytics to with ',\n",
       " 'jurisprudence juridique décision arabe ontologie ',\n",
       " 'wiki isicil entreprise projet social ',\n",
       " \"diffuseur tmd-miner membre réseau d'hypergraphe \",\n",
       " 'clustering heuristics and network in ',\n",
       " 'transfert pondéré topologique matricielle apprentissage ',\n",
       " 'dissimilarité matrice pondération classe partition ',\n",
       " 'assistant utilisateur paramétrages visuel paramétrage ',\n",
       " 'image quantification grand descripteur échelle ',\n",
       " 'diffusion twitter individuel information multi-dimensionnelle ',\n",
       " 'histogramme distance nominaux entité étage ',\n",
       " 'why and has this in ',\n",
       " 'inter-domaines transfert invariants médiation complétion ',\n",
       " 'décomposition hiérarchique graphe blondel multi-échelle ',\n",
       " 'agronomiques imprécision gérant agricoles entité ',\n",
       " 'spatio-séquentiels motif dengue épidémie anthropiques ',\n",
       " 'taxonomie hiérarchie construction expertisées automatique ',\n",
       " \"marquage webmarks ressource n'exploitaient bookmarks \",\n",
       " 'nucléaire krex @ explicites tacites ',\n",
       " 'journalistiques porteur tweets recherche information ',\n",
       " 'prétopologique lexico-sémantiques structuration texte acquisition ',\n",
       " 'tarification assurance non-vie risque généralisés ',\n",
       " 'électriques compteur communicant duplication agrégées ',\n",
       " 'suspect visuel comportement communication accompagnée ',\n",
       " 'exhaustif professionnel guidé recommandation expert ',\n",
       " 'textuelles ressource analysons construction méthodologie ',\n",
       " 'indice liaison comportement probabiliste limite ',\n",
       " 'afc image contingence tableau mot ',\n",
       " 'spatiotemporelle comptage blob franchissant ligne ',\n",
       " 'transduction couverture nommées annotation règle ',\n",
       " 'connaissances1 iconique coopérative catégorisation apport ',\n",
       " 'cold-start collaboratives recommandation logs froid ',\n",
       " 'som topologiques carte contrainte triviale ',\n",
       " 'logique clause prédicat markov réseau ',\n",
       " 'carte cognitives cognitive influence vue ',\n",
       " 'mesure dégager règle 61 propriété ',\n",
       " 'fa aléatoires voisinage coller libérait ',\n",
       " 'pose isar image cible transformée ',\n",
       " 'representative rules output its an ',\n",
       " \"discriminante '' échantillon `` léchantillon \",\n",
       " 'to and processing high-level defence ',\n",
       " 'tanagra cellulaire discrétisation implémentation conception ',\n",
       " 'renforcement transport automatisés sécurité construction ',\n",
       " 'champignon ontologique expression séquence construction ',\n",
       " 'histograms summarization on-line stream by ',\n",
       " 'satellite évolution image sits série ',\n",
       " 'sociaux graphe réseau document ',\n",
       " 'flux changement détection état-de-art favorablement ',\n",
       " 'tableau annotation guidée floues flou ',\n",
       " 'profil utilisateur court-terme long-terme social ',\n",
       " 'early in will accuracy sequences ',\n",
       " 'criterion similarity with two clustering ',\n",
       " 'motif transaction support pondérant reçoive ',\n",
       " 'équivalence proximité topologique mesure plan ',\n",
       " 'arc graphe clusters densité taille ',\n",
       " 'usager question internet ',\n",
       " 'acabit quezao terminologique aspect 2424actu ',\n",
       " 'achat client motif contextuels séquentiels ',\n",
       " 'événement intervalle motif temporel représentatifs ',\n",
       " 'relationnelles étape réseau sociaux agrégation ',\n",
       " 'clique étiquette sommet homogènes contrainte ',\n",
       " 'ensemblistes bruités motif bruit heuristique ',\n",
       " 'barrière format transformation brut utilisateur ',\n",
       " 'haptiques simulateur eiah apprenant chirurgie ',\n",
       " 'roc courbe graphique interprétation ',\n",
       " 'spectrale relationnelle problème lagrange multiplicateur ',\n",
       " 'territoriale collectivité progiciel ingénierie introduction ',\n",
       " 'isicil intelligence travers communauté ligne ',\n",
       " 'moteur wiki wikis sémantique art ',\n",
       " 'm3a assisté ingénierie maintenance plateforme ',\n",
       " 'concordance mesure réduire évidentielles source ',\n",
       " 'participant p2p hétérogénéité disparité considérant ',\n",
       " 'glose testing unité extraire rigoureuses ',\n",
       " 'mobility and data mining gps ',\n",
       " 'linéarité énoncé français phénomène catégorie ',\n",
       " 'rto termino-ontologique annotation tableau n-aires ',\n",
       " 'phénomène spatio-temporels séquence motif dynamique ',\n",
       " 'simulation propagation publication reproduire détaillant ',\n",
       " 'moteur réponse question web sémantique ',\n",
       " 'wikipedia moteur permis question typer ',\n",
       " 'motif delta-libres séquentiels difficile produit ',\n",
       " 'matching métadonnées interopérabilité assurer mumie ',\n",
       " 'géolocalisée nomao personnalisée recherche ',\n",
       " 'graphe ac-réduits isomorphisme intéressante sous-graphes ',\n",
       " 'alignement contrainte différence programmation prête ',\n",
       " 'naïf bayésien variable classifieur poids ',\n",
       " 'yacaree parameter-free rule with mining ',\n",
       " 'socio-semantic view network based point ',\n",
       " 'pondération codées mixtes simultanée binaire ',\n",
       " 'pmi ppmi positives variante formelle ',\n",
       " 'cyclone longitude latitude 6 vent ',\n",
       " 'fusion prise source compte réseau ',\n",
       " 'attribut propositionalisation gèrent discrétiser catégoriels ',\n",
       " 'learning and data failure process ',\n",
       " 'action estimés magnitude vidéo séquence ',\n",
       " 'logs requête logarithme olap résumé ',\n",
       " 'variable un-à-plusieurs multi-tables cible relationnel ',\n",
       " 'web3.0 audiovisuel service contenu recherche ',\n",
       " 'répétition télévisuels structuration supervisé flux ',\n",
       " 'musique morceau calme perception adaptable ',\n",
       " 'emploi offre catégorisation internet choisis ',\n",
       " 'engines search are high and ',\n",
       " 'skylines skycube treillis accord skycuboïdes ',\n",
       " 'règle robustes critère robustesse bayésien ',\n",
       " 'folksonomies tagging cycle complet vie ',\n",
       " 'géolocalisation actualité accès faciliter résumé ',\n",
       " 'navigation espace outil sémantique ',\n",
       " 'cnss cellulaire induction neuro-symbolique neurone ',\n",
       " 'floues contextuelles préférence requête règle ',\n",
       " 'ontologie homologue distance légères calcule ',\n",
       " 'client méthodologie économique recommandation 000 ',\n",
       " 'opinion médiocre vocable film expriment ',\n",
       " 'treemap arbre grille topologique partition ',\n",
       " 'géographiques vis ontologie entité globale ',\n",
       " 'courriel indésirables cellulaire machine détection ',\n",
       " 'alignement enrichir ontologie taxomap topographie ',\n",
       " 'diagnostique codage partielle fusion hétérogènes ',\n",
       " 'intra description structure visualisation inter-groupes ',\n",
       " '” item paire hautement corrélées ',\n",
       " 'meta-actions rules action and ',\n",
       " 'publicité portail affichage atteignent crées ',\n",
       " 'convexes géométrie fermeture topologie agrégation ',\n",
       " 'maintenance train préventive ferroviaire décision ',\n",
       " 'motif locaux contrainte n-aires csps ',\n",
       " 'document pédagogiques discursifs pédagogiqes placées ',\n",
       " 'traminer événement séquence analyse ',\n",
       " 'olap opérateur ligne analyse factorielle ',\n",
       " 'foule évènements orientation scène détection ',\n",
       " 'pair routage incrémentale usage requête ',\n",
       " 'logic non-relational relational to deduplication ',\n",
       " 'climatiques engendrés spatiales risque apport ',\n",
       " 'lexico-syntaxiques patron apprentissage souhaitables fastidieuse ',\n",
       " 'csp spécification apprentissage ',\n",
       " 'nominales adaptatif formels supervisé concept ',\n",
       " 'visualisation quantité grandissante multidimensionels biomimétique ',\n",
       " 'ancien traits détaillées bref image ',\n",
       " 'privée média préservation vie sociaux ',\n",
       " \"cube univsersité s'agréger hal tf-idf \",\n",
       " 'usager santé accès cancer terminologie ',\n",
       " 'cartographie casi cartocel booléenne cellulaire ',\n",
       " 'chorml géographiques résumé visuel base ',\n",
       " 'événement séquence ordonnancement notion différence ',\n",
       " 'structurelle document classe revient distance ',\n",
       " 'multimédia recherche modèle ii catégorie ',\n",
       " 'expliquer co-partitionnement variable groupement préparation ',\n",
       " 'cube cnd-cube fermé ” concise ',\n",
       " 'signe codex maya interprétables situant ',\n",
       " 'réconciliation alignement numérique combiner logique ',\n",
       " 'expertise wikis disparité créés blog ',\n",
       " 'pureté critère connaissance intégration mobilisé ',\n",
       " 'non-supervisé sous-ensemble caractérisés détectées similitude ',\n",
       " 'composition social soco réseau service ',\n",
       " \"noyau arbre induit d'optimalité parallèlement \",\n",
       " 'cube émergent quotient fermé cubique ',\n",
       " 'dafoe thésaurus plateforme construire texte ',\n",
       " 'plcm fermé itemsets fréquents creuses ',\n",
       " 'dfc dépendance fonctionnelles conditionnelles df ',\n",
       " 'data stream change distribution detection ',\n",
       " 'data using state integration over ',\n",
       " 'anormal vidéo mouvement détection ',\n",
       " 'weka formels plateforme basées développement ',\n",
       " 'gmm-smos locuteur vecteur variante smo ',\n",
       " 'etude comparative langage lien complexe ',\n",
       " 'protéiques stabilité etude séquence sélection ',\n",
       " 'sql expansion base domaine transparente ',\n",
       " 'réconciliation colorés petri explication renforcer ',\n",
       " 'fonctionnelles dépendance olap exploration association ',\n",
       " 'distinctifs itemsets flux idkf heikinheimo ',\n",
       " 'obstacle région intérêt extraction ',\n",
       " 'graduel motif élevé peinent génèrent ',\n",
       " 'événement association proportionnels séquentielle risque ',\n",
       " 'traduction rail séquence corpus fréquentes ',\n",
       " 'vdm réalité graphique technique virtuelle ',\n",
       " 'amo droits accès gestion stratégie ',\n",
       " 'communities network presence nodes metrics ',\n",
       " 'fonctionnelles dépendance inférer incfds inférence ',\n",
       " 'image afc gpu parallèle incrémental ',\n",
       " 'complexité indice catégorielles états séquence ',\n",
       " 'bayesienne cancer maximum diagnostic entropie ',\n",
       " 'breiman priori machine intégration maximum ',\n",
       " 'contrainte ajoutées interactive proche réduction ',\n",
       " 'sgfd flux portent fenêtre conserver ',\n",
       " 'lab k-words clé explorer scientifique ',\n",
       " 'kgram graal abstraite implémente langage ',\n",
       " 'conflit théorie croyance imparfaites revenir ',\n",
       " 'mot langue dépendance surfacique explorées ',\n",
       " 'xml interrogation multidimensionnelles entrepôt exécute ',\n",
       " 'your mysins make semantic system ',\n",
       " 'régles confidence confiance ” seuil ',\n",
       " 'recouvrantes recouvrante osom carte auto-organisatrices ',\n",
       " 'pattern one past future and ',\n",
       " 'pcar cycliques cyclique varient caractérisé ',\n",
       " 'graduel motif multi-threading pgp-mc ordinateur ',\n",
       " 'vidéo série temporelles prédiction séquence ',\n",
       " 'pretopolib prétopologie java librairie implémentant ',\n",
       " 'multidimensionnel olap opérateur complexe objet ',\n",
       " 'règle classe associative association significatives ',\n",
       " 'protein pgr graphe repository graph ',\n",
       " 'interval-valued partitioning advances recent algorithms ',\n",
       " 'cas rapc modulaires ontologie requête ',\n",
       " 'reconnaissance concept basée apprentissage ',\n",
       " 'image bi-directionnelle caractéristique réduction version ',\n",
       " 'série mémoire al ) 2002 ',\n",
       " 'textuelles nommer discussion regrouper groupe ',\n",
       " 'préférence gros skyline requête volume ',\n",
       " 'flux résumé généraliste sgfd variées ',\n",
       " 'merger rss ',\n",
       " 'multidimensionnels motif entrepôt séquentiels automate ',\n",
       " 'entropie catégorisation textuels descripteur sélection ',\n",
       " 'segmentation client purchase customer self-clustering ',\n",
       " \"gène séquentiels jusqu'alors sequencesviewer accompagnement \",\n",
       " 'siam médicaux indexation système article ',\n",
       " 'vol simplification enregistrées perte drastique ',\n",
       " 'pair p2p simtole plateforme alignement ',\n",
       " 'hiérarchique topologique ” partition so-tree ',\n",
       " 'sous-échantillonnage déséquilibre apprentissage classe observation ',\n",
       " 'automobile ascendant suivi hiérarchique classification ',\n",
       " 'dynamique rbd connaissance bayésiens décision ',\n",
       " 'tulip graph framework and representations ',\n",
       " 'croyance masse posteriori maximum probabilité ',\n",
       " 'relation candidat sémantique hypothèse ontologue ',\n",
       " 'prédicat pléthoriques ajout initiale corrélation ',\n",
       " 'lissage communauté identification modèle probabiliste ',\n",
       " 'ashms imc enfant précoce management ',\n",
       " 'correspondance ^ étudiées ontologie complexe ',\n",
       " 'stratégie bayésienne échelon unidimensionnelles comparatives ',\n",
       " 'inventif artefact acquisition conception connaissance ',\n",
       " 'flot itemsets fenêtre impossible hiérarchie ',\n",
       " 'essentielles traduction idée texte graphe ',\n",
       " 'co-localisations expert géologiques géo-référencées découverte ',\n",
       " 'to patches visual sentence words ',\n",
       " 'wikipedia wikipediaviz lecteur visualisation qualité ',\n",
       " 'wcum site usage web analyse ',\n",
       " 'user profile contextualization and contexts ',\n",
       " '21ème siècle organisation succinctement accompagner ',\n",
       " 'ei dépendance acquisition pourcent 755 ',\n",
       " 'matériel image skin3d calibration réalité ',\n",
       " 'semi-structured in retrieval structuring reach ',\n",
       " 'risk multiarmed bandit approach with ',\n",
       " 'dissimilarités facteur variabilité états induction ',\n",
       " 'prédictives variable prédire procédure identifiées ',\n",
       " 'investissement optimiser commercial opération régression ',\n",
       " 'marché matériau métier commercial opération ',\n",
       " 'olap opération r-olap implanteopération deshiérarchies ',\n",
       " 'owl-dl sémantique ontologie entreindividus approchemise ',\n",
       " 'assessing knn uncertainty fusion data ',\n",
       " 'pattern graphs sequence databases reductionof ',\n",
       " 'pondération variable observation caractérisation timisée ',\n",
       " 'règle cibler décideur intéressantes lesconnaissances ',\n",
       " 'ld+règles cisna gérer hybride système ',\n",
       " 'envi télédétection image innovants pixelsclassique- ',\n",
       " 'intrusion detection anomaly ids outlier ',\n",
       " 'syntaxiques induites valider utilisantdes qualitéde ',\n",
       " 'équivalence ordre induit mesure degré ',\n",
       " 'programming constraint mining for data ',\n",
       " 'bruit bruités descripteur construction qualitésobtenues ',\n",
       " 'flux contrôle traitement observation facteur100 ',\n",
       " 'galois correspondance cettecorrespondance donnéessimples degalois ',\n",
       " 'dbfrequentqueries fréquentes requête extraction ',\n",
       " 'multi-agents communication ads cmp sma ',\n",
       " 'stratégie robot observables humains boucle ',\n",
       " 'adn puce demon séquentiels don-nées ',\n",
       " 'demon-visualisation biologiques séquentiels extrait motif ',\n",
       " 'desesper hydraulique surveillance pré-traitement centrale ',\n",
       " 'attaque signature intrusion collaboratif détection ',\n",
       " 'multi-résolution atypiques flot détection objet ',\n",
       " 'markov ordre démontronsexpérimentalement rechercheet existantesun ',\n",
       " 'croki2 classe validation traversun dedéterminer ',\n",
       " 'diagnostic adaptatif diagnostiqueurs multi-sources vérifiée ',\n",
       " 'reposant caractérisation treillis lefiltrage ligneillustre ',\n",
       " 'rfid traçabilité groupe spatio-temporelles individu ',\n",
       " \"vente performantsd'aide 2007montre classifieurbayesien proposéeest \",\n",
       " 'explorer3d visualisation classification donnée ',\n",
       " 'n-aires bruitées fermé relation propositionpour ',\n",
       " 'corrélation décisionnelles contingence vecteur lectique ',\n",
       " 'graduelles réelsmontrent associés.des gradualité définissonsformellement ',\n",
       " 'fp-growth fcp-growth adaptation générer association ',\n",
       " 'riche schéma hiérarchie destructures analyseclassique ',\n",
       " \"fusion heuristiquesmises d'ex-périmentations opé-ration informa-tions \",\n",
       " 'formels concept dopage classifieur boosting ',\n",
       " 'tournebool mot corpus reuters randomisation ',\n",
       " 'in answer text mass vectors ',\n",
       " 'gène al formelle envi-ronnements compor-tement ',\n",
       " 'ghsom som pourrendre analyseexploratoire objecif ',\n",
       " 'créativité ilp calculatoire synthèse prédicat ',\n",
       " 'syr symbolique logiciel analyse donnée ',\n",
       " 'dtmvic text inférence logiciel mining ',\n",
       " 'culturel patrimoine management domaine connaissance ',\n",
       " 'voisinage quiconsidère par-tie sontproches paramétriques ',\n",
       " \"préférence contextuelles olap contexted'analyse lesdonnées \",\n",
       " 'tei spécialisées bibliothèque structuresarborescentes justifions ',\n",
       " 'wokm okmed okm classe métrique ',\n",
       " 'audit unlabelled adaptive anomaly intrusion ',\n",
       " 'ontologie bloc alignement défi deuxméthodes ',\n",
       " 'data will new in developments ',\n",
       " 'voting multi-classifier probabilistic rule svms ',\n",
       " 'relationnelles logiciel aussiun processuspeut sup-porté ',\n",
       " 'approcheautomatique siglesissues définition biomédicaux sigle ',\n",
       " 'résumé requête obtenussont kdd_99 nousutilisons ',\n",
       " \"chaîne jaccard caractère indice d'entitésnommées \",\n",
       " 'stream spam séquentiels transaction incrémentale ',\n",
       " 'incrémental processeur gpu parallèle svm ',\n",
       " 'taaable recipes dish replacing theconstraints ',\n",
       " 'traminer librairie séquentielles analyse donnée ',\n",
       " '| .la réseau décomposition changement ',\n",
       " 'arbre bayésienne décision arttout notreméthode ',\n",
       " 'aléatoires forêt arbre oblique algorithme ',\n",
       " 'multi-métiers cross-lingue européen entreprise prototype ',\n",
       " 'médicale génomique corrélation linéaire système ',\n",
       " 'obtenu premierplan learningchallenge pascal pré-sentons ',\n",
       " 'segmentation image pixel supervisée intensité ',\n",
       " 'image afc adt recherche factorielle ',\n",
       " 'changement simulation cours usageà uneméthodologie ',\n",
       " 'symbolique échelle traitement grand donnée ',\n",
       " \"d'images graphique relation tentons interactionplutôt \",\n",
       " 'rough in set spatial theory ',\n",
       " 'nsvm boosting psvm ls-svm machine ',\n",
       " 'film opinion rapprochement montrerons vocabulaire ',\n",
       " 'définition alignement svm noms-adjectifs investigués ',\n",
       " 'événement presse automatique contenant annotev ',\n",
       " 'visage reconnaissance classification galois hybride ',\n",
       " 'n-gramme vie parcours séquence précisera ',\n",
       " 'plainte pollution réalisable apparie domestique ',\n",
       " 'binaires probabiliste carte bernoulli gtm ',\n",
       " 'sgbd nautilus métier entreprise optimisés ',\n",
       " 'gène cellulaire cycle expression référence ',\n",
       " 'petit monde vue réseau document ',\n",
       " 'clusterings individuel clustering dimensionalité soutenue ',\n",
       " 'ssvc étoile coordonnée optimale projection ',\n",
       " 'co-classification contrainte quadratiques résidu somme ',\n",
       " \"spatio-temporelle asti concepteur l'utilisateur conception \",\n",
       " 'video in people activity stage ',\n",
       " 'séquentiels inattendus motif fréquence inattendues ',\n",
       " 'délestage dégradation flux cube mécanisme ',\n",
       " 'atypiques salaire groupe variable fort ',\n",
       " 'bootstrap coupure ouvrons systématiquement chi-merge ',\n",
       " \"forêt détérioré contourner d'applications minoritaire \",\n",
       " 'flot séquentiels échantillonnage motif hypothèse ',\n",
       " 'flux distribués consommation électriques capteur ',\n",
       " 'file personal tags context tag ',\n",
       " 'okm recouvrante moc théoriques mélange ',\n",
       " \"influence étape l'influé maximisant règle \",\n",
       " 'asymétriques arbre entropie décision asymétrique ',\n",
       " 'lsa grammaticales conceptuelle éducatives syntaxico-sémantiques ',\n",
       " 'itemsets période compact deico cohérent ',\n",
       " 'drone élémentaires photographie zone fine ',\n",
       " 'multidimensionnels motif séquentiels clos candidat ',\n",
       " 'annotation contextuelles sémantique émanant transmettre ',\n",
       " 'croisement validation relation ontologie domaine ',\n",
       " 'itemset fiasco item nouvel batches ',\n",
       " 'homophone acoustique mot 41 sélectionné ',\n",
       " 'and to in data internet ',\n",
       " 'vidéo soft condensé computing longueur ',\n",
       " 'prototypicalité gradient endogroupe ontologie pragmatique ',\n",
       " 'depotentiel hypersmooth calcul serveur client ',\n",
       " 'entreprise mining produire théoriques forte ',\n",
       " 'contrainte médicales carte version mélanome ',\n",
       " 'probabiliste document structure modèle révélés ',\n",
       " 'itinéraire récit voyage attendu déplacement ',\n",
       " 'image segmentation évolutive pixel identifier ',\n",
       " 'variable supervisé outil khiops millier ',\n",
       " 'geodoc territoire décideur diagnostic document ',\n",
       " 'graphe période sommet relationnelles décomposer ',\n",
       " 'fia itemset itemsets flot automate ',\n",
       " 'sociaux réseau ordonnancement parlerons prisée ',\n",
       " 'soda récentes avancée visualiser symbolique ',\n",
       " 'cognitives carte hiérarchiques concepteur cognitive ',\n",
       " 'annotation hiérarchiques erreur hiérarchie mesure ',\n",
       " \"évaluation degré ontologiques '' `` \",\n",
       " 'instrumental plateau créativité e-services méthodologie ',\n",
       " 'trees unlabeled are rules from ',\n",
       " 'trajectoire arrêt mouvement temporel concept ',\n",
       " 'afc image mot axe recherche ',\n",
       " 'risque industriel raisonnement ineris national ',\n",
       " 'primal optimisation svm shrinking différentiables ',\n",
       " 'neurone rbf-gene rbf optimiser poids ',\n",
       " 'pondération locale carte variable segmentation ',\n",
       " 'réaction schéma chimiste graphe chimiques ',\n",
       " 'symbolique concept histogrammesuites lavariation analysantces ',\n",
       " 'sigle acquisition dictionnaire présenté textuelles ',\n",
       " 'intrusion associatives détection générique sdis ',\n",
       " 'spatiale solap multidimensionnelle introduction analyse ',\n",
       " 'licorn adaptative régulation corégulation co-régulation ',\n",
       " 'chaîne noyau image graphe région ',\n",
       " 'bibliothèque personnalisée numériques retriés citation ',\n",
       " 'routier trafic atypiques urbain spatio-temporels ',\n",
       " 'alternative requête génomique entrepôt multi-niveaux ',\n",
       " 'référent carte pondérée voisinage segmentation ',\n",
       " 'fenêtrage spatial traitées requête temporel ',\n",
       " 'ontologie générique semi-automatisation conceptualisation terminae ',\n",
       " 'wordnet som textuels basés synsets ',\n",
       " 'superposées fenêtre basées stratégie repérer ',\n",
       " 'conditional entropy bayesian generalized network ',\n",
       " 'itemsets clé suppression essentiel petit ',\n",
       " 'décideur argumentative cruciales multi-agent conflit ',\n",
       " 'groupe nombre stabilité déterminer classification ',\n",
       " 'cartogramme spatiotemporelles ex dimension composante ',\n",
       " 'noyau vectoriel latent cvsm vsm ',\n",
       " 'comprenant ontologie descriptifs axiome algèbre ',\n",
       " 'nk immunitaire artificiel système killer ',\n",
       " \"d'utilisateurs enrichissement sig accomplie offert \",\n",
       " 'vote opinion classification thématiquement jugement ',\n",
       " 'somerdfs mappings mediad telecom découverte ',\n",
       " 'boosting ensembliste finale inspirée supervisée ',\n",
       " 'conformité contrôle norme c3r bâtiment ',\n",
       " 'j-mesure élaguer chronique orientée modèle ',\n",
       " 'treillis navigation conceptuel concept niveau ',\n",
       " 'adaboost bruitées boosting agrégation face ',\n",
       " 'intervalle divisive homogène partie poisson ',\n",
       " 'relation répartie prédicats-arguments prédicatives recherchée ',\n",
       " 'diffusion liste faq cop sémantique ',\n",
       " 'svm intelligibilité boite actionnables noir ',\n",
       " 'augmentation capacité progresse lourde focalisation ',\n",
       " 'cube olap intégration prédiction ',\n",
       " 'brevet patent unifiées offices invention ',\n",
       " 'ancien français séquentiels extrait motif ',\n",
       " 'vie parcours méthodologie suisse événement ',\n",
       " 'page site étape descripteur textuelle ',\n",
       " 'ressource alignement entité appariement règle ',\n",
       " 'archéologiques navigation annotation approximatifs implémentée ',\n",
       " 'colonne annotation symbolique numériques floue ',\n",
       " 'banc pmms npc nasopharynx impliqués ',\n",
       " 'moteur entreprise géo-localisation jaune activité ',\n",
       " 'émotion homme-machine redirection dialogue automatisés ',\n",
       " 'ordonnancement semi-supervisé bénéfiques non–étiquetées cacm ',\n",
       " 'étiquetées topologie delaunay euclidien génératif ',\n",
       " 'cancer profil non-cancer npc épidémiologique ',\n",
       " 'réconciliation schéma logique référence picsel ',\n",
       " 'cube visualisation pixel orientée calcul ',\n",
       " 'temporisées conversation transition service logs ',\n",
       " 'détenu organisation connaissance relatif acteur ',\n",
       " \"règle implication conclusion validation s'agit \",\n",
       " 'xml document classement item recherche ',\n",
       " 'distribution coupure emplacement infinie fonctionnelles ',\n",
       " 'ls-svm boosting grand machine ensemble ',\n",
       " 'séquence biologiques substitution protéiques matrice ',\n",
       " 'quelques-unes espérant dis- relevées model-based ',\n",
       " 'classement ct-svm modèle mixtes carte ',\n",
       " 'al ancrer herrmann seeme cahier ',\n",
       " 'construction lieu texte treillis formelle ',\n",
       " 'comportement raison site résumé jour ',\n",
       " 'fourmi graphe artificielles incrémentale voisinage ',\n",
       " 'datées événement discrets fabrication supervision ',\n",
       " 'oubli spécification fonction entrepôt intelligentes ',\n",
       " 'consommation abonné téléphonie détermination flou ',\n",
       " 'carte prédiction pertinente volumineuses lié ',\n",
       " 'conceptuelle inertie sens tableau implique-t-il ',\n",
       " 'préparation protocole fiabilité représentation métrique ',\n",
       " 'inconsistance annotation sémantique ontologie évolution ',\n",
       " 'floue proximité terme avantageux modèle ',\n",
       " 'document collection liste entité espérons ',\n",
       " 'aca adaptation connaissance cas cabamaka ',\n",
       " 'retroweb décrit internet migrateur dé\\x02nies ',\n",
       " 'divergentes convergente multidimensionnelles m2s_cd srikant.même ',\n",
       " \"approximer-et-pousser top-k contrainte motif d'intérêt \",\n",
       " 'webangels filter violent structurel textuel ',\n",
       " 'motif sous-classe quelconques découverte conjonctive ',\n",
       " 'médicales image annotation région fusion ',\n",
       " 'patron couple amorce phrase liste ',\n",
       " \"entrepôt hiérarchie si-alors représentons l'entrepôt \",\n",
       " 'interestingness measures mining and in ',\n",
       " 'communauté pratique émergence échange membre ',\n",
       " 'sdet géographiques enrichissement enrichment supplément ',\n",
       " 'itemsets concises essentiel fermeture représentation ',\n",
       " 'milieu catégorisation indice adapté difficile ',\n",
       " \"entropie mesure l'échantillon insensibilité transmission \",\n",
       " 'symétrique déséquilibré modalité bâtir endogène ',\n",
       " 'thermique compact ajustés rc renforcé ',\n",
       " 'polygone appariement fodomust géographiques objet ',\n",
       " 'ip instantanées communication toip messagerie ',\n",
       " 'recouvrement classe applicatifs motivées part ',\n",
       " 'piecewise histogram is fisher function ',\n",
       " 'fort diamètre degré faible réseau ',\n",
       " 'exception possiblement surprenantes utile simultanée ',\n",
       " 'privées service remplaçabilité protocole conversation ',\n",
       " 'ras annotation citation outil document ',\n",
       " 'étape ré-ordonnancement candidat document transformation ',\n",
       " 'vidéo acp espace bloc dimension ',\n",
       " 'sonar imperfection imprécision théorie milieu ',\n",
       " \"vecteur thématique segmentation deft'06 recherchant \",\n",
       " \"instantané – global contexte c'est-à-dire \",\n",
       " 'k-faibles sous-bases valides confiance sens ',\n",
       " 'incomplètes contiennent temporairement représentative occultées ',\n",
       " 'rdf graphe non-contradiction contredisent imposons ',\n",
       " 'électrique consommation sgfd gestion flux ',\n",
       " 'web logarithme fichier serveur site ',\n",
       " 'multi-agent construction ontologie centralisé mouvance ',\n",
       " 'motif utilisateur théorique collection confidentialité ',\n",
       " 'gène expression réseau tigr mev ',\n",
       " \"guidé phrase tirons n'utilisons segmenteur \",\n",
       " 'états série bioprocédé temporelles singularité ',\n",
       " 'estimateur rang prédicteurs univarié densité ',\n",
       " 'kilomètre sociotechnique vision précisément décrit ',\n",
       " 'neurone multicouches architecture couche choix ',\n",
       " 'nexi xquery extension intégrer xml ',\n",
       " 'sortie interpréter importance interprétation variable ',\n",
       " 'variable exogène paire partitionnement exogènes ',\n",
       " 'fcm dca dc floue programmation ',\n",
       " 'alignement owl-lite ontologie circularité remédiant ',\n",
       " \"-- > règle d'exception implicative \",\n",
       " 'catégorisation wordnet multilingue monolingue consacré ',\n",
       " 'bootstrap réplication total corrige auto-organisées ',\n",
       " 'multi-agents clustering permanence dynamique évolutives ',\n",
       " 'cerveau irm anatomiques humain image ',\n",
       " 'projet b-ontology connaissance extraction aperçu ',\n",
       " 'motif borne condensée représentation séquentiels ',\n",
       " \"règle '' `` association visualisation \",\n",
       " 'appui tulip interactive graphe visualisation ',\n",
       " 'arbre décision résultat compréhension focus+context ',\n",
       " 'webdocenrich enrichissement document html semi-structurés ',\n",
       " 'écouter audio accès résumé réduirela ',\n",
       " 'intervalle noeud terminal fils kolmogorov-smirnov ',\n",
       " 'non-pertinence hospitalier composante gestion visualisation ',\n",
       " 'dimension semi-interactif atypiques génétique simplementdes ',\n",
       " 'implication alignement desmesures asymétrique concept ',\n",
       " 'signature indicateur financier courte auxstratégies ',\n",
       " ...]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On résume un article (titre + abstract) à partir des 5 mots les plus importants.\n",
    "best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calcul du LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le LSA va calculer, pour un document donné, les documents les plus similaires à son sujet. On classe le résultat le plus similaire dans le cluster du document donné. On reproduit cette étape et on arrête quand à un nombre de clusters ou une similarité trop faible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En gros, le LSA nous fournit une similarité qu'on utilise pour former une classification hiérarchique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On aura donc des clusters par sujets. On pourra élaborer nos techniques de datation pour déterminer les tendances et, si possible, créer de nouveaux clusters (avec les K-moyennes) plus précis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vectorisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Le tf-idf ne sert à rien d'autre que de convertir notre liste en un vecteur utilisable.\n",
    "vectorizer = TfidfVectorizer(min_df =1)\n",
    "\n",
    "dtm = vectorizer.fit_transform(best)\n",
    "\n",
    "## Document Term Matrix vers Latent Sementic Analysis\n",
    "lsa = TruncatedSVD(2, algorithm = 'randomized')\n",
    "\n",
    "dtm_lsa = lsa.fit_transform(dtm)\n",
    "\n",
    "dtm_lsa = Normalizer(copy=False).fit_transform(dtm_lsa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calcul de similarité des Documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>idéo2017 citoyen politiques tweets candidat</th>\n",
       "      <th>co-clustering modl gagne taille passent</th>\n",
       "      <th>recommandation mobilité factorisation inféré visités</th>\n",
       "      <th>arabe sentiment commentaire marocain dialectal</th>\n",
       "      <th>cds résumé texte composant rôle</th>\n",
       "      <th>big scénario ontologique data contexte</th>\n",
       "      <th>tentative décès supervisées patient risque</th>\n",
       "      <th>formés majoritaire vote global obtenus</th>\n",
       "      <th>label heureux musique morceau co-occurrence</th>\n",
       "      <th>a/b test procédure stratification contextuel</th>\n",
       "      <th>...</th>\n",
       "      <th>flou sous-ensemble appelons défini l'ontologie</th>\n",
       "      <th>uitliation documentaire fondé contenu aide</th>\n",
       "      <th>nextclosure fermé partitionnement itemsets treillis</th>\n",
       "      <th>incomplets classement probabiliste arbre décision</th>\n",
       "      <th>galois treillis supervisée étude basée</th>\n",
       "      <th>appropriation mask mémoires savoir-faire entreprise</th>\n",
       "      <th>voisinage graphe prétraitement voisin proximité</th>\n",
       "      <th>conceptuels graphe connaissance contrainte validation</th>\n",
       "      <th>veille technologique texte rechercher scientifique</th>\n",
       "      <th>entrepôt naturel risque examinons volumineux</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>idéo2017 citoyen politiques tweets candidat</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.989824</td>\n",
       "      <td>0.968987</td>\n",
       "      <td>0.927127</td>\n",
       "      <td>0.723651</td>\n",
       "      <td>0.695330</td>\n",
       "      <td>0.924563</td>\n",
       "      <td>0.722175</td>\n",
       "      <td>0.867517</td>\n",
       "      <td>0.726303</td>\n",
       "      <td>...</td>\n",
       "      <td>0.750095</td>\n",
       "      <td>0.735816</td>\n",
       "      <td>0.842486</td>\n",
       "      <td>0.840003</td>\n",
       "      <td>0.791061</td>\n",
       "      <td>0.760180</td>\n",
       "      <td>0.563010</td>\n",
       "      <td>0.705331</td>\n",
       "      <td>0.831147</td>\n",
       "      <td>0.639846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>co-clustering modl gagne taille passent</th>\n",
       "      <td>0.989824</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.994290</td>\n",
       "      <td>0.971018</td>\n",
       "      <td>0.814497</td>\n",
       "      <td>0.585986</td>\n",
       "      <td>0.969375</td>\n",
       "      <td>0.813256</td>\n",
       "      <td>0.929470</td>\n",
       "      <td>0.816725</td>\n",
       "      <td>...</td>\n",
       "      <td>0.836568</td>\n",
       "      <td>0.824691</td>\n",
       "      <td>0.910572</td>\n",
       "      <td>0.908664</td>\n",
       "      <td>0.870061</td>\n",
       "      <td>0.844898</td>\n",
       "      <td>0.674884</td>\n",
       "      <td>0.799026</td>\n",
       "      <td>0.901815</td>\n",
       "      <td>0.742692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>recommandation mobilité factorisation inféré visités</th>\n",
       "      <td>0.968987</td>\n",
       "      <td>0.994290</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.990979</td>\n",
       "      <td>0.871757</td>\n",
       "      <td>0.496168</td>\n",
       "      <td>0.990047</td>\n",
       "      <td>0.870709</td>\n",
       "      <td>0.963529</td>\n",
       "      <td>0.873638</td>\n",
       "      <td>...</td>\n",
       "      <td>0.890255</td>\n",
       "      <td>0.880337</td>\n",
       "      <td>0.949482</td>\n",
       "      <td>0.948031</td>\n",
       "      <td>0.917696</td>\n",
       "      <td>0.897157</td>\n",
       "      <td>0.749776</td>\n",
       "      <td>0.858630</td>\n",
       "      <td>0.942778</td>\n",
       "      <td>0.809910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>arabe sentiment commentaire marocain dialectal</th>\n",
       "      <td>0.927127</td>\n",
       "      <td>0.971018</td>\n",
       "      <td>0.990979</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.929554</td>\n",
       "      <td>0.375332</td>\n",
       "      <td>0.999977</td>\n",
       "      <td>0.928765</td>\n",
       "      <td>0.990701</td>\n",
       "      <td>0.930967</td>\n",
       "      <td>...</td>\n",
       "      <td>0.943265</td>\n",
       "      <td>0.935967</td>\n",
       "      <td>0.982975</td>\n",
       "      <td>0.982121</td>\n",
       "      <td>0.962661</td>\n",
       "      <td>0.948262</td>\n",
       "      <td>0.831693</td>\n",
       "      <td>0.919582</td>\n",
       "      <td>0.978958</td>\n",
       "      <td>0.881214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cds résumé texte composant rôle</th>\n",
       "      <td>0.723651</td>\n",
       "      <td>0.814497</td>\n",
       "      <td>0.871757</td>\n",
       "      <td>0.929554</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.007160</td>\n",
       "      <td>0.932034</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.971073</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999235</td>\n",
       "      <td>0.999842</td>\n",
       "      <td>0.981471</td>\n",
       "      <td>0.982340</td>\n",
       "      <td>0.994652</td>\n",
       "      <td>0.998515</td>\n",
       "      <td>0.977811</td>\n",
       "      <td>0.999657</td>\n",
       "      <td>0.985229</td>\n",
       "      <td>0.993420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>big scénario ontologique data contexte</th>\n",
       "      <td>0.695330</td>\n",
       "      <td>0.585986</td>\n",
       "      <td>0.496168</td>\n",
       "      <td>0.375332</td>\n",
       "      <td>0.007160</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.369035</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>0.245730</td>\n",
       "      <td>0.011011</td>\n",
       "      <td>...</td>\n",
       "      <td>0.046272</td>\n",
       "      <td>0.024952</td>\n",
       "      <td>0.198635</td>\n",
       "      <td>0.194131</td>\n",
       "      <td>0.110400</td>\n",
       "      <td>0.061635</td>\n",
       "      <td>-0.202483</td>\n",
       "      <td>-0.019026</td>\n",
       "      <td>0.178292</td>\n",
       "      <td>-0.107411</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tentative décès supervisées patient risque</th>\n",
       "      <td>0.924563</td>\n",
       "      <td>0.969375</td>\n",
       "      <td>0.990047</td>\n",
       "      <td>0.999977</td>\n",
       "      <td>0.932034</td>\n",
       "      <td>0.369035</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.931258</td>\n",
       "      <td>0.991601</td>\n",
       "      <td>0.933423</td>\n",
       "      <td>...</td>\n",
       "      <td>0.945496</td>\n",
       "      <td>0.938334</td>\n",
       "      <td>0.984199</td>\n",
       "      <td>0.983375</td>\n",
       "      <td>0.964476</td>\n",
       "      <td>0.950394</td>\n",
       "      <td>0.835440</td>\n",
       "      <td>0.922226</td>\n",
       "      <td>0.980320</td>\n",
       "      <td>0.884400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>formés majoritaire vote global obtenus</th>\n",
       "      <td>0.722175</td>\n",
       "      <td>0.813256</td>\n",
       "      <td>0.870709</td>\n",
       "      <td>0.928765</td>\n",
       "      <td>0.999998</td>\n",
       "      <td>0.005025</td>\n",
       "      <td>0.931258</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.970561</td>\n",
       "      <td>0.999982</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999149</td>\n",
       "      <td>0.999801</td>\n",
       "      <td>0.981059</td>\n",
       "      <td>0.981939</td>\n",
       "      <td>0.994429</td>\n",
       "      <td>0.998396</td>\n",
       "      <td>0.978256</td>\n",
       "      <td>0.999711</td>\n",
       "      <td>0.984861</td>\n",
       "      <td>0.993662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label heureux musique morceau co-occurrence</th>\n",
       "      <td>0.867517</td>\n",
       "      <td>0.929470</td>\n",
       "      <td>0.963529</td>\n",
       "      <td>0.990701</td>\n",
       "      <td>0.971073</td>\n",
       "      <td>0.245730</td>\n",
       "      <td>0.991601</td>\n",
       "      <td>0.970561</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.971986</td>\n",
       "      <td>...</td>\n",
       "      <td>0.979671</td>\n",
       "      <td>0.975168</td>\n",
       "      <td>0.998833</td>\n",
       "      <td>0.998601</td>\n",
       "      <td>0.990542</td>\n",
       "      <td>0.982641</td>\n",
       "      <td>0.899503</td>\n",
       "      <td>0.964488</td>\n",
       "      <td>0.997619</td>\n",
       "      <td>0.937337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>a/b test procédure stratification contextuel</th>\n",
       "      <td>0.726303</td>\n",
       "      <td>0.816725</td>\n",
       "      <td>0.873638</td>\n",
       "      <td>0.930967</td>\n",
       "      <td>0.999993</td>\n",
       "      <td>0.011011</td>\n",
       "      <td>0.933423</td>\n",
       "      <td>0.999982</td>\n",
       "      <td>0.971986</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.999378</td>\n",
       "      <td>0.999903</td>\n",
       "      <td>0.982201</td>\n",
       "      <td>0.983054</td>\n",
       "      <td>0.995043</td>\n",
       "      <td>0.998717</td>\n",
       "      <td>0.976997</td>\n",
       "      <td>0.999549</td>\n",
       "      <td>0.985881</td>\n",
       "      <td>0.992972</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 1269 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    idéo2017 citoyen politiques tweets candidat   \\\n",
       "idéo2017 citoyen politiques tweets candidat                                             1.000000   \n",
       "co-clustering modl gagne taille passent                                                 0.989824   \n",
       "recommandation mobilité factorisation inféré vi...                                      0.968987   \n",
       "arabe sentiment commentaire marocain dialectal                                          0.927127   \n",
       "cds résumé texte composant rôle                                                         0.723651   \n",
       "big scénario ontologique data contexte                                                  0.695330   \n",
       "tentative décès supervisées patient risque                                              0.924563   \n",
       "formés majoritaire vote global obtenus                                                  0.722175   \n",
       "label heureux musique morceau co-occurrence                                             0.867517   \n",
       "a/b test procédure stratification contextuel                                            0.726303   \n",
       "\n",
       "                                                    co-clustering modl gagne taille passent   \\\n",
       "idéo2017 citoyen politiques tweets candidat                                         0.989824   \n",
       "co-clustering modl gagne taille passent                                             1.000000   \n",
       "recommandation mobilité factorisation inféré vi...                                  0.994290   \n",
       "arabe sentiment commentaire marocain dialectal                                      0.971018   \n",
       "cds résumé texte composant rôle                                                     0.814497   \n",
       "big scénario ontologique data contexte                                              0.585986   \n",
       "tentative décès supervisées patient risque                                          0.969375   \n",
       "formés majoritaire vote global obtenus                                              0.813256   \n",
       "label heureux musique morceau co-occurrence                                         0.929470   \n",
       "a/b test procédure stratification contextuel                                        0.816725   \n",
       "\n",
       "                                                    recommandation mobilité factorisation inféré visités   \\\n",
       "idéo2017 citoyen politiques tweets candidat                                                  0.968987       \n",
       "co-clustering modl gagne taille passent                                                      0.994290       \n",
       "recommandation mobilité factorisation inféré vi...                                           1.000000       \n",
       "arabe sentiment commentaire marocain dialectal                                               0.990979       \n",
       "cds résumé texte composant rôle                                                              0.871757       \n",
       "big scénario ontologique data contexte                                                       0.496168       \n",
       "tentative décès supervisées patient risque                                                   0.990047       \n",
       "formés majoritaire vote global obtenus                                                       0.870709       \n",
       "label heureux musique morceau co-occurrence                                                  0.963529       \n",
       "a/b test procédure stratification contextuel                                                 0.873638       \n",
       "\n",
       "                                                    arabe sentiment commentaire marocain dialectal   \\\n",
       "idéo2017 citoyen politiques tweets candidat                                                0.927127   \n",
       "co-clustering modl gagne taille passent                                                    0.971018   \n",
       "recommandation mobilité factorisation inféré vi...                                         0.990979   \n",
       "arabe sentiment commentaire marocain dialectal                                             1.000000   \n",
       "cds résumé texte composant rôle                                                            0.929554   \n",
       "big scénario ontologique data contexte                                                     0.375332   \n",
       "tentative décès supervisées patient risque                                                 0.999977   \n",
       "formés majoritaire vote global obtenus                                                     0.928765   \n",
       "label heureux musique morceau co-occurrence                                                0.990701   \n",
       "a/b test procédure stratification contextuel                                               0.930967   \n",
       "\n",
       "                                                    cds résumé texte composant rôle   \\\n",
       "idéo2017 citoyen politiques tweets candidat                                 0.723651   \n",
       "co-clustering modl gagne taille passent                                     0.814497   \n",
       "recommandation mobilité factorisation inféré vi...                          0.871757   \n",
       "arabe sentiment commentaire marocain dialectal                              0.929554   \n",
       "cds résumé texte composant rôle                                             1.000000   \n",
       "big scénario ontologique data contexte                                      0.007160   \n",
       "tentative décès supervisées patient risque                                  0.932034   \n",
       "formés majoritaire vote global obtenus                                      0.999998   \n",
       "label heureux musique morceau co-occurrence                                 0.971073   \n",
       "a/b test procédure stratification contextuel                                0.999993   \n",
       "\n",
       "                                                    big scénario ontologique data contexte   \\\n",
       "idéo2017 citoyen politiques tweets candidat                                        0.695330   \n",
       "co-clustering modl gagne taille passent                                            0.585986   \n",
       "recommandation mobilité factorisation inféré vi...                                 0.496168   \n",
       "arabe sentiment commentaire marocain dialectal                                     0.375332   \n",
       "cds résumé texte composant rôle                                                    0.007160   \n",
       "big scénario ontologique data contexte                                             1.000000   \n",
       "tentative décès supervisées patient risque                                         0.369035   \n",
       "formés majoritaire vote global obtenus                                             0.005025   \n",
       "label heureux musique morceau co-occurrence                                        0.245730   \n",
       "a/b test procédure stratification contextuel                                       0.011011   \n",
       "\n",
       "                                                    tentative décès supervisées patient risque   \\\n",
       "idéo2017 citoyen politiques tweets candidat                                            0.924563   \n",
       "co-clustering modl gagne taille passent                                                0.969375   \n",
       "recommandation mobilité factorisation inféré vi...                                     0.990047   \n",
       "arabe sentiment commentaire marocain dialectal                                         0.999977   \n",
       "cds résumé texte composant rôle                                                        0.932034   \n",
       "big scénario ontologique data contexte                                                 0.369035   \n",
       "tentative décès supervisées patient risque                                             1.000000   \n",
       "formés majoritaire vote global obtenus                                                 0.931258   \n",
       "label heureux musique morceau co-occurrence                                            0.991601   \n",
       "a/b test procédure stratification contextuel                                           0.933423   \n",
       "\n",
       "                                                    formés majoritaire vote global obtenus   \\\n",
       "idéo2017 citoyen politiques tweets candidat                                        0.722175   \n",
       "co-clustering modl gagne taille passent                                            0.813256   \n",
       "recommandation mobilité factorisation inféré vi...                                 0.870709   \n",
       "arabe sentiment commentaire marocain dialectal                                     0.928765   \n",
       "cds résumé texte composant rôle                                                    0.999998   \n",
       "big scénario ontologique data contexte                                             0.005025   \n",
       "tentative décès supervisées patient risque                                         0.931258   \n",
       "formés majoritaire vote global obtenus                                             1.000000   \n",
       "label heureux musique morceau co-occurrence                                        0.970561   \n",
       "a/b test procédure stratification contextuel                                       0.999982   \n",
       "\n",
       "                                                    label heureux musique morceau co-occurrence   \\\n",
       "idéo2017 citoyen politiques tweets candidat                                             0.867517   \n",
       "co-clustering modl gagne taille passent                                                 0.929470   \n",
       "recommandation mobilité factorisation inféré vi...                                      0.963529   \n",
       "arabe sentiment commentaire marocain dialectal                                          0.990701   \n",
       "cds résumé texte composant rôle                                                         0.971073   \n",
       "big scénario ontologique data contexte                                                  0.245730   \n",
       "tentative décès supervisées patient risque                                              0.991601   \n",
       "formés majoritaire vote global obtenus                                                  0.970561   \n",
       "label heureux musique morceau co-occurrence                                             1.000000   \n",
       "a/b test procédure stratification contextuel                                            0.971986   \n",
       "\n",
       "                                                    a/b test procédure stratification contextuel   \\\n",
       "idéo2017 citoyen politiques tweets candidat                                              0.726303   \n",
       "co-clustering modl gagne taille passent                                                  0.816725   \n",
       "recommandation mobilité factorisation inféré vi...                                       0.873638   \n",
       "arabe sentiment commentaire marocain dialectal                                           0.930967   \n",
       "cds résumé texte composant rôle                                                          0.999993   \n",
       "big scénario ontologique data contexte                                                   0.011011   \n",
       "tentative décès supervisées patient risque                                               0.933423   \n",
       "formés majoritaire vote global obtenus                                                   0.999982   \n",
       "label heureux musique morceau co-occurrence                                              0.971986   \n",
       "a/b test procédure stratification contextuel                                             1.000000   \n",
       "\n",
       "                                                                        ...                        \\\n",
       "idéo2017 citoyen politiques tweets candidat                             ...                         \n",
       "co-clustering modl gagne taille passent                                 ...                         \n",
       "recommandation mobilité factorisation inféré vi...                      ...                         \n",
       "arabe sentiment commentaire marocain dialectal                          ...                         \n",
       "cds résumé texte composant rôle                                         ...                         \n",
       "big scénario ontologique data contexte                                  ...                         \n",
       "tentative décès supervisées patient risque                              ...                         \n",
       "formés majoritaire vote global obtenus                                  ...                         \n",
       "label heureux musique morceau co-occurrence                             ...                         \n",
       "a/b test procédure stratification contextuel                            ...                         \n",
       "\n",
       "                                                    flou sous-ensemble appelons défini l'ontologie   \\\n",
       "idéo2017 citoyen politiques tweets candidat                                                0.750095   \n",
       "co-clustering modl gagne taille passent                                                    0.836568   \n",
       "recommandation mobilité factorisation inféré vi...                                         0.890255   \n",
       "arabe sentiment commentaire marocain dialectal                                             0.943265   \n",
       "cds résumé texte composant rôle                                                            0.999235   \n",
       "big scénario ontologique data contexte                                                     0.046272   \n",
       "tentative décès supervisées patient risque                                                 0.945496   \n",
       "formés majoritaire vote global obtenus                                                     0.999149   \n",
       "label heureux musique morceau co-occurrence                                                0.979671   \n",
       "a/b test procédure stratification contextuel                                               0.999378   \n",
       "\n",
       "                                                    uitliation documentaire fondé contenu aide   \\\n",
       "idéo2017 citoyen politiques tweets candidat                                            0.735816   \n",
       "co-clustering modl gagne taille passent                                                0.824691   \n",
       "recommandation mobilité factorisation inféré vi...                                     0.880337   \n",
       "arabe sentiment commentaire marocain dialectal                                         0.935967   \n",
       "cds résumé texte composant rôle                                                        0.999842   \n",
       "big scénario ontologique data contexte                                                 0.024952   \n",
       "tentative décès supervisées patient risque                                             0.938334   \n",
       "formés majoritaire vote global obtenus                                                 0.999801   \n",
       "label heureux musique morceau co-occurrence                                            0.975168   \n",
       "a/b test procédure stratification contextuel                                           0.999903   \n",
       "\n",
       "                                                    nextclosure fermé partitionnement itemsets treillis   \\\n",
       "idéo2017 citoyen politiques tweets candidat                                                  0.842486      \n",
       "co-clustering modl gagne taille passent                                                      0.910572      \n",
       "recommandation mobilité factorisation inféré vi...                                           0.949482      \n",
       "arabe sentiment commentaire marocain dialectal                                               0.982975      \n",
       "cds résumé texte composant rôle                                                              0.981471      \n",
       "big scénario ontologique data contexte                                                       0.198635      \n",
       "tentative décès supervisées patient risque                                                   0.984199      \n",
       "formés majoritaire vote global obtenus                                                       0.981059      \n",
       "label heureux musique morceau co-occurrence                                                  0.998833      \n",
       "a/b test procédure stratification contextuel                                                 0.982201      \n",
       "\n",
       "                                                    incomplets classement probabiliste arbre décision   \\\n",
       "idéo2017 citoyen politiques tweets candidat                                                  0.840003    \n",
       "co-clustering modl gagne taille passent                                                      0.908664    \n",
       "recommandation mobilité factorisation inféré vi...                                           0.948031    \n",
       "arabe sentiment commentaire marocain dialectal                                               0.982121    \n",
       "cds résumé texte composant rôle                                                              0.982340    \n",
       "big scénario ontologique data contexte                                                       0.194131    \n",
       "tentative décès supervisées patient risque                                                   0.983375    \n",
       "formés majoritaire vote global obtenus                                                       0.981939    \n",
       "label heureux musique morceau co-occurrence                                                  0.998601    \n",
       "a/b test procédure stratification contextuel                                                 0.983054    \n",
       "\n",
       "                                                    galois treillis supervisée étude basée   \\\n",
       "idéo2017 citoyen politiques tweets candidat                                        0.791061   \n",
       "co-clustering modl gagne taille passent                                            0.870061   \n",
       "recommandation mobilité factorisation inféré vi...                                 0.917696   \n",
       "arabe sentiment commentaire marocain dialectal                                     0.962661   \n",
       "cds résumé texte composant rôle                                                    0.994652   \n",
       "big scénario ontologique data contexte                                             0.110400   \n",
       "tentative décès supervisées patient risque                                         0.964476   \n",
       "formés majoritaire vote global obtenus                                             0.994429   \n",
       "label heureux musique morceau co-occurrence                                        0.990542   \n",
       "a/b test procédure stratification contextuel                                       0.995043   \n",
       "\n",
       "                                                    appropriation mask mémoires savoir-faire entreprise   \\\n",
       "idéo2017 citoyen politiques tweets candidat                                                  0.760180      \n",
       "co-clustering modl gagne taille passent                                                      0.844898      \n",
       "recommandation mobilité factorisation inféré vi...                                           0.897157      \n",
       "arabe sentiment commentaire marocain dialectal                                               0.948262      \n",
       "cds résumé texte composant rôle                                                              0.998515      \n",
       "big scénario ontologique data contexte                                                       0.061635      \n",
       "tentative décès supervisées patient risque                                                   0.950394      \n",
       "formés majoritaire vote global obtenus                                                       0.998396      \n",
       "label heureux musique morceau co-occurrence                                                  0.982641      \n",
       "a/b test procédure stratification contextuel                                                 0.998717      \n",
       "\n",
       "                                                    voisinage graphe prétraitement voisin proximité   \\\n",
       "idéo2017 citoyen politiques tweets candidat                                                 0.563010   \n",
       "co-clustering modl gagne taille passent                                                     0.674884   \n",
       "recommandation mobilité factorisation inféré vi...                                          0.749776   \n",
       "arabe sentiment commentaire marocain dialectal                                              0.831693   \n",
       "cds résumé texte composant rôle                                                             0.977811   \n",
       "big scénario ontologique data contexte                                                     -0.202483   \n",
       "tentative décès supervisées patient risque                                                  0.835440   \n",
       "formés majoritaire vote global obtenus                                                      0.978256   \n",
       "label heureux musique morceau co-occurrence                                                 0.899503   \n",
       "a/b test procédure stratification contextuel                                                0.976997   \n",
       "\n",
       "                                                    conceptuels graphe connaissance contrainte validation   \\\n",
       "idéo2017 citoyen politiques tweets candidat                                                  0.705331        \n",
       "co-clustering modl gagne taille passent                                                      0.799026        \n",
       "recommandation mobilité factorisation inféré vi...                                           0.858630        \n",
       "arabe sentiment commentaire marocain dialectal                                               0.919582        \n",
       "cds résumé texte composant rôle                                                              0.999657        \n",
       "big scénario ontologique data contexte                                                      -0.019026        \n",
       "tentative décès supervisées patient risque                                                   0.922226        \n",
       "formés majoritaire vote global obtenus                                                       0.999711        \n",
       "label heureux musique morceau co-occurrence                                                  0.964488        \n",
       "a/b test procédure stratification contextuel                                                 0.999549        \n",
       "\n",
       "                                                    veille technologique texte rechercher scientifique   \\\n",
       "idéo2017 citoyen politiques tweets candidat                                                  0.831147     \n",
       "co-clustering modl gagne taille passent                                                      0.901815     \n",
       "recommandation mobilité factorisation inféré vi...                                           0.942778     \n",
       "arabe sentiment commentaire marocain dialectal                                               0.978958     \n",
       "cds résumé texte composant rôle                                                              0.985229     \n",
       "big scénario ontologique data contexte                                                       0.178292     \n",
       "tentative décès supervisées patient risque                                                   0.980320     \n",
       "formés majoritaire vote global obtenus                                                       0.984861     \n",
       "label heureux musique morceau co-occurrence                                                  0.997619     \n",
       "a/b test procédure stratification contextuel                                                 0.985881     \n",
       "\n",
       "                                                    entrepôt naturel risque examinons volumineux   \n",
       "idéo2017 citoyen politiques tweets candidat                                              0.639846  \n",
       "co-clustering modl gagne taille passent                                                  0.742692  \n",
       "recommandation mobilité factorisation inféré vi...                                       0.809910  \n",
       "arabe sentiment commentaire marocain dialectal                                           0.881214  \n",
       "cds résumé texte composant rôle                                                          0.993420  \n",
       "big scénario ontologique data contexte                                                  -0.107411  \n",
       "tentative décès supervisées patient risque                                               0.884400  \n",
       "formés majoritaire vote global obtenus                                                   0.993662  \n",
       "label heureux musique morceau co-occurrence                                              0.937337  \n",
       "a/b test procédure stratification contextuel                                             0.992972  \n",
       "\n",
       "[10 rows x 1269 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# On calcule la similarité des documents à l'aide du LSA.\n",
    "\n",
    "similarity = np.asarray(np.asmatrix(dtm_lsa) * np.asmatrix(dtm_lsa).T)\n",
    "\n",
    "pd.DataFrame(similarity, index=best, columns=best).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Hiérarchique Ascendant (HAC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le vrai point d'intérêt de cet algorithme est le seuil de similarité qui servira de point d'arrêt.\n",
    "\n",
    "On va répartir toutes les données réparties précédemment en clusters aux sujets similaires. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On choisit ici le nombre de clusters à utiliser.\n",
    "n_clusters = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On applique le HAC pour créer nos clusters.\n",
    "clusters = AgglomerativeClustering(n_clusters=n_clusters).fit_predict(similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_list = []\n",
    "for i in range(n_clusters):\n",
    "    cluster_list.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(clusters)):\n",
    "    try:\n",
    "        cluster_list[clusters[i]].append({best[i]:doc[\"year\"][i]})\n",
    "    except:\n",
    "        print(\"Error while filling clusters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57, 7, 2, 35, 29, 9, 15, 13, 23, 9, 7, 46, 54, 21, 44, 6, 37, 7, 9, 2, 27, 19, 22, 11, 32, 6, 2, 30, 3, 22, 42, 40, 26, 28, 9, 10, 7, 3, 5, 6, 2, 36, 4, 12, 12, 5, 1, 14, 2, 1, 9, 6, 2, 4, 1, 29, 5, 5, 22, 1, 18, 4, 22, 5, 17, 2, 5, 21, 1, 1, 5, 22, 8, 4, 19, 13, 24, 10, 3, 1, 1, 1, 1, 4, 21, 3, 12, 15, 16, 2, 1, 3, 8, 1, 1, 1, 15, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print([len(x) for x in cluster_list])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Notes\n",
    "Il va falloir affiner les clusters en les résumant en 5 mots-clés."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Travail temporel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va classer les clusters par tendances. C'est à dire que l'on va finir avec cinq classes contenant des clusters.\n",
    "\n",
    "Ces classes représenteront des sujets actuels ou abandonnés, d'origine récente ou ancienne.\n",
    "\n",
    "De manière évidente, un sujet qui est récent ou qui parle des tendances actuelles aura plus de chances de se retrouver dans les sujets de l'EGC 2020. A l'inverse, les sujets vieux ou dont on ne parle plus depuis un moment ont peu de chances de revenir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Un cluster temporel ne contient que les années qui composent chaque cluster.\n",
    "# On crée autant de clusters temporel que de clusters crées précédemments.\n",
    "temporal_list = []\n",
    "for i in range(n_clusters):\n",
    "    temporal_list.append([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On récupère les valeurs des clusters\n",
    "n = 0\n",
    "for cluster in cluster_list:\n",
    "    for i in range(len(cluster)):\n",
    "        temporal_list[n].append(list(cluster[i].values()))\n",
    "    n += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On crée les clusters temporels à partir des dates contenues dans les clusters.\n",
    "def remplir(c, t):\n",
    "    for i in range(len(c)):\n",
    "        t.append(t[i][0])\n",
    "    t = t[int(len(t)/2):]\n",
    "    return t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_clusters):\n",
    "    temporal_list[i] = remplir(cluster_list[i], temporal_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[57, 7, 2, 35, 29, 9, 15, 13, 23, 9, 7, 46, 54, 21, 44, 6, 37, 7, 9, 2, 27, 19, 22, 11, 32, 6, 2, 30, 3, 22, 42, 40, 26, 28, 9, 10, 7, 3, 5, 6, 2, 36, 4, 12, 12, 5, 1, 14, 2, 1, 9, 6, 2, 4, 1, 29, 5, 5, 22, 1, 18, 4, 22, 5, 17, 2, 5, 21, 1, 1, 5, 22, 8, 4, 19, 13, 24, 10, 3, 1, 1, 1, 1, 4, 21, 3, 12, 15, 16, 2, 1, 3, 8, 1, 1, 1, 15, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "print([len(x) for x in temporal_list])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# L'évaluation temporelle utilise trois valeurs :\n",
    "# La date de l'article le plus vieux du cluster\n",
    "# La date de l'article le plus récent du cluster\n",
    "# La date moyenne des articles du cluster\n",
    "def minYear(t):\n",
    "    return np.min(t)\n",
    "\n",
    "def maxYear(t):\n",
    "    return np.max(t)\n",
    "\n",
    "def moyYear(t):\n",
    "    return int(np.mean(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On va classer ces clusters selon la combinaison des trois valeurs précédentes :\n",
    "# Si minYear proche de maxYear alors récent\n",
    "# Si maxYear proche de minYear alors ancien\n",
    "# Si moyYear proche de minYear alors ancien\n",
    "# Si moyYear proche de maxYear alors récent\n",
    "# Sinon moyen\n",
    "recent = []\n",
    "ancien = []\n",
    "moyen = []\n",
    "mort = []\n",
    "nouveau = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On calcule l'appartenance aux tendances selon le schéma décrit précédement.\n",
    "def classe(c, t):\n",
    "    if minYear(t) < 2011:\n",
    "        if maxYear(t) < 2011:\n",
    "            mort.append(c)\n",
    "        else:\n",
    "            if moyYear(t) < 2009:\n",
    "                ancien.append(c)\n",
    "            elif moyYear(t) > 2014:\n",
    "                recent.append(c)\n",
    "            else:\n",
    "                moyen.append(c)\n",
    "    elif minYear(t) > 2011:\n",
    "        nouveau.append(c)\n",
    "    else:\n",
    "        if moyYear(t) < 2011:\n",
    "            ancien.append(c)\n",
    "        elif moyYear(t) > 2011:\n",
    "            recent.append(c)\n",
    "        else:\n",
    "            moyen.append(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_clusters):\n",
    "    classe(cluster_list[i], temporal_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Recent: [[{'big scénario ontologique data contexte ': 2018}, {'long-range influence network social in ': 2018}, {'apps android through text features ': 2017}, {'/owl schema rdf to data ': 2015}, {'drift drifted dataset new sudden ': 2014}, {'logic non-relational relational to deduplication ': 2010}], [{'co-clustering modl gagne taille passent ': 2018}, {'témoignage saisi rédigeant saisie préserver ': 2017}, {'recommandation utilisateur sociale préférence recevant ': 2015}, {'classe new mixture labeling examining ': 2015}, {'pertinence recommandation confusion intrinsèquement natif ': 2014}, {'profil utilisateur court-terme long-terme social ': 2011}], [{'protocole stockage libre cohérence cassandra ': 2016}, {'criterion similarity with two clustering ': 2011}], [{'waves dénotée potable souterrain décuplées ': 2017}, {'exhaustif professionnel guidé recommandation expert ': 2011}, {'naïf bayésien variable classifieur poids ': 2011}]]\n",
      "\n",
      "Ancien: [[{'label multilabel ... court interactif ': 2017}, {'évidentielles skyline évidentiel pareto incertaines ': 2015}, {'métrique étiquetage sélection variable textuelles ': 2014}, {'généralisée vraisemblance thématique press associated ': 2013}, {'som topologiques carte contrainte triviale ': 2011}, {'champignon ontologique expression séquence construction ': 2011}, {'simulation propagation publication reproduire détaillant ': 2011}, {'diagnostique codage partielle fusion hétérogènes ': 2011}, {\"fusion heuristiquesmises d'ex-périmentations opé-ration informa-tions \": 2009}, {'cognitives carte hiérarchiques concepteur cognitive ': 2008}, {'projet b-ontology connaissance extraction aperçu ': 2007}, {'dimension semi-interactif atypiques génétique simplementdes ': 2006}, {'pédagogiques compétence ressource gestion connaissance ': 2006}, {'adaptative temporel épisode patient version ': 2006}, {'elem lem continues variante lentement ': 2005}, {'équipe inria homogène xml thème ': 2005}, {'kohonen fonder prédicteurs carte supervisé ': 2005}, {'arbre décision construction variable ': 2004}, {'communauté reconnues période rendue utilisateur ': 2004}], [{'financement campagne montant participatif levé ': 2017}, {'phrase sémantique similarité noyau indicateur ': 2016}, {'régulation inférence réseau sortie interaction ': 2013}, {'pco strate multicouche spécialisation mixte ': 2013}, {'logs requête logarithme olap résumé ': 2011}, {'opinion médiocre vocable film expriment ': 2011}, {'traminer événement séquence analyse ': 2010}, {\"préférence contextuelles olap contexted'analyse lesdonnées \": 2009}, {'film opinion rapprochement montrerons vocabulaire ': 2008}, {'vidéo soft condensé computing longueur ': 2008}, {'afc image mot axe recherche ': 2008}, {'cartogramme spatiotemporelles ex dimension composante ': 2008}, {'réconciliation schéma logique référence picsel ': 2007}, {\"entrepôt hiérarchie si-alors représentons l'entrepôt \": 2007}, {'polygone appariement fodomust géographiques objet ': 2007}, {'ip instantanées communication toip messagerie ': 2007}, {'vidéo comportement indexationvidéo avonsdéveloppé comportementintra ': 2006}, {'bloom pair filtre pair-à-pair signature ': 2006}, {'parallèle distribué svm pc 20dimensions ': 2006}, {'sujet typicalité statistique implicative supplémentaires ': 2006}, {'incomplètes bayésiens ams-em actuellement capables ': 2005}, {'anthropocentrée hiérarchiques association recherche jouant ': 2004}], [{'ald poids multimédia discriminante modalité ': 2012}, {'cold-start collaboratives recommandation logs froid ': 2011}, {'prédictives variable prédire procédure identifiées ': 2009}, {'stream bordure statistique flot incrémentale ': 2006}, {'xml document structure orientent lining ': 2004}, {'document filtrage utilisateur textuels profil ': 2004}], [{'étiquetage thème pixel couverte classification ': 2014}, {'parallélisation cpu gpu radiale configuration ': 2013}, {'diamètre graphe erreur rapide commentés ': 2012}, {'cnss cellulaire induction neuro-symbolique neurone ': 2011}, {'image afc gpu parallèle incrémental ': 2010}, {'conflit théorie croyance imparfaites revenir ': 2010}, {'wcum site usage web analyse ': 2010}, {'investissement optimiser commercial opération régression ': 2009}, {'segmentation image pixel supervisée intensité ': 2009}, {'image segmentation évolutive pixel identifier ': 2008}, {'boosting ensembliste finale inspirée supervisée ': 2008}, {'page site étape descripteur textuelle ': 2008}, {'états série bioprocédé temporelles singularité ': 2007}, {\"-- > règle d'exception implicative \": 2007}, {\"règle '' `` association visualisation \": 2007}, {'légitimement décisionnel qualité règle intéressantes ': 2006}, {'maladie médicales association athérosclérose règles.nous ': 2005}, {'markov chaîne ordre arborescent paru ': 2005}, {'règle significatives multitude faux procédure ': 2004}, {'texte fractale 90 image géométrie ': 2004}, {'manipulation cube représentation donnée ': 2004}, {'bloc multidimensionnelles cube théorie recouvrir ': 2004}]]\n",
      "\n",
      "Moyen: [[{'nnms mean-shift k-plus scalable voisin ': 2018}, {'révolution captation implique assurance opportunité ': 2016}, {'mappings adaptation ontologie heuristique guidant ': 2015}, {'amazighe langue repérage nommées entité ': 2015}, {'alignement ontologie liées nalt agrovoc ': 2014}, {'volume diminue pondération descripteur influençant ': 2014}, {'humanité matériau numériques tenterons numérique ': 2014}, {'exception skyline point anomalie requête ': 2014}, {'élastique axe réduction matière complexité ': 2014}, {'catégorisation texte proche coût k-voisins ': 2013}, {\"routier trajectoire réseau contrainte l'empruntent \": 2013}, {'protéine espèce trimères coli escherichia ': 2013}, {'alignement choquet intégrale paramétrage ontologie ': 2013}, {'tca triadiques biclusters biclustering numériques ': 2012}, {'gof groupe outliers outlier factor ': 2012}, {'sous-parties généraliste ontologie enrichir focalisées ': 2012}, {'diffusion twitter individuel information multi-dimensionnelle ': 2012}, {'géolocalisation actualité accès faciliter résumé ': 2011}, {'géographiques vis ontologie entité globale ': 2011}, {'alignement enrichir ontologie taxomap topographie ': 2011}, {'csp spécification apprentissage ': 2010}, {'événement séquence ordonnancement notion différence ': 2010}, {'dafoe thésaurus plateforme construire texte ': 2010}, {'anormal vidéo mouvement détection ': 2010}, {'xml interrogation multidimensionnelles entrepôt exécute ': 2010}, {'cas rapc modulaires ontologie requête ': 2010}, {'gène al formelle envi-ronnements compor-tement ': 2009}, {'wokm okmed okm classe métrique ': 2009}, {'ontologie bloc alignement défi deuxméthodes ': 2009}, {'voting multi-classifier probabilistic rule svms ': 2009}, {'depotentiel hypersmooth calcul serveur client ': 2008}, {\"évaluation degré ontologiques '' `` \": 2008}, {'comprenant ontologie descriptifs axiome algèbre ': 2008}, {'somerdfs mappings mediad telecom découverte ': 2008}, {'svm intelligibilité boite actionnables noir ': 2008}, {'aca adaptation connaissance cas cabamaka ': 2007}, {\"vecteur thématique segmentation deft'06 recherchant \": 2007}, {'rdf graphe non-contradiction contredisent imposons ': 2007}, {'multi-agent construction ontologie centralisé mouvance ': 2007}, {'intervalle noeud terminal fils kolmogorov-smirnov ': 2006}, {'arabase arabe écriture optique reconnaissance ': 2006}, {'tableau croisé vraisemblance contingence mélange ': 2006}, {'didactique orthopédique chirurgie diagnostic informatique ': 2006}, {'vidéo flot mixte couleur actif ': 2006}, {'protéine classer 3-grammes croiséepermettant fabr-cl ': 2006}, {'multimédia index voisinage interrogation expérimentationssur ': 2006}, {'bit redondantes vecteur motif règle ': 2006}, {'géotechnique appliquées représentation connaissance approche ': 2006}, {'bdg distribuée sig géographiques résumé ': 2006}, {'ontologie concept legraphe sharedspecificity pss ': 2006}, {'expert multi-experts œuvre connaissance exploitation ': 2005}, {'university stanford students gifted education ': 2005}, {'signature stochastique discrets événement mca ': 2005}, {'symbole image document découverte fréquents ': 2005}, {'logiciel tanagra accessible diffusés échiquier ': 2005}, {'medline résumé nom biologiste protéine ': 2004}, {'uitliation documentaire fondé contenu aide ': 2004}], [{'incrémentales clustering som collaboratif auto-organisatrices ': 2018}, {'suicide parole drift monitorées mental ': 2016}, {\"recommandation géographique lbsn ' factorisation \": 2016}, {'base – rdf jour maintien ': 2015}, {'opinion détectant tweet polarité subjectivité ': 2014}, {'journalistiques porteur tweets recherche information ': 2011}, {'poboc organisation textuellesla softclusters pole-based ': 2004}], [{'importance noeud réseau influe différencie ': 2018}, {\"temps-réel l'architecture oblige pertinence démontrées \": 2018}, {'nosql colonne orienté entrepôt hbase ': 2017}, {'collaboration egc tendance réseau conceptuelles ': 2016}, {'2016 egc logique défi exploration ': 2016}, {'opinion veille amiei analyse plateforme ': 2015}, {'rankmerging lien réseau classement épars ': 2015}, {'multiplexe réseau graine communauté centrée ': 2015}, {'sémiotique style indicateur expliquerons neuronal ': 2015}, {'protéine-arn rosettadock fonction score interaction ': 2014}, {'communauté prédiction réseau interaction prédit ': 2014}, {'orientés attribués graphe isomorphisme combinatoire ': 2014}, {'densité groupe non-disjoints classification observation ': 2014}, {'rôle communautaire réseau connectivité twitter ': 2014}, {'image ontologiques terme tomodensitométriques annotation ': 2014}, {'fa aléatoires voisinage coller libérait ': 2011}, {'réconciliation colorés petri explication renforcer ': 2010}, {'événement association proportionnels séquentielle risque ': 2010}, {'régles confidence confiance ” seuil ': 2010}, {'dissimilarités facteur variabilité états induction ': 2009}, {'incrémental processeur gpu parallèle svm ': 2009}, {'| .la réseau décomposition changement ': 2009}, {'image afc adt recherche factorielle ': 2009}, {\"influence étape l'influé maximisant règle \": 2008}, {'médicales image annotation région fusion ': 2007}, {'fort diamètre degré faible réseau ': 2007}, {'appui tulip interactive graphe visualisation ': 2007}, {'bayésiens réseau aéronautique interruptionsopérationnelles estvalidée ': 2006}, {'connaissance graphique graphe desgraphes laforme ': 2006}, {'point visualisation encours lespoints zoomer ': 2006}, {'règle sémantique armstrong axiome fonctionnelles ': 2005}, {'oubli spécification fonction entrepôt ancien ': 2004}, {'xlive adaptateur web architecture intégrer ': 2004}, {'multivariées positionnement multidimensionnel partitionnement visualisation ': 2004}, {'règle relationnelle requête évaluation coût ': 2004}], [{'spark temps-réel singularité basés combinaison ': 2018}, {'structuration multi-instance apprendre cadre apprentissage ': 2017}, {'neutralité personnalité innovante diffusion compréhension ': 2017}, {'société calcul numériques vie conférence ': 2017}, {'transformé échantillonnage flux donnée ': 2017}, {'affixe chimiques nommées entité reconnaissance ': 2016}, {'requête négatif discriminantes exploration exemple ': 2016}, {'egc 2016 thème défi lorsdes ': 2016}, {'k-moyennes centre vise standard kmoyennes ': 2016}, {'risque chimique alimentaire confondue substance ': 2015}, {'géographiques bdg openjump gdb remplissage ': 2014}, {'recouvrement k-moyennes contrôler groupe généralisation ': 2014}, {'caméra opérateur trajectoire sélectionner jour ': 2014}, {'multicritères cruciales conflit collaborative décideur ': 2014}, {'protéiques epsilon aligneurs multi-étiquettes alignement ': 2013}, {'ré-écriture requête adressées intégration phase ': 2013}, {'roc courbe graphique interprétation ': 2011}, {'fusion prise source compte réseau ': 2011}, {'composition social soco réseau service ': 2010}, {'vdm réalité graphique technique virtuelle ': 2010}, {'multi-agents communication ads cmp sma ': 2009}, {'diagnostic adaptatif diagnostiqueurs multi-sources vérifiée ': 2009}, {'spatiales logique inductive présentera programmation ': 2006}, {'acka multi-acteurs acquisition ? multi-agents ': 2005}, {'mélange k-means mndki2 loi contingence ': 2005}, {'discriminantes séquentielles séquence noyau vecteur ': 2005}, {'incrémentiel échantillon clusterings clusters phase ': 2005}, {'comportement site vis logs fréquents ': 2004}, {'conceptuels graphe connaissance contrainte validation ': 2004}], [{'users survey published have that ': 2016}, {'constraint programming generic solvers settings ': 2016}, {'network actor centrality measures in ': 2015}, {'representative rules output its an ': 2011}, {'early in will accuracy sequences ': 2011}, {'interval-valued partitioning advances recent algorithms ': 2010}, {'pattern graphs sequence databases reductionof ': 2009}, {'video in people activity stage ': 2008}, {'gene nearness condition genes in ': 2006}], [{'community complex network in structure ': 2018}, {'message system and ais falsified ': 2016}, {'mi pattern to sequential patient ': 2016}, {'and project learning mtr sop ': 2016}, {'tom topic and modeling browsing ': 2016}, {'knowledge uncertainty representation an we ': 2015}, {'and property re real-estate 2.0 ': 2015}, {'method and with knowledge campaign ': 2015}, {'recommender distributed was encouraged research ': 2012}, {'spatial learning relational and already ': 2012}, {'meta-actions rules action and ': 2010}, {'risk multiarmed bandit approach with ': 2009}, {'rough in set spatial theory ': 2008}, {'and to in data internet ': 2008}, {'retroweb décrit internet migrateur dé\\x02nies ': 2007}], [{'3d géométriques propriété raisonnement assemblage ': 2017}, {'comportementale action client segmentation adapteroffre ': 2016}, {'mf recommandation technique item relation ': 2015}, {'secondaire variable discrétisation table attribut ': 2012}, {'lab k-words clé explorer scientifique ': 2010}, {'pondération variable observation caractérisation timisée ': 2009}, {'flux distribués consommation électriques capteur ': 2008}, {'émotion homme-machine redirection dialogue automatisés ': 2007}, {'webdocenrich enrichissement document html semi-structurés ': 2007}, {'document restructuration semi-structurés détaillons adapter ': 2005}, {'supervisé enveloppe sv apprentissage variable ': 2005}, {'e-business bm²l ontologie xml e-bmh ': 2005}, {'régression taxonomiques linéaire variable regroupées ': 2004}], [{'dbpédia catégorie apparentés lacune encode ': 2018}, {'performante indexation prise compte document ': 2018}, {'manuscrit chancellerie registre historique transcription ': 2018}, {\"item zone intermédiaire valider l'application \": 2017}, {\"confiance source navire information d'information \": 2017}, {'veille re-watch thème document web ': 2017}, {'géographique référentiel localisation thématique interconnexion ': 2014}, {'bi-partitionnement pondération bloc variable topologique ': 2014}, {'variable hiérarchiques mesurer sélection supervisé ': 2013}, {'maritime surveillance navire accident prévision ': 2012}, {'renforcement transport automatisés sécurité construction ': 2011}, {'répétition télévisuels structuration supervisé flux ': 2011}, {'multimédia recherche modèle ii catégorie ': 2010}, {'geodoc territoire décideur diagnostic document ': 2008}, {'instrumental plateau créativité e-services méthodologie ': 2008}, {\"d'utilisateurs enrichissement sig accomplie offert \": 2008}, {'xml document classement item recherche ': 2007}, {'classement ct-svm modèle mixtes carte ': 2007}, {'symétrique déséquilibré modalité bâtir endogène ': 2007}, {'nexi xquery extension intégrer xml ': 2007}, {'sncf tacites conduite formaliser organisation ': 2006}, {'uml simulation connaissance agent rdf ': 2005}, {'compétence ckim modèle connaissance gestion ': 2004}], [{'client profil aide prototype crm ': 2017}, {'thématique egc mot partageant topic ': 2016}, {'média noeud social non-incrémentales social-attribute ': 2014}, {'carte cognitives cognitive influence vue ': 2011}, {'isicil intelligence travers communauté ligne ': 2011}, {'ssvc étoile coordonnée optimale projection ': 2008}, {\"spatio-temporelle asti concepteur l'utilisateur conception \": 2008}, {'variable supervisé outil khiops millier ': 2008}, {\"table relation document tournée d'enrichissement \": 2006}], [{'assainissement date conduite pose reconstitution ': 2018}, {'méta evaluation cadre analyse donnée ': 2017}, {'cube olap orientée architecture calcul ': 2014}, {'réorganisation olap cube hiérarchique bond ': 2012}, {'cube cnd-cube fermé ” concise ': 2010}, {'spatiale solap multidimensionnelle introduction analyse ': 2008}, {'opac ligne couplage opérateur olap ': 2004}], [{'treillis distributif fca concept médians ': 2018}, {'motif échantillonnage norme aléatoire séquentiels ': 2018}, {'temporalité graduel fermé fréquents contrainte ': 2018}, {'personnalisées réponse mot-clé fournir raison ': 2018}, {'crf définissent séquentielles appel relation ': 2017}, {'flux parallélisme opérateur traitement degré ': 2017}, {'publication egc conférence 3 méthodologie ': 2016}, {'édition thématique publication conférence intéressé ': 2016}, {'encodage programmation asp motif séquentiels ': 2016}, {'transmute trace interactif motif àse ': 2016}, {'cosc télédétection collaboratif segmentation thématique ': 2016}, {'mobilité trace contextualisés individu motif ': 2016}, {'site social microblogging conversational interaction ': 2015}, {\"changement étiqueté fenêtre détection n'avoir \": 2013}, {'spatio-séquentiels motif dengue épidémie anthropiques ': 2012}, {'nucléaire krex @ explicites tacites ': 2011}, {'indice liaison comportement probabiliste limite ': 2011}, {'motif transaction support pondérant reçoive ': 2011}, {'acabit quezao terminologique aspect 2424actu ': 2011}, {'clique étiquette sommet homogènes contrainte ': 2011}, {'motif delta-libres séquentiels difficile produit ': 2011}, {'variable un-à-plusieurs multi-tables cible relationnel ': 2011}, {'ontologie homologue distance légères calcule ': 2011}, {'n-aires bruitées fermé relation propositionpour ': 2009}, {'formels concept dopage classifieur boosting ': 2009}, {'séquentiels inattendus motif fréquence inattendues ': 2008}, {'flot séquentiels échantillonnage motif hypothèse ': 2008}, {'asymétriques arbre entropie décision asymétrique ': 2008}, {'multidimensionnels motif séquentiels clos candidat ': 2008}, {'fia itemset itemsets flot automate ': 2008}, {'licorn adaptative régulation corégulation co-régulation ': 2008}, {'routier trafic atypiques urbain spatio-temporels ': 2008}, {'j-mesure élaguer chronique orientée modèle ': 2008}, {'ancien français séquentiels extrait motif ': 2008}, {'consommation abonné téléphonie détermination flou ': 2007}, {\"entropie mesure l'échantillon insensibilité transmission \": 2007}, {'motif borne condensée représentation séquentiels ': 2007}, {'conditionnel markov champ modèle séquence ': 2006}, {'mot paramètre unité discriminant renforcent ': 2006}, {\"réécriture présence problème l'expérimentons rattache \": 2006}, {'agence dépêche extensible presse métier ': 2006}, {'classification obtenue triée document distance ': 2005}, {'géographiques enrichissement connaissance extraction base ': 2005}, {'flou séquentiels intervalle motif numériques ': 2005}, {'arbre mère participation classificatoire féminine ': 2005}, {'conjoint production optimisation pertinentes apprentissage ': 2004}], [{'événement négatif motif ntgsp électricité ': 2018}, {'a/b généraliste conception test évaluation ': 2017}, {'chronique discriminantes motif discriminant séquence ': 2017}, {'graduel motif groupement variation temporel ': 2017}, {'désambiguïsation catégorisation renseignent individu wikipédia ': 2016}, {'fpof motif fréquents aberrantes approchée ': 2016}, {'mettons egc auteur thématique décrivonsune ': 2016}, {'pondérés chemin condensé complète motiver ': 2015}, {'complexité motif axiome évaluation lié ': 2015}, {'changement qualitatives détection flot cdcstream ': 2014}, {\"minimaux motif d'ensembles minimisable profondeur \": 2014}, {'motif récursifs transactionnelles séquentielles résumé ': 2014}, {'symétrie motif ensemblistes toivonen mannila ': 2014}, {'ralentissement quantitative bibliographique découverte motif ': 2013}, {'snow subspace copac dénommé sous-espaces ': 2013}, {'ràpc ontologie composition recherche information ': 2013}, {'modularité maximisation spectrale catégorielles algèbrique ': 2012}, {'opinion internaute critère expression mot ': 2012}, {'agrégée élément bayésiens xml inex2009 ': 2012}, {'événement intervalle motif temporel représentatifs ': 2011}, {'ensemblistes bruités motif bruit heuristique ': 2011}, {'m3a assisté ingénierie maintenance plateforme ': 2011}, {'linéarité énoncé français phénomène catégorie ': 2011}, {'rto termino-ontologique annotation tableau n-aires ': 2011}, {'phénomène spatio-temporels séquence motif dynamique ': 2011}, {'treemap arbre grille topologique partition ': 2011}, {'courriel indésirables cellulaire machine détection ': 2011}, {'nominales adaptatif formels supervisé concept ': 2010}, {'non-supervisé sous-ensemble caractérisés détectées similitude ': 2010}, {'graduel motif élevé peinent génèrent ': 2010}, {'multidimensionnels motif entrepôt séquentiels automate ': 2010}, {'siam médicaux indexation système article ': 2010}, {'demon-visualisation biologiques séquentiels extrait motif ': 2009}, {'tournebool mot corpus reuters randomisation ': 2009}, {'approcheautomatique siglesissues définition biomédicaux sigle ': 2009}, {'aléatoires forêt arbre oblique algorithme ': 2009}, {'référent carte pondérée voisinage segmentation ': 2008}, {'ontologie générique semi-automatisation conceptualisation terminae ': 2008}, {'conformité contrôle norme c3r bâtiment ': 2008}, {'intervalle divisive homogène partie poisson ': 2008}, {'privées service remplaçabilité protocole conversation ': 2007}, {'corrélés nasopharynx motif cancer fréquents ': 2006}, {\"icare vivre métier l'exemple avecl'environnement \": 2006}, {'récolte parcours trace graphique interface ': 2006}, {'motif fréquents déclarer symptôme praticien ': 2006}, {'transcription adn puce signature facteur ': 2005}, {'échantillon kolmogorov-smirnov arbre intervalle découpage ': 2005}, {'xml tableau tags format entrepôt ': 2005}, {'gène falciparum plasmodium expression parasite ': 2005}, {'signature famille protéine distantes motif ': 2004}, {'job caractérisation exécution globale dualité ': 2004}, {'terminologiques maintenance animation connaissance base ': 2004}, {'émergents motif condensée représentation croissance ': 2004}, {'galois treillis supervisée étude basée ': 2004}], [{'lien wikipédia inter-langues édition erronés ': 2018}, {'inventaire thr satellitaires image terrain ': 2018}, {'image satellitaires série temporelles évolution ': 2017}, {'logistique régression image parallèle échelle ': 2016}, {'1d-sax sax série symbolisation temporelles ': 2014}, {'skypatterns soft-skypatterns toxicophores chémoinformatique manqué ': 2013}, {'croyance classification théorie supervisée classificateur ': 2012}, {'négative règle méta-règles positives part ': 2012}, {'image quantification grand descripteur échelle ': 2012}, {'fonctionnelles dépendance olap exploration association ': 2010}, {'matériel image skin3d calibration réalité ': 2009}, {'fp-growth fcp-growth adaptation générer association ': 2009}, {'traminer librairie séquentielles analyse donnée ': 2009}, {'soda récentes avancée visualiser symbolique ': 2008}, {'cerveau irm anatomiques humain image ': 2007}, {'forage méta-classificateur confiance descriptif règle ': 2006}, {'zone image région imagettes zipf ': 2005}, {'hiéarchique 2-3 web classification donnée ': 2005}, {'diagramme symbolique règle association apriori ': 2005}, {'image automatique classification ': 2004}, {'méta-règles réduction logique issue association ': 2004}], [{'treillis similarité concept mesure basées ': 2017}, {'clé liage publiées candidat peut-être ': 2016}, {'proximité topologique mesure discriminante discrimination ': 2015}, {'reformulation plagiat mot détection détecter ': 2015}, {'fusion typés ontologie formelle grammaire ': 2014}, {'domicile dépendantes mouvement capteur risque ': 2014}, {'multi-label ebr ecc chains ml-knn ': 2014}, {'topologiques statis carte partition compromis ': 2014}, {'simplifiés arbre décision obtenir puissante ': 2014}, {'bitm bi-partitionnement topologique carte bi-clusters ': 2013}, {'ppc contrainte programmation maximal clusters ': 2013}, {'incrémentale itemsets fréquentes flux séquence ': 2012}, {'client probabilité produit typologie campagne ': 2012}, {\"marquage webmarks ressource n'exploitaient bookmarks \": 2012}, {'équivalence proximité topologique mesure plan ': 2011}, {'achat client motif contextuels séquentiels ': 2011}, {'glose testing unité extraire rigoureuses ': 2011}, {'alignement contrainte différence programmation prête ': 2011}, {'attribut propositionalisation gèrent discrétiser catégoriels ': 2011}, {'web3.0 audiovisuel service contenu recherche ': 2011}, {'musique morceau calme perception adaptable ': 2011}, {'motif locaux contrainte n-aires csps ': 2010}, {'sql expansion base domaine transparente ': 2010}, {\"gène séquentiels jusqu'alors sequencesviewer accompagnement \": 2010}, {'co-localisations expert géologiques géo-référencées découverte ': 2010}, {'owl-dl sémantique ontologie entreindividus approchemise ': 2009}, {'galois correspondance cettecorrespondance donnéessimples degalois ': 2009}, {'adn puce demon séquentiels don-nées ': 2009}, {'arbre bayésienne décision arttout notreméthode ': 2009}, {'homophone acoustique mot 41 sélectionné ': 2008}, {'sigle acquisition dictionnaire présenté textuelles ': 2008}, {\"approximer-et-pousser top-k contrainte motif d'intérêt \": 2007}, {'ras annotation citation outil document ': 2007}, {'catégorisation wordnet multilingue monolingue consacré ': 2007}, {'arbre décision résultat compréhension focus+context ': 2007}, {'factorielle variable plan classe noeud ': 2006}, {'contrainte motif extraire égalementun pertinents.afin ': 2006}, {'champ localisation code reconnaissance manuscrit ': 2006}, {'référentielles étoile schéma fonctionnelles contrainte ': 2005}, {'acteur individu relation intégration mise ': 2005}, {'simulation agentuml individu outil production ': 2005}, {'gouvernance entreprise connaissance textuelles désireux ': 2005}, {\"mesure étude expérimentale arguant d'examiner \": 2004}, {'mot-clé langage précision document web.nous ': 2004}], [{'uni-label numéro multi-label défi tâche ': 2017}, {'crowdsourcing expertise participant campagne degré ': 2017}, {'multistratégie fodomust plateforme interface dynamictimewarping ': 2016}, {'défaillance détection système robotisé estimées ': 2016}, {'séquence évènements grille dimension temporel ': 2014}, {'kd-ariane déploiement programmation ariane visuel ': 2014}, {'attribués sommet hiérarchie dynamique motif ': 2014}, {'formateur nucléaire pleine simulateur activité ': 2013}, {'sous-population expert éprouvée idéal converger ': 2012}, {'impact service recommandation découverte web ': 2012}, {'tarification assurance non-vie risque généralisés ': 2011}, {'foule évènements orientation scène détection ': 2010}, {'amo droits accès gestion stratégie ': 2010}, {'bayesienne cancer maximum diagnostic entropie ': 2010}, {'mot langue dépendance surfacique explorées ': 2010}, {'reconnaissance concept basée apprentissage ': 2010}, {'textuelles nommer discussion regrouper groupe ': 2010}, {'entropie catégorisation textuels descripteur sélection ': 2010}, {'relation candidat sémantique hypothèse ontologue ': 2010}, {'équivalence ordre induit mesure degré ': 2009}, {\"d'images graphique relation tentons interactionplutôt \": 2009}, {'itemsets période compact deico cohérent ': 2008}, {'croisement validation relation ontologie domaine ': 2008}, {'intrusion associatives détection générique sdis ': 2008}, {'bibliothèque personnalisée numériques retriés citation ': 2008}, {'étiquetées topologie delaunay euclidien génératif ': 2007}, {'préparation protocole fiabilité représentation métrique ': 2007}, {'patron couple amorce phrase liste ': 2007}, {'milieu catégorisation indice adapté difficile ': 2007}, {'chronique uneoccurrence classesévénements approcheentropique dediagnostic ': 2006}, {'multi arbre décision supervisé mode ': 2006}, {'teximus expertise logiciel suiteintégrée dynamiquede ': 2006}, {\"automate texte s'intéresser nature précédemment/ \": 2006}, {'schéma conception aid systémique averti ': 2005}, {'plate-forme agent tfidf madkit fonctionnant ': 2004}, {'prion beluga maladie auteur indexation ': 2004}, {\"contingence table sgbd traitement s'appliquer \": 2004}], [{'bayésien modèle co-clustering mixtes critère ': 2018}, {\"croisé collaboratif item filtrage qu'elle \": 2015}, {'naïf bayésien log-vraisemblance obtenue variable ': 2014}, {'chanson parole collection explorer interface ': 2014}, {'ghsom som pourrendre analyseexploratoire objecif ': 2009}, {'itemset fiasco item nouvel batches ': 2008}, {'rdf interrogation triplet document raisonnement ': 2005}], [{'could for knowledge learning ontology ': 2017}, {'in data and category fairness-aware ': 2016}, {'data both and knowledge t-box ': 2016}, {'data will explore and decision ': 2015}, {'data datasets and broad to ': 2014}, {'mobility and data mining gps ': 2011}, {'tulip graph framework and representations ': 2010}, {'quality are digital data and ': 2006}, {'accord autonome internet systèmesautonomes transit ': 2006}], [{'socio-semantic view network based point ': 2011}, {'your mysins make semantic system ': 2010}], [{'chemin information propagation trajectoire retrouver ': 2018}, {'peerus scientifique review expert entrainé ': 2018}, {\"qu'est-ce apprentissage machine évolué question \": 2018}, {'sujet biclustering investigation variante textuels ': 2017}, {'label relation apprendre cycliques ignorer ': 2017}, {'rdf format web service objet ': 2017}, {'k-sc centroïd k-spectral translation barycentre ': 2017}, {'ferré voyageur prévision bayésiens flux ': 2017}, {'opinion twitter plateforme recommandation réel ': 2017}, {'passage question reclassement sqr retourner ': 2016}, {'multi-tables variable khiops million table ': 2016}, {'skyline retournés pertinencede relaxéest preferred ': 2016}, {'biclustering mapreduce dataset cores scalable ': 2015}, {'spectral amènent notoirement satisfaisante fonctionne ': 2014}, {'réclamation allocataire typologie caf nationale ': 2013}, {'aboutissons acycliques pondérés acyclique extrayant ': 2013}, {'factorisation note recommandation prédiction matrice ': 2013}, {'transfert pondéré topologique matricielle apprentissage ': 2012}, {'géolocalisée nomao personnalisée recherche ': 2011}, {'action estimés magnitude vidéo séquence ': 2011}, {'préférence gros skyline requête volume ': 2010}, {'lissage communauté identification modèle probabiliste ': 2010}, {'flux contrôle traitement observation facteur100 ': 2009}, {'contrainte médicales carte version mélanome ': 2008}, {'cancer profil non-cancer npc épidémiologique ': 2007}, {'prépayée churn téléphonie détection mobile ': 2004}, {'musette réutiliser expérience capture illustrent ': 2004}], [{'commentaire commentsminer extraction publiquement 84 ': 2016}, {'contrainte clustering intervention satellite expert ': 2016}, {'samples models population learned training ': 2014}, {'genre auteur hybride profil statisque ': 2014}, {'text2geo étang géospatiales thau information ': 2013}, {'secondaire table variable itemsets construites ': 2013}, {'variable construction table constructibles choisissant ': 2013}, {'file personal tags context tag ': 2008}, {'wordnet som textuels basés synsets ': 2008}, {'superposées fenêtre basées stratégie repérer ': 2008}, {'sortie interpréter importance interprétation variable ': 2007}], [{'cds résumé texte composant rôle ': 2018}, {'formés majoritaire vote global obtenus ': 2018}, {'a/b test procédure stratification contextuel ': 2018}, {'défaillance événement compteur cause sous-tendant ': 2018}, {'universal-endpoint.com plateforme web sparql accès ': 2018}, {'sous-parties discriminantes aléatoire 3d objet ': 2017}, {'pharmacovigilance web social fondé sémantique ': 2017}, {'rôle communautaires centralité communauté mesure ': 2017}, {\"duplicats redondantes d'argumentation transparent figurant \": 2016}, {'skyline calcul espace candidat réduction ': 2015}, {'sous-groupes molécule olfactives neuroscientifiques olfaction ': 2015}, {'réseau attribut entité relation étendant ': 2013}, {'caractéristique classificateur sélection combinaison sélectionné ': 2012}, {'intervalle séquence temporelles incertitude fréquentes ': 2012}, {'prétopologique lexico-sémantiques structuration texte acquisition ': 2011}, {'participant p2p hétérogénéité disparité considérant ': 2011}, {\"noyau arbre induit d'optimalité parallèlement \": 2010}, {'traduction rail séquence corpus fréquentes ': 2010}, {'multi-résolution atypiques flot détection objet ': 2009}, {'rfid traçabilité groupe spatio-temporelles individu ': 2009}, {'résumé requête obtenussont kdd_99 nousutilisons ': 2009}, {'multi-métiers cross-lingue européen entreprise prototype ': 2009}, {'médicale génomique corrélation linéaire système ': 2009}, {'définition alignement svm noms-adjectifs investigués ': 2008}, {'délestage dégradation flux cube mécanisme ': 2008}, {'temporisées conversation transition service logs ': 2007}, {'écouter audio accès résumé réduirela ': 2006}, {'maximum flot paysage optimal densité ': 2006}, {'tactique agent réactifs schéma simulation ': 2006}, {\"annotation ressource ontologie propagation l'informatique \": 2005}, {'tbi indexé bit fréquentes tableau ': 2005}, {'table spatiales fouille alternative jointure ': 2004}], [{'dynamics power understanding big human ': 2018}, {'city captured cities and how ': 2017}, {'ratings user-user recommendation filtering enhanced ': 2017}, {'to more learning and in ': 2016}, {'summaries real time seems chorems ': 2016}, {'slider to reasoner more knowledge ': 2016}, {'advantage to consider window sequential ': 2015}, {'and to — help cities ': 2015}, {'vocabulary tweet tweets this to ': 2015}, {'tweets in creation to users ': 2015}, {'xplor xewgraph everywhere intelligence and ': 2015}, {'crowd for mining human that ': 2014}, {'plagiarism methods academic by to ': 2013}, {'in and work dimensional narrative ': 2013}, {'event biological and which statements ': 2012}, {'genetic biological utilizing guarantee high-throughput ': 2012}, {'visual and analytics to with ': 2012}, {'why and has this in ': 2012}, {'to and processing high-level defence ': 2011}, {'engines search are high and ': 2011}, {'communities network presence nodes metrics ': 2010}, {'pattern one past future and ': 2010}, {'to patches visual sentence words ': 2010}, {'user profile contextualization and contexts ': 2009}, {'in answer text mass vectors ': 2009}, {'trees unlabeled are rules from ': 2008}, {'conditional entropy bayesian generalized network ': 2008}, {'interestingness measures mining and in ': 2007}, {'are regression customers loans default ': 2006}, {'algorithms lattices to millilitre lattice ': 2004}], [{'discovery metabolomic biomarkers métabolomiques methodologies ': 2016}, {'conduisent syndrome alarmiste déclaration revanche ': 2014}, {'itinéraire récit voyage attendu déplacement ': 2008}], [{'motif utilisateur préféré implicitement transaction ': 2017}, {'décision justice crf hmm loi ': 2017}, {'agroalimentaire – acteur décision découlant ': 2016}, {'semi-interactif purement visuel extrait accepté ': 2016}, {'anomalie trace unitaire motif collection ': 2016}, {'paramètre recherche modifiant rungeneration terrier ': 2015}, {'multi-label centrée économique raisonnement indexation ': 2015}, {'cluster affectation borne bernstein accélération ': 2014}, {'maintenance train préventive ferroviaire décision ': 2010}, {'lexico-syntaxiques patron apprentissage souhaitables fastidieuse ': 2010}, {'attaque signature intrusion collaboratif détection ': 2009}, {\"forêt détérioré contourner d'applications minoritaire \": 2008}, {'itemsets clé suppression essentiel petit ': 2008}, {'moteur entreprise géo-localisation jaune activité ': 2007}, {'sdet géographiques enrichissement enrichment supplément ': 2007}, {'étape ré-ordonnancement candidat document transformation ': 2007}, {'vivant mode appliqué science solutionsde ': 2006}, {'porphyry confrontation vue point système ': 2006}, {'médiateur virtuelles xquery mot-clé vue ': 2006}, {\"xml schéma adopté recherche ainsil'intérêt \": 2006}, {'transaction booloader denses charger implanté ': 2004}, {'veille technologique texte rechercher scientifique ': 2004}], [{'elaboration poster géotechnique élaboration domaine ': 2018}, {'capacitaire planning perforecast tôt service ': 2018}, {'neuro-imagerie réaliste observation réutilisation cliniciens ': 2018}, {'textuel lignée su influencers hybrid ': 2017}, {'normalité singularité expression langage sélection ': 2017}, {'entité relation événement lexico-sémantique rachetées ': 2017}, {'chaînage entretien supervisés histoire science ': 2017}, {'latent spectral arbitraire espace transformation ': 2016}, {'changement composé formalisation ontologiques élémentaires ': 2015}, {'textuel extrait prend ontologie génération ': 2014}, {'nommées expansion entité requête recherche ': 2014}, {'discours subjectivité médical médecin médicaux ': 2014}, {'intervalle généralisation concept probabilités/fréquences ordinales ': 2012}, {'multi-niveaux topologique hiérarchique clustering graphe ': 2012}, {'propriété co-variations sommet réseau motif ': 2012}, {'dissimilarité matrice pondération classe partition ': 2012}, {'connaissances1 iconique coopérative catégorisation apport ': 2011}, {'folksonomies tagging cycle complet vie ': 2011}, {'chorml géographiques résumé visuel base ': 2010}, {'expertise wikis disparité créés blog ': 2010}, {'pureté critère connaissance intégration mobilisé ': 2010}, {'plcm fermé itemsets fréquents creuses ': 2010}, {'protéiques stabilité etude séquence sélection ': 2010}, {'dynamique rbd connaissance bayésiens décision ': 2010}, {'dbfrequentqueries fréquentes requête extraction ': 2009}, {'nsvm boosting psvm ls-svm machine ': 2008}, {'n-gramme vie parcours séquence précisera ': 2008}, {'prototypicalité gradient endogroupe ontologie pragmatique ': 2008}, {'vie parcours méthodologie suisse événement ': 2008}, {'séquence biologiques substitution protéiques matrice ': 2007}, {'datées événement discrets fabrication supervision ': 2007}, {'webangels filter violent structurel textuel ': 2007}, {'signature indicateur financier courte auxstratégies ': 2006}, {'uncorpus ‘ domaine ontologie concept ': 2006}, {'indice applicationest intéressantesdans .ces iad ': 2006}, {'multivariées scénario série temporelles apprentissage ': 2005}, {'ressource orienter terminologique fouille construction ': 2005}, {'connaissance médiation coopérative activité système ': 2005}, {'réécriture requête multimédia standard ontologie ': 2005}, {\"'' `` générique trie-itemset éristique \": 2004}, {'édifice antique théâtre vestige 3d ': 2004}, {\"flou sous-ensemble appelons défini l'ontologie \": 2004}], [{'bord personnalité média communauté suivi ': 2018}, {'communauté stabilité représentatives période temporelle ': 2017}, {\"ciblée d'images descripteur visuel sélection \": 2017}, {'nell instance français anglais chaîne ': 2017}, {'po-motifs sous-séquences arc relationnelle concept ': 2016}, {'hyperplan parallèle point phase commune ': 2016}, {'lexico-scientométrique egc 2016 défi websous-tendu ': 2016}, {'sommet multi-résolution temps qualitatif catégoriels ': 2014}, {'caractérisation chemin voisinage capte linéarisation ': 2014}, {'locuteur articulateurs articulatoire 3-way tableau ': 2014}, {'ontologie page noyau jardinage fiche ': 2013}, {'régularité fréquents lien noeud flmin ': 2012}, {'analysis multi-block canonical regularized correlation ': 2012}, {'suspect visuel comportement communication accompagnée ': 2011}, {'tableau annotation guidée floues flou ': 2011}, {'navigation espace outil sémantique ': 2011}, {'fonctionnelles dépendance inférer incfds inférence ': 2010}, {'pair p2p simtole plateforme alignement ': 2010}, {'hiérarchique topologique ” partition so-tree ': 2010}, {'marché matériau métier commercial opération ': 2009}, {'croki2 classe validation traversun dedéterminer ': 2009}, {'corrélation décisionnelles contingence vecteur lectique ': 2009}, {'riche schéma hiérarchie destructures analyseclassique ': 2009}, {'annotation hiérarchiques erreur hiérarchie mesure ': 2008}, {'primal optimisation svm shrinking différentiables ': 2008}, {'alternative requête génomique entrepôt multi-niveaux ': 2008}, {'décideur argumentative cruciales multi-agent conflit ': 2008}, {'nk immunitaire artificiel système killer ': 2008}, {'adaboost bruitées boosting agrégation face ': 2008}, {'diffusion liste faq cop sémantique ': 2008}, {'comportement raison site résumé jour ': 2007}, {'conceptuelle inertie sens tableau implique-t-il ': 2007}, {'vidéo acp espace bloc dimension ': 2007}, {\"période `` '' arbitraire denses \": 2006}, {'latente petit amélioration taille corpus ': 2005}, {'pyramide palier n-1 interprétation nombre ': 2005}, {'svm intervalle symbolique artificiellement radial ': 2005}, {'psvm svm proximal nombre restreint ': 2004}, {'bloc partition homogènes colonne instance ': 2004}, {\"l'édifice tridimensionnelle patrimoine réponse s'imposer \": 2004}], [{'multi-couches edoi itérative exploration graphe ': 2018}, {'financement campagne participatif prédiction succès ': 2018}, {'graphe communautaire générateur réseau dynamique ': 2017}, {'multidimensionnel hiérarchique hiérarchie factuelles rolap ': 2016}, {'critère ecart partition modularité graphe ': 2015}, {'précision-rappel compromis performance espace fonction ': 2015}, {'prosopographiques visualisation portail accès interface ': 2014}, {'ex réseau concept afc analyse ': 2013}, {\"diffuseur tmd-miner membre réseau d'hypergraphe \": 2012}, {'logique clause prédicat markov réseau ': 2011}, {'mesure dégager règle 61 propriété ': 2011}, {'privée média préservation vie sociaux ': 2010}, {'etude comparative langage lien complexe ': 2010}, {'image bi-directionnelle caractéristique réduction version ': 2010}, {'markov ordre démontronsexpérimentalement rechercheet existantesun ': 2009}, {\"chaîne jaccard caractère indice d'entitésnommées \": 2009}, {'obtenu premierplan learningchallenge pascal pré-sentons ': 2009}, {'visage reconnaissance classification galois hybride ': 2008}, {'sociaux réseau ordonnancement parlerons prisée ': 2008}, {'fourmi graphe artificielles incrémentale voisinage ': 2007}, {'incomplètes contiennent temporairement représentative occultées ': 2007}, {'web logarithme fichier serveur site ': 2007}, {'pair-à-pair grandeconsommation téléchargement fichierssouvent réseau.cette ': 2006}, {'étiquetage ré-étiquetage morpho-syntaxique règle logiciel ': 2005}, {'gvsr annuaire édition manipulation logiciel ': 2004}, {'sdv accès rapide cns efficiente ': 2004}], [{'mot-clé catégorisation scientifique relation sémantique ': 2018}, {'verbalisation fait temporel formalisation indice ': 2018}, {'recommandation actualité personnalisées hybride échelle ': 2018}, {'treillis correspondance resp attribut objet ': 2017}, {'plateforme collaboratives qualité noeud relation ': 2016}, {'sous-ensemble topologique sélection discrimination induite ': 2016}, {'détection auteur plagiat passage regroupement ': 2015}, {'solaire rayonnement prédiction photovoltaïque flux ': 2014}, {'stratégie agent trace élicitation méthodologie ': 2014}, {'n-aires ontologie dédiée relation représentation ': 2013}, {'attribués arbre fréquents motif sous-arbres ': 2013}, {'taxonomie lexique hypothèse approximer defaits ': 2012}, {'robot hog corps svm détection ': 2012}, {'ricsh fragment corpus thématique contextuelle ': 2012}, {'jurisprudence juridique décision arabe ontologie ': 2012}, {'graduel motif multi-threading pgp-mc ordinateur ': 2010}, {'ashms imc enfant précoce management ': 2010}, {'inventif artefact acquisition conception connaissance ': 2010}, {'flot itemsets fenêtre impossible hiérarchie ': 2010}, {'sgbd nautilus métier entreprise optimisés ': 2008}, {'gène cellulaire cycle expression référence ': 2008}, {'détenu organisation connaissance relatif acteur ': 2007}, {'motif sous-classe quelconques découverte conjonctive ': 2007}, {'air intérieur surveillance décision qualité ': 2006}, {'meat mémoire expérience scientifique projet ': 2006}, {'passage qr contenir question réponse ': 2005}, {'exit itérative terminologie extraction ': 2004}, {'thyroïde gène cancer impliqués indexation ': 2004}], [{'recommandation mobilité factorisation inféré visités ': 2018}, {'variable coviz partitionnées khiops catégorielles ': 2016}, {'bayes naïf classifieur flux variable ': 2014}, {\"préférence profil contextuelles l'utilisateur effectuer \": 2014}, {'incertitude théorie due incertain distribution ': 2014}, {'antenne appel habitant france mobile ': 2013}, {'wiki isicil entreprise projet social ': 2012}, {'pondération codées mixtes simultanée binaire ': 2011}, {'variable exogène paire partitionnement exogènes ': 2007}], [{'user engagement query system search ': 2017}, {'their semantics may to data ': 2016}, {'multidimensional big data and business ': 2015}, {'text groups representation terms sequence ': 2013}, {'relationship attribute community detection network ': 2012}, {'clustering heuristics and network in ': 2012}, {'yacaree parameter-free rule with mining ': 2011}, {'learning and data failure process ': 2011}, {'semi-structured in retrieval structuring reach ': 2009}, {'data will new in developments ': 2009}], [{'co-clustering mixtes table continues bloc ': 2017}, {'clustering préférence distance attribut utilisateur ': 2016}, {'2d outil projection interactif clustering ': 2015}, {'ri sociale social information modèle ': 2013}, {'segmentation client purchase customer self-clustering ': 2010}, {'audit unlabelled adaptive anomaly intrusion ': 2009}, {'stream motif séquentiels data contrainte ': 2006}], [{'idéo2017 citoyen politiques tweets candidat ': 2018}, {'porgy pilotés rewriting port modelling ': 2017}, {'codés psychologique orientés expertise rdf ': 2006}, {'desmétadonnées métadonnées interpréter utilisateur traduirons ': 2006}, {'profil filtrage trec filtrées fur ': 2004}], [{\"mobile m-learning apprentissage l'apprenant prochaine \": 2014}, {'data stream change distribution detection ': 2010}], [{'cardiaque similarité quasi-arithmétiques moyenne type ': 2018}, {'gpo séquence ordonnés graduel partiellement ': 2018}, {'influenceurs centralité comparative détection évaluation ': 2018}, {'approximatifs déséquilibré incrémental décision mai2p ': 2018}, {'palm espace treillis parallèle branche ': 2018}, {'groupe centralité collaboration scientifique thématique ': 2016}, {'desdonnées propriété textuelles représentationsphonologiques unicode ': 2016}, {'événement flot exploratoire méritant outskewer ': 2014}, {'3d stéréoscopique stéréoscopie exploratoire perspective ': 2013}, {'séquence similarité mesure efficacement extrême ': 2013}, {'probabiliste instant formulation séquentielles génération ': 2012}, {'agronomiques imprécision gérant agricoles entité ': 2012}, {'intra description structure visualisation inter-groupes ': 2011}, {'complexité indice catégorielles états séquence ': 2010}, {'sous-échantillonnage déséquilibre apprentissage classe observation ': 2010}, {'bruit bruités descripteur construction qualitésobtenues ': 2009}, {'culturel patrimoine management domaine connaissance ': 2009}, {'tei spécialisées bibliothèque structuresarborescentes justifions ': 2009}, {'petit monde vue réseau document ': 2008}, {'co-classification contrainte quadratiques résidu somme ': 2008}, {'trajectoire arrêt mouvement temporel concept ': 2008}, {'treillis navigation conceptuel concept niveau ': 2008}, {'banc pmms npc nasopharynx impliqués ': 2007}, {'carte prédiction pertinente volumineuses lié ': 2007}, {'itemsets concises essentiel fermeture représentation ': 2007}, {'fcm dca dc floue programmation ': 2007}, {'élagage roc taux courbe terminologie ': 2006}, {'datalab esiea nettoyage préparation logiciel ': 2006}, {'biomédicaux entité genia mesurantles autresdescripteurs ': 2006}, {'spécialisées unité regroupement ontologie textuelles ': 2006}, {\"d'évènements inductives chronique temporel fréquents \": 2005}, {'automate textuel texte cheminement admettre ': 2005}, {'ntic capitalisation service connaissance ': 2005}, {'discrétisation partition stratifiée feuille arrêter ': 2004}, {'cartographie carte sémantique connaissance ': 2004}, {'appropriation mask mémoires savoir-faire entreprise ': 2004}], [{'stream shedding load data processing ': 2015}, {'antipatterns antipattern bad ontology queries ': 2012}, {'piecewise histogram is fisher function ': 2007}, {'kilomètre sociotechnique vision précisément décrit ': 2007}], [{'carreau désagrégation population échelle bâtiment ': 2018}, {\"clowdflows relationnelles fouille d'exécuter venons \": 2016}, {'blog olap projet billet véhiculées ': 2015}, {'spatiotemporelle comptage blob franchissant ligne ': 2011}, {'olap opérateur ligne analyse factorielle ': 2010}, {'cube émergent quotient fermé cubique ': 2010}, {'série mémoire al ) 2002 ': 2010}, {'explorer3d visualisation classification donnée ': 2009}, {'créativité ilp calculatoire synthèse prédicat ': 2009}, {'syr symbolique logiciel analyse donnée ': 2009}, {'cube olap intégration prédiction ': 2008}, {'cube visualisation pixel orientée calcul ': 2007}], [{'arabe sentiment commentaire marocain dialectal ': 2018}, {'tentative décès supervisées patient risque ': 2018}, {'blockchain notarisation nfb protocole document ': 2018}, {'binarisées variable coclustering individu party ': 2017}, {'cabine simulation conception confort passager ': 2013}, {'probabiliste catégorielles som auto-organisatrice topologique ': 2012}, {'tanagra cellulaire discrétisation implémentation conception ': 2011}, {'document pédagogiques discursifs pédagogiqes placées ': 2010}, {'sgfd flux portent fenêtre conserver ': 2010}, {'fenêtrage spatial traitées requête temporel ': 2008}, {'document collection liste entité espérons ': 2007}, {'connues document xml mot collection ': 2006}], [{'format rdf sparql non-rdf ii ': 2017}, {'subspace flux clustering identifer visualisant ': 2017}, {'rs-ndf bootstrap nouveauté filtre ndf ': 2012}, {'” item paire hautement corrélées ': 2010}, {'spatialisable architectural informationnelle évolutives méthodologique ': 2006}], [{'mapreduce musée accessibilité paradigme parallèle ': 2014}, {'traverse minimales local-generator hypergraphe régner ': 2014}, {'traduction rim rail inter-langues exprimées ': 2014}, {'traverse minimales hypergraphe hypergraphes imt-extractor ': 2013}, {'boycott caractérisation identification type analyse ': 2012}, {'visualisation quantité grandissante multidimensionels biomimétique ': 2010}, {'multidimensionnel olap opérateur complexe objet ': 2010}, {'règle classe associative association significatives ': 2010}, {'règle cibler décideur intéressantes lesconnaissances ': 2009}, {'relationnelles logiciel aussiun processuspeut sup-porté ': 2009}, {'groupe nombre stabilité déterminer classification ': 2008}, {'site dissimilarités page navigation dediverses ': 2006}, {'associatives fast-mgb règle générique génériqueminimale ': 2006}, {'universitaire réussite génération prédiction règle ': 2005}], [{'recommandation hôtel contextuelles distinctives volatilité ': 2018}, {'recommandation tags persorec ressource utilisateur ': 2016}, {'populaires thématique twitter message délivrée ': 2014}, {'syntagme nominaux sri indexation filtrage ': 2013}, {'barrière format transformation brut utilisateur ': 2011}, {'syntaxiques induites valider utilisantdes qualitéde ': 2009}, {'probabiliste document structure modèle révélés ': 2008}, {'distribution coupure emplacement infinie fonctionnelles ': 2007}, {'douce eau secteur multilingue distribué ': 2006}], [{'recommandation folksonomie filtrage préférence basées ': 2017}, {'lbsn recommandation poisson factorisation point ': 2017}, {'c-sparql flux rdf extension sparql ': 2016}, {'g-stream flux clustering topologique gas ': 2015}, {'jointe grille modèle consistance distribution ': 2012}, {'expertise matérialisée etexpertise décisionnellessont décisionnellesmais ': 2006}], [{'moving objects done trip andboat ': 2016}, {'vol simplification enregistrées perte drastique ': 2010}], [{'each neighborhood-based we stream group ': 2016}, {'programme clustering détournant attributs-valeurs proscrit ': 2014}, {'histograms summarization on-line stream by ': 2011}, {'territoire datamining spatial appliqué lié ': 2006}], [{\"unitex/gramlab grammaire s'étend marne-la-vallée paris-est \": 2018}, {\"0-subsomption pli d'acteurs subsomption passent \": 2017}, {\"image l'arbre exploration visuel construite \": 2016}, {'segmentation annotation maillage objet étape ': 2015}, {'attribut règle liaison nombre regroupement ': 2015}, {'évolutifs oubli floue incrémental récursifs ': 2013}, {'classe itératif supervisée centre définie ': 2013}, {'inter-domaines transfert invariants médiation complétion ': 2012}, {'afc image contingence tableau mot ': 2011}, {'pmi ppmi positives variante formelle ': 2011}, {'règle robustes critère robustesse bayésien ': 2011}, {'convexes géométrie fermeture topologie agrégation ': 2010}, {'prédicat pléthoriques ajout initiale corrélation ': 2010}, {'ei dépendance acquisition pourcent 755 ': 2009}, {'drone élémentaires photographie zone fine ': 2008}, {'symbolique concept histogrammesuites lavariation analysantces ': 2008}, {'ressource alignement entité appariement règle ': 2007}, {'archéologiques navigation annotation approximatifs implémentée ': 2007}, {'colonne annotation symbolique numériques floue ': 2007}, {'communauté pratique émergence échange membre ': 2007}, {'thermique compact ajustés rc renforcé ': 2007}, {'navigateur paradigme interactive navigation polar ': 2006}, {'voronoi cellule instance partition surapprendpas ': 2006}, {'cartographiques objet satellitales modèle image ': 2005}, {'chic implicative offertes possibilité logiciel ': 2005}, {'cache gml politique spatiales entrepôt ': 2005}, {'rasma multi-agent spatiales amélioration association ': 2005}, {'em convergence comparative variante réalisons ': 2004}, {'r^p image topologique représentée vecteur ': 2004}], [{'spatial modeling raster ocelet format ': 2014}, {'haptiques simulateur eiah apprenant chirurgie ': 2011}, {'emploi offre catégorisation internet choisis ': 2011}, {'intrusion detection anomaly ids outlier ': 2009}, {\"instantané – global contexte c'est-à-dire \": 2007}], [{'prédictives k-moyennes modifiée compromis mesurer ': 2017}, {'label doublon trace binaires treillis ': 2016}, {'agrégation réduction dimensionnalité qualitatives préférence ': 2016}, {'gisement historien historique entrer projet ': 2013}, {'df approximatives modifiée exactes relation ': 2012}, {'histogramme distance nominaux entité étage ': 2012}, {'textuelles ressource analysons construction méthodologie ': 2011}, {'flux changement détection état-de-art favorablement ': 2011}, {'concordance mesure réduire évidentielles source ': 2011}, {'usager santé accès cancer terminologie ': 2010}, {'stratégie bayésienne échelon unidimensionnelles comparatives ': 2010}, {'changement simulation cours usageà uneméthodologie ': 2009}, {'construction lieu texte treillis formelle ': 2007}, {'motif utilisateur théorique collection confidentialité ': 2007}, {'alignement owl-lite ontologie circularité remédiant ': 2007}, {'multi-agents clustering permanence dynamique évolutives ': 2007}, {'i-semantec capitalisation collaborative industriel métier ': 2006}, {'préfixe ip trafic massif temps ': 2006}, {'homme-machine dialogue émotion expression modélisation ': 2006}, {'expert terme exit pertinents logiciel ': 2005}, {'ecd vue processus point garder ': 2005}, {'groupage khiops modalité robustesse attribut ': 2004}], [{'carlo monte bs sd exploration ': 2017}, {'récurrentes attribué graphe noeud figés ': 2017}, {'mappings soc évolution adaptation biomédical ': 2016}, {'haie markov hilbert courbe densité ': 2016}, {'secm évidentielle croyance fonction schéma ': 2015}, {'cpt prédiction compact ppm all-k-order ': 2015}, {'compréhension cuisine recette site déduite ': 2014}, {'tendance enseigne commercial clientèle précoce ': 2013}, {'arc graphe clusters densité taille ': 2011}, {'croyance masse posteriori maximum probabilité ': 2010}, {'essentielles traduction idée texte graphe ': 2010}, {'wikipedia wikipediaviz lecteur visualisation qualité ': 2010}, {'ld+règles cisna gérer hybride système ': 2009}, {'réaction schéma chimiste graphe chimiques ': 2008}, {\"règle implication conclusion validation s'agit \": 2007}, {'exception area apprendre logiciel règle ': 2006}, {'règle informationnel contraposées indice qualité ': 2004}, {'voisinage graphe prétraitement voisin proximité ': 2004}], [{'icm compacité énergie haute convergence ': 2015}, {'cyclone longitude latitude 6 vent ': 2011}, {'olap opération r-olap implanteopération deshiérarchies ': 2009}, {'neurone multicouches architecture couche choix ': 2007}], [{'analogiques proportion analogique paire n-uplets ': 2015}, {'videos tags video we system ': 2013}, {'neurone rbf-gene rbf optimiser poids ': 2008}, {'bootstrap réplication total corrige auto-organisées ': 2007}, {'pattern 19x19 go programme voisin ': 2005}], [{'table anonymiser publiées coclustering individu ': 2017}, {'pervasifs treillis formelle smartphone généraliser ': 2016}, {'viewpoints overlaps bi-cluster variant extract ': 2016}, {'composante parcimonieuse em principal probabiliste ': 2015}, {'knn voisin catégorisation texte proche ': 2014}, {'tags pourcent relation taux annotateur ': 2013}, {'mot-clé document similitude présentant similaires ': 2013}, {\"discriminante '' échantillon `` léchantillon \": 2011}, {'structurelle document classe revient distance ': 2010}, {'reposant caractérisation treillis lefiltrage ligneillustre ': 2009}, {'stream spam séquentiels transaction incrémentale ': 2009}, {'atypiques salaire groupe variable fort ': 2008}, {'floue proximité terme avantageux modèle ': 2007}, {'électrique consommation sgfd gestion flux ': 2007}, {'mammographie comparaison apprentissage méthode ': 2006}, {'compétence connaissance métier gestion organisation ': 2005}, {'résumé module profil lecture présentation ': 2005}], [{': personnalisés rdf/sparql type xsd ': 2017}, {'collection function is proportional parameter ': 2015}, {'twitter message stream can system ': 2014}, {'clusterings individuel clustering dimensionalité soutenue ': 2008}, {'okm recouvrante moc théoriques mélange ': 2008}], [{'réseau lien conceptuels clusters évolution ': 2017}, {\"mahout écosystème forest random d'amélioration \": 2015}, {'p-etl massives etl * parallèle ': 2015}, {'orientée objet haute région résolution ': 2013}, {'série temporelles descripteur coclustering objets-attributs ': 2013}, {'courbe clusters paramétrique fonctionnelles hiérarchique ': 2012}, {'transduction couverture nommées annotation règle ': 2011}, {'relationnelles étape réseau sociaux agrégation ': 2011}, {'ancien traits détaillées bref image ': 2010}, {'voisinage quiconsidère par-tie sontproches paramétriques ': 2009}, {'graphe période sommet relationnelles décomposer ': 2008}, {'noyau vectoriel latent cvsm vsm ': 2008}, {'oubli spécification fonction entrepôt intelligentes ': 2007}, {'divergentes convergente multidimensionnelles m2s_cd srikant.même ': 2007}, {'archiview hôpital topographique paramètre visualisation ': 2006}, {\"symbolique graphe agent ultérieures l'agent \": 2005}, {'conservation détail oubli entrepôt fonction ': 2005}, {'subsomption règle hiérarchie structurant niveau ': 2005}, {'chaînés maximaux bayésiens graphe réseau ': 2004}, {'entrepôt hétérogènes gestion donnée ': 2004}, {'textuelle catégorisation association outil règle ': 2004}], [{'electricity long provider period weron ': 2016}, {'semantic relation in rich divided ': 2016}, {'cooper-herskovitz simplified criterion assumptions bayesian ': 2012}, {'usager question internet ': 2011}, {'dtmvic text inférence logiciel mining ': 2009}], [{'cah formule ascendant part noyau ': 2017}, {\"paysage l'image enjeu d'automatisation alliant \": 2017}, {'archive télévisés face2graph 15 visage ': 2017}, {'règle association étape savoir fouille ': 2017}, {\"positives négative règle rapn s'est \": 2013}, {'dépendance règle multivaluées définies analogie ': 2012}, {'pose isar image cible transformée ': 2011}, {'satellite évolution image sits série ': 2011}, {'automobile ascendant suivi hiérarchique classification ': 2010}, {'envi télédétection image innovants pixelsclassique- ': 2009}, {'chaîne noyau image graphe région ': 2008}, {'vote opinion classification thématiquement jugement ': 2008}, {'non-supervisée relationnelles classification donnée ': 2006}, {'règle permetenfin pied significatives.elle fallacieusement ': 2006}, {'réaction synthèse organique chimie graphe ': 2006}, {'8594 composée règle prémisse ; ': 2006}, {'série agrégation warping dynamic k-means ': 2005}, {\"p-tree d'images classification image règle \": 2005}, {'hypertexte page lien fonction correspondance ': 2005}, {'relationnelle supervisées stratégiques analyse graphe ': 2004}, {'ptrees ptree fort minimum association ': 2004}, {'image fixe descripteur texture couleur ': 2004}], [{'métadonnées vault physique data stockage ': 2018}, {'big data research is it ': 2015}, {'textual data representation parameterized offering ': 2014}, {'matching métadonnées interopérabilité assurer mumie ': 2011}, {'data using state integration over ': 2010}, {'orders finding total 0-1 data ': 2006}, {'compétence raisonnement expérimenter data-mining knowledge-mining ': 2005}, {'policier text-mining informationnelle veille grâce ': 2004}], [{'multi-label prétraiter label sélection learning ': 2017}, {\"stemma codicum triplet obtained 's \": 2015}, {'mi corrélation corrélées mesure contextuelle ': 2006}, {'subspace cluster attribut clustering compréhensibilité ': 2005}], [{'thématique scientifique sémantique corpus fusionnons ': 2018}, {'ontobiotope vie microbiens agriculture microorganismes ': 2018}, {'approximatifs massives théorie caractéristique sélection ': 2018}, {'élastique noyau régularisation kdtw non-linéaire ': 2015}, {'ultramétricité dissimilaritées regroupement genere pam ': 2015}, {'agrégation représentation respectent rapportées attendues ': 2014}, {'eclipse logiciel passé leçon fondation ': 2014}, {'concept vidéo définir indexer segment ': 2012}, {'taxonomie hiérarchie construction expertisées automatique ': 2012}, {'wikipedia moteur permis question typer ': 2011}, {'pair routage incrémentale usage requête ': 2010}, {'correspondance ^ étudiées ontologie complexe ': 2010}, {'plainte pollution réalisable apparie domestique ': 2008}, {'lsa grammaticales conceptuelle éducatives syntaxico-sémantiques ': 2008}, {'k-faibles sous-bases valides confiance sens ': 2007}, {\"guidé phrase tirons n'utilisons segmenteur \": 2007}, {'implication alignement desmesures asymétrique concept ': 2006}, {'spécialité corpus ambiguïté étiqueté étiqueteur ': 2006}, {'convivail etiq inductif étiqueteur spécialité ': 2004}], [{'carte & cd7online utilisateur cartographie ': 2016}, {\"d113 flux '' `` centrerons \": 2015}, {'fusion éventuel driven linked conflit ': 2015}, {'propagation comparer modèle réécriture plate-forme ': 2015}, {\"document administratif numérisation homme l'opposé \": 2014}, {\"supervision domotiques interaction l'utilisateur capteur \": 2012}, {'assistant utilisateur paramétrages visuel paramétrage ': 2012}, {'skylines skycube treillis accord skycuboïdes ': 2011}, {'client méthodologie économique recommandation 000 ': 2011}, {'recouvrantes recouvrante osom carte auto-organisatrices ': 2010}, {'carte som auto-organisatrice binaires probabiliste ': 2006}, {'owl vérification conceptuels environnementgraphique interrogationet ': 2006}, {'document localisation terme tête positionné ': 2005}], [{'similarité campagne sémantique vectorielles complémentarité ': 2018}, {\"reconstruction évènements chronologie enquêteur d'enquête \": 2014}, {'environments pos tagging collaboration literary ': 2013}, {'arc concept relationnelle relationnelles treillis ': 2013}, {'procédure chirurgicales chirurgien dtw surgical ': 2012}, {'sociaux graphe réseau document ': 2011}, {'moteur wiki wikis sémantique art ': 2011}, {'moteur réponse question web sémantique ': 2011}, {'floues contextuelles préférence requête règle ': 2011}, {'gmm-smos locuteur vecteur variante smo ': 2010}, {'obstacle région intérêt extraction ': 2010}, {'desesper hydraulique surveillance pré-traitement centrale ': 2009}, {'événement presse automatique contenant annotev ': 2008}, {'annotation contextuelles sémantique émanant transmettre ': 2008}, {'ls-svm boosting grand machine ensemble ': 2007}, {'inconsistance annotation sémantique ontologie évolution ': 2007}, {'recouvrement classe applicatifs motivées part ': 2007}, {'non-pertinence hospitalier composante gestion visualisation ': 2006}, {'page annotation élément web ontologie ': 2006}, {'mobile objet entrepôt urbaine1 regain ': 2005}, {'reconnaissance/identification radar aériennes cible traitement ': 2005}, {\"génération concept treillis d'association exhaustive \": 2005}, {'page retournées websum pertinence évaluation ': 2005}, {'site parcours catégorie internaute perception ': 2005}], [{'âgées chute information bayésien risque ': 2018}, {'carte cognitive valident critère influence ': 2013}, {'expliquer co-partitionnement variable groupement préparation ': 2010}, {'weka formels plateforme basées développement ': 2010}, {'distinctifs itemsets flux idkf heikinheimo ': 2010}, {'21ème siècle organisation succinctement accompagner ': 2009}, {'binaires probabiliste carte bernoulli gtm ': 2008}, {'pondération locale carte variable segmentation ': 2008}, {'xml source xquery échanger matérialisées ': 2005}, {'mesh américain synonyme paragraphe cismef ': 2005}], [{'sarem architecture architectureextraction softwarearchitecture specifies ': 2016}, {'territoriale collectivité progiciel ingénierie introduction ': 2011}, {\"cube univsersité s'agréger hal tf-idf \": 2010}, {'pretopolib prétopologie java librairie implémentant ': 2010}], [{'métier antécédent recrutent e-recrutement sociaux ': 2018}, {'solaire rayonnement prédiction météorologiques régression ': 2018}, {'interrogation graphe conceptuels optionnel langage ': 2015}, {'phrase dépendance traduisant grammaire syntaxique ': 2014}, {'fcl intersection conceptuels communauté lien ': 2014}, {\"d'épisodes déposés quotidiennement prédiction épisode \": 2014}, {'noyau recouvrante ensembliste formulons autoriser ': 2014}, {'modularité newman 2mod-louvain inertie vectorielles ': 2014}, {'betti génératif simplicial complexe csg ': 2013}, {'twitter capitaliste sociaux réseau graphe ': 2013}, {'décomposition hiérarchique graphe blondel multi-échelle ': 2012}, {'spectrale relationnelle problème lagrange multiplicateur ': 2011}, {'graphe ac-réduits isomorphisme intéressante sous-graphes ': 2011}, {'dfc dépendance fonctionnelles conditionnelles df ': 2010}, {'exception possiblement surprenantes utile simultanée ': 2007}, {'sonar imperfection imprécision théorie milieu ': 2007}, {'estimateur rang prédicteurs univarié densité ': 2007}, {'floues classe membre classification 3 ': 2005}, {'sgbd bitmap index traitement oracle ': 2005}, {'algèbre dimensionnelles opérateur intègre multidimensionnelles ': 2005}, {\"graphe granulaire acp sommet '' \": 2004}], [{'thématique nouveauté émergentes observer scénario ': 2018}, {'morphologique multilingue terme structure extraction ': 2006}, {'biomédicales préférence source plate-forme scénario ': 2004}], [{'label heureux musique morceau co-occurrence ': 2018}, {'complétude prescriptives contrainte commande vérifier ': 2018}, {'document interrogation schéma orientés dictionnaire ': 2018}, {'microblogs localisation extraire apprentissage méthode ': 2018}, {'hotspots hotspot photographie relation fendu ': 2015}, {'déclaratif ppc contrainte hétérogène croisant ': 2014}, {'électriques compteur communicant duplication agrégées ': 2011}, {'flux résumé généraliste sgfd variées ': 2010}, {'stratégie robot observables humains boucle ': 2009}, {'relation répartie prédicats-arguments prédicatives recherchée ': 2008}, {'ordonnancement semi-supervisé bénéfiques non–étiquetées cacm ': 2007}, {\"variable critère sélection évaluation d'ambigüité \": 2005}], [{'2-colorability verification chromatic hypergraphs hypergraph ': 2016}, {'plongement isométrique métrique lch chodorow ': 2016}, {'lod définition étiquetage raisonnement ontologie ': 2016}, {'auteur pan-clef rédigé authentification rédigés ': 2015}, {'descripteur action st-surf reconnaissance spatio-temporels ': 2014}, {'légende corese concevoir raisonner cartographie ': 2012}, {'cartographie casi cartocel booléenne cellulaire ': 2010}, {'contrainte ajoutées interactive proche réduction ': 2010}, {'pcar cycliques cyclique varient caractérisé ': 2010}, {\"vente performantsd'aide 2007montre classifieurbayesien proposéeest \": 2009}, {'radar préparation radarsexpérimentales dequalité imagesisar ': 2006}, {'annotation document xml mécanisme coopératif ': 2004}, {'source spécifier spécification vue xml ': 2004}, {'nextclosure fermé partitionnement itemsets treillis ': 2004}, {'incomplets classement probabiliste arbre décision ': 2004}], [{'cardiovasculaires maladie facteur risque interaction ': 2018}, {'dt-cwt transform isar phase reconnaissance ': 2017}, {'télédétection référence réalignement préentons venant ': 2017}, {'hospitalier myocarde infarctus pmsi soumis ': 2016}, {'texte intersection copier/coller maximale alternative ': 2015}, {'nodaux communauté réseau noeud attribut ': 2014}, {'réconciliation alignement numérique combiner logique ': 2010}, {'breiman priori machine intégration maximum ': 2010}, {'désuffixation eda médical langage algorithme ': 2006}, {'molécule médicament attributsconstruits résultatscomparables méthodede ': 2006}, {'visuel prétraitement medicaldataset kent affectationvisuelle ': 2006}, {'statistique récentes avancée symbolique connaissance ': 2005}, {'forage agrégation télécharger distribué échantillon ': 2005}, {'hypermédias navigation veut aide détecté ': 2005}, {'induction apprenti inductifs extensionnelle acquisition ': 2004}, {'attribut complexe sélection objet classification ': 2004}], [{'météorologiques journée série mesurées ville ': 2018}, {'enquête ordinale méta-analyse ordre opinion ': 2018}, {'artificielle voir dela vision savoir ': 2018}, {'publicité portail affichage atteignent crées ': 2010}, {'symbolique échelle traitement grand donnée ': 2009}, {'al ancrer herrmann seeme cahier ': 2007}, {'sgbd relationnelles fouille donnée ': 2005}, {'hypertextuels e-lien partage structurées solution ': 2004}], [{'précision/rappel cost-sensitive ville maladie défi ': 2017}, {'multi-modales faux sociaux détection réseau ': 2017}, {\"géographiques spatiales régissant pourextraire d'associations \": 2016}, {'consensus partition adjoint fondons pex ': 2014}, {'climatiques engendrés spatiales risque apport ': 2010}, {'kgram graal abstraite implémente langage ': 2010}, {'vidéo série temporelles prédiction séquence ': 2010}, {'protein pgr graphe repository graph ': 2010}, {'gène expression réseau tigr mev ': 2007}, {'cure volumineuses extension fouille algorithme ': 2006}, {\"qualité facteur personnalisation besoinset afind'assister \": 2006}, {'annotation causalité conceptuels corpus schéma ': 2005}, {'optimisation temporelles requête web ': 2004}, {'datawarehouse hospitalier milieu qualité ': 2004}, {'entrepôt naturel risque examinons volumineux ': 2004}]]\n",
      "\n",
      "Mort: [[{'compte-rendu owl mammographiques classerautomatiquement mammaire ': 2006}], [{'graduelles réelsmontrent associés.des gradualité définissonsformellement ': 2009}, {\"siad fonctionnement éditoriale l'alimentent alimentation \": 2004}], [{'taaable recipes dish replacing theconstraints ': 2009}], [{'cognitifs cognition proposition sociale – ': 2005}], [{'programming constraint mining for data ': 2009}, {'entreprise mining produire théoriques forte ': 2008}, {'microarray advances recent mining data ': 2005}, {'summarizer summary uses method sentence ': 2004}, {\"trafic internet entrelacement cohorte d'internautes \": 2004}], [{'brevet patent unifiées offices invention ': 2008}], [{'augmentation capacité progresse lourde focalisation ': 2008}], [{'risque industriel raisonnement ineris national ': 2008}, {'juridiques dimensionnalité sontutilisés nouscomparons conceptspermettent ': 2006}, {'enseignement apprenant pédagogique dispositif distance ': 2005}], [{'quelques-unes espérant dis- relevées model-based ': 2007}], [{'bootstrap coupure ouvrons systématiquement chi-merge ': 2008}, {'iceberg galois text-mining ontologie treillis ': 2005}], [{'merger rss ': 2010}], [{'signe codex maya interprétables situant ': 2010}], [{'assessing knn uncertainty fusion data ': 2009}]]\n",
      "\n",
      "Nouveau: [[{'forum réputation vote santé message ': 2016}, {'forum patient sentiment santé surprise ': 2014}], [{'apprenant leader ” profil recommandation ': 2018}, {'model training tree method stream ': 2016}, {'évolutif statique suivi clustering ordinaire ': 2013}], [{'smarter computers dream texts large ': 2014}, {'vigilance eeg électrode 58 cart ': 2012}], [{'across profiles snss individual that ': 2014}], [{'sms sentiment new social explosive ': 2016}], [{'manquant analogique proportion valeur booléens ': 2014}], [{'factory network social overloading sub-typing ': 2016}], [{'bilingual multilingual document and collection ': 2015}], [{'cybercriminalité mondiale déplaisant interconnecter mondialisation ': 2015}], [{'manquant hydrologie gapit hydrométriques station ': 2015}], [{'linéaire shift distribution variées dataset ': 2018}]]\n"
     ]
    }
   ],
   "source": [
    "# On affiche la classification de tendances.\n",
    "print(\"\\nRecent:\",recent)\n",
    "print(\"\\nAncien:\",ancien)\n",
    "print(\"\\nMoyen:\",moyen)\n",
    "print(\"\\nMort:\",mort)\n",
    "print(\"\\nNouveau:\",nouveau)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recent : 4 , Ancien : 4 , Moyen : 68 , Mort :  13 , Nouveau : 11\n"
     ]
    }
   ],
   "source": [
    "print(\"Recent :\", len(recent), \", Ancien :\", len(ancien), \", Moyen :\", len(moyen), \", Mort : \", len(mort), \", Nouveau :\", len(nouveau))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La majorité des clusters sont classés en moyen, c'est cohérent mais une classification plus affinée pourrait être intéressante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 925.44 secondes ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- %s secondes ---\" % round(time.process_time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Évaluation de la classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maximiser la distance inter cluster et minimiser la distance intra cluster\n",
    "\n",
    "Distance intra max < Distance inter min\n",
    "\n",
    "Comparons l'efficacité de la similarité de Jaccard et de la similarité Cosine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### En l'état actuel, l'évaluation ne fonctionne pas car elle se base sur une ancienne version des clusters.\n",
    "#### La priorité est d'obtenir de meilleurs clusters donc ces fonctions sont mises de coté en attendant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaccard(vec1, vec2):\n",
    "    commun = 0\n",
    "    diff = 0\n",
    "    for key in vec1.keys():\n",
    "        if key in vec2.keys():\n",
    "            commun = commun + 1\n",
    "        else:\n",
    "            diff = diff + 1\n",
    "    if diff == 0:\n",
    "        return -1\n",
    "    return commun/diff\n",
    "\n",
    "def cosine(vec1, vec2):\n",
    "    v1 = np.array(list(vec1.values()))\n",
    "    v2 = np.array(list(vec2.values()))\n",
    "    # La partie qui suit devra être retirée plus tard quand on aura des clusters normalisés.\n",
    "    if len(vec1) > len(vec2):\n",
    "        v1 = v1[:len(vec2)]\n",
    "    else:\n",
    "        v2 = v2[:len(vec1)]\n",
    "    return np.dot(v1, v2) / (np.sqrt(np.sum(v1**2)) * np.sqrt(np.sum(v2**2)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"J\",jaccard(cluster_list[0],cluster_list[1]))\n",
    "#print(\"C\",cosine(cluster_list[0],cluster_list[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 925.59 seconds ---\n"
     ]
    }
   ],
   "source": [
    "print(\"--- %s seconds ---\" % round(time.process_time() - start_time, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Post-soutenance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Réduire le vocabulaire.\n",
    "Augmenter les clusters.\n",
    "Évaluer correctement la classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
